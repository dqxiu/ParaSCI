belz and kow proposed another smt based nlg system which made use of the phrase-based smt model .
in order to measure translation quality , we use bleu 7 and ter scores .
note that we use the naive bayes multinomial classifier in weka for classification .
as for multiwords , we used the phrases from the pre-trained google news word2vec vectors , which were obtained using a simple statistical approach .
liu et al proposed a recursive neural network designed to model the subtrees , and cnn to capture the most important features on the shortest dependency path .
parameters were tuned using minimum error rate training .
taxonomies , which serve as backbones for structured knowledge , are useful for many nlp applications such as question answering and document clustering .
relation extraction is the task of finding semantic relations between two entities from text .
like we used support vector machines via the classifier svmlight .
we pre-train the word embedding via word2vec on the whole dataset .
in this paper we presented the results of a corpus study of naturally occurring crs in task-oriented dialogue .
we estimate a 5-gram language model using interpolated kneser-ney discounting with srilm .
a 5-gram language model with kneser-ney smoothing was trained with srilm on monolingual english data .
zeng et al use convolutional neural network for learning sentence-level features of contexts and obtain good performance even without using syntactic features .
we use several classifiers including logistic regression , random forest and adaboost implemented in scikit-learn .
negation is a complex phenomenon present in all human languages , allowing for the uniquely human capacities of denial , contradiction , misrepresentation , lying , and irony ( horn and wansing , 2015 ) .
lei et al employ three-way tensors to obtain a low-dimensional input representation optimized for parsing performance .
word sense disambiguation ( wsd ) is the task of identifying the correct sense of an ambiguous word in a given context .
for representing words , we used 100 dimensional pre-trained glove embeddings .
we trained a 4-gram language model on this data with kneser-ney discounting using srilm .
our model uses non-negative matrix factorization in order to find latent dimensions .
for training the translation model and for decoding we used the moses toolkit .
in this run , we use a sentence vector derived from word embeddings obtained from word2vec .
traditional supervised re models heavily rely on abundant amounts of high-quality annotated data .
results confirmed that our method using noun-phrase chunking is effective for automatic evaluation for machine translation .
the english side of the parallel corpus is trained into a language model using srilm .
we use 300-dimensional vectors that were trained and provided by word2vec tool using a part of the google news dataset 4 .
bilingual dictionaries are an essential resource in many multilingual natural language processing tasks such as machine translation and cross-language information retrieval .
we use the moses toolkit to train our phrase-based smt models .
our baseline system is an standard phrase-based smt system built with moses .
grammar induction is the task of inducing high-level rules for application of grammars in spoken dialogue systems .
we use moses , a statistical machine translation system that allows training of translation models .
we use the popular moses toolkit to build the smt system .
we used a standard pbmt system built using moses toolkit .
we used the sri language modeling toolkit with kneser-kney smoothing .
the language model has an embedding size of 250 and two lstm layers with a hidden size of 1000 .
conditional random fields are undirected graphical models of a conditional distribution .
table 4 shows end-to-end translation bleu score results .
for example , faruqui et al introduce knowledge in lexical resources into the models in word2vec .
negation is a complex phenomenon present in all human languages , allowing for the uniquely human capacities of denial , contradiction , misrepresentation , lying , and irony ( cite-p-18-3-7 ) .
in addition , we utilize the pre-trained word embeddings with 300 dimensions from for initialization .
we use the stanford named entity recognizer for this purpose .
bahdanau et al propose integrating an attention mechanism in the decoder , which is trained to determine on which portions of the source sentence to focus .
we used 100 dimensional glove embeddings for this purpose .
the system was trained using moses with default settings , using a 5-gram language model created from the english side of the training corpus using srilm .
table 2 shows the blind test results using bleu-4 , meteor and ter .
we learn the noise model parameters using an expectation-maximization approach .
discourse segmentation is the first step in building discourse parsers .
semantic parsing is the task of transducing natural language ( nl ) utterances into formal meaning representations ( mrs ) , commonly represented as tree structures .
using word or phrase representations as extra features has been proven to be an effective and simple way to improve the predictive performance of an nlp system .
hatzivassiloglou and mckeown proposed a supervised algorithm to determine the semantic orientation of adjectives .
latent dirichlet allocation is one of the widely adopted generative models for topic modeling .
a 5-gram language model with kneser-ney smoothing was trained with srilm on monolingual english data .
we use the 300-dimensional skip-gram word embeddings built on the google-news corpus .
we obtained distributed word representations using word2vec 4 with skip-gram .
for this purpose , we use the minipar dependency parser .
gong et al and xiao et al introduce topic-based similarity models to improve smt system .
all han models and a subset of bca models are initialized with pretrained glove word embeddings 1 .
bleu is widely used for automatic evaluation of machine translation systems .
high quality word embeddings have been proven helpful in many nlp tasks .
we used srilm for training the 5-gram language model with interpolated modified kneser-ney discounting , .
for language model , we use a trigram language model trained with the srilm toolkit on the english side of the training corpus .
an english 5-gram language model is trained using kenlm on the gigaword corpus .
with the user and product attention , our model can take account of the global user preference and product characteristics in both word level and semantic level .
segmentation is the first step in a discourse parser , a system that constructs discourse trees from elementary discourse units .
we further used a 5-gram language model trained using the srilm toolkit with modified kneser-ney smoothing .
we use 300-dimensional word embeddings from glove to initialize the model .
dependency parsing is the task of predicting the most probable dependency structure for a given sentence .
empirical results showed that our model can generate either general or specific responses , and significantly outperform state-of-the-art generation methods .
unpruned language models were trained using lmplz which employs modified kneser-ney smoothing .
we compute the syntactic features only for pairs of event mentions from the same sentence , using the stanford dependency parser .
for opinion mining , wu et al also utilized a dependency structure based on mwus , although they restricted mwus with predefined relations .
we use srilm for training a trigram language model on the english side of the training corpus .
we use srilm toolkit to train a trigram language model with modified kneser-ney smoothing on the target side of training corpus .
mikolov et al and mikolov et al introduce efficient methods to directly learn high-quality word embeddings from large amounts of unstructured raw text .
for feature building , we use word2vec pre-trained word embeddings .
a kn-smoothed 5-gram language model is trained on the target side of the parallel data with srilm .
unsupervised word embeddings trained from large amounts of unlabeled data have been shown to improve many nlp tasks .
for japanese , we produce rmrs from the dependency parser cabocha .
our model is a first order linear chain conditional random field .
roth and yih also described a classification-based framework in which they jointly learn to identify named entities and relations .
we train the models for 20 epochs using categorical cross-entropy loss and the adam optimization method .
in this paper , we propose to use the co-training approach to address the problem of cross-lingual sentiment classification .
semantic parsing is reduced to query graph generation , formulated as a staged search problem .
a pun is the exploitation of the various meanings of a word or words with phonetic similarity but different meanings .
we use 4-gram language models in both tasks , and conduct minimumerror-rate training to optimize feature weights on the dev set .
we use pre-trained word vectors from glove .
the optimisation of the feature weights of the model is done with minimum error rate training against the bleu evaluation metric .
tai et al , and le and zuidema extended sequential lstms to tree-structured lstms by adding branching factors .
on the remaining tweets , we trained a 10-gram word length model , and a 5-gram language model , using srilm with kneyser-ney smoothing .
the tuning step used minimum error rate training .
we build a french tagger based on englishfrench data from the europarl corpus .
model fitting for our model is based on the expectation-maximization algorithm .
we use the cube pruning method to approximately intersect the translation forest with the language model .
collobert et al used word embeddings as the input of various nlp tasks , including part-of-speech tagging , chunking , ner , and semantic role labeling .
specifically , we adopt linear-chain conditional random fields as the method for sequence labeling .
we observe that the propbank roles are more robust in all tested experimental conditions , i.e. , the performance decrease is more severe for verbnet .
all systems are evaluated using case-insensitive bleu .
we used the scikit-learn implementation of svrs and the skll toolkit .
we trained a specific language model using srilm from each of these corpora in order to estimate n-gram log-probabilities .
al-onaizan and knight present a hybrid model for arabic-to-english transliteration , which is a linear combination of phoneme-based and grapheme-based models .
we used srilm to build a 4-gram language model with interpolated kneser-ney discounting .
we use the collapsed tree formalism of the stanford dependency parser .
in this paper , we present a method for the semantic tagging of word chunks extracted from a written transcription of conversations .
we used glove word embeddings with 300 dimensions pre-trained using commoncrawl to get a vector representation of the evidence sentence .
semeval is a yearly event in which teams compete in natural language processing tasks .
distributional semantic models represent the meanings of words by relying on their statistical distribution in text .
a 4-gram language model was trained on the monolingual data by the srilm toolkit .
we used svm-light-tk , which enables the use of the partial tree kernel .
to obtain these features , we use the word2vec implementation available in the gensim toolkit to obtain word vectors with dimension 300 for each word in the responses .
we trained a phrase-based smt engine to translate known words and phrases using the training tools available with moses .
active learning is a promising way for sentiment classification to reduce the annotation cost .
a manual annotation effort demonstrates implicit relations yield substantial additional meaning .
firstly , we built a forward 5-gram language model using the srilm toolkit with modified kneser-ney smoothing .
the weights associated to feature functions are optimally combined using the minimum error rate training .
we used the srilm toolkit to build unpruned 5-gram models using interpolated modified kneser-ney smoothing .
we implemented the different aes models using scikit-learn .
we apply sri language modeling toolkit to train a 4-gram language model with kneser-ney smoothing .
we used minimum error rate training for tuning on the development set .
relation extraction is the key component for building relation knowledge graphs , and it is of crucial significance to natural language processing applications such as structured search , sentiment analysis , question answering , and summarization .
we learn our word embeddings by using word2vec 3 on unlabeled review data .
djuric et al , 2015 ) used binary classification to detect hate speech .
text categorization is a classical text information processing task which has been studied adequately ( cite-p-18-1-9 ) .
we train a trigram language model with the srilm toolkit .
case-insensitive bleu-4 is our evaluation metric .
sentiment analysis is a much-researched area that deals with identification of positive , negative and neutral opinions in text .
the weights of the different feature functions were optimised by means of minimum error rate training .
we propose two kinds of probabilistic models defined on parsing actions to compute the probability of entire sentence .
we evaluate using the standard penalty metrics p k and windowdiff .
we used a 5-gram language model with modified kneser-ney smoothing implemented using the srilm toolkit .
mikolov et al proposed the word2vec method for learning continuous vector representations of words from large text datasets .
translation quality is measured in truecase with bleu on the mt08 test sets .
with english gigaword corpus , we use the skip-gram model as implemented in word2vec 3 to induce embeddings .
the system was trained using the moses toolkit .
we used nltk wordnet synsets for obtaining the ambiguity of the word .
our first layer was a 200-dimensional embedding layer , using the glove twitter embeddings .
word2vec is an appropriate tool for this problem .
we used the scikit-learn implementation of svrs and the skll toolkit .
the web-derived ukwac is already tokenized and pos-tagged with the treetagger .
the language model is a 5-gram with interpolation and kneserney smoothing .
stance detection has been defined as automatically detecting whether the author of a piece of text is in favor of the given target or against it .
part-of-speech tagging is the problem of determining the syntactic part of speech of an occurrence of a word in context .
we built a trigram language model with kneser-ney smoothing using kenlm toolkit .
a key insight in our approach is to reduce content selection and surface realization into a common parsing problem .
we use the word2vec framework in the gensim implementation to generate the embedding spaces .
for input representation , we used glove word embeddings .
we trained a 5-gram sri language model using the corpus supplied for this purpose by the shared task organizers .
the negated event is the property that is negated by the cue .
we use 300 dimension word2vec word embeddings for the experiments .
bleu is used as a standard evaluation metric .
these features were extracted using stanford corenlp .
lstms were introduced by hochreiter and schmidhuber in order to mitigate the vanishing gradient problem .
we use the adam optimizer for the gradient-based optimization .
we apply sri language modeling toolkit to train a 4-gram language model with kneser-ney smoothing .
word sense disambiguation ( wsd ) is the task of identifying the correct sense of an ambiguous word in a given context .
as monolingual baselines , we use the skip-gram and cbow methods of mikolov et al as implemented in the gensim package .
in our experiment , word embeddings were 200-dimensional as used in , trained on gigaword with word2vec .
as a baseline system , we used the moses statistical machine translation package to build grapheme-based and phoneme-based translation systems , using a bigram language model .
several recent syntax-based models for machine translation can be seen as instances of the general framework of synchronous grammars and tree transducers .
the translations are evaluated in terms of bleu score .
our method involved using the machine translation software moses .
information extraction ( ie ) is the process of finding relevant entities and their relationships within textual documents .
we use the attentive nmt model introduced by bahdanau et al as our text-only nmt baseline .
more recently , mikolov et al propose two log-linear models , namely the skip-gram and cbow model , to efficiently induce word embeddings .
the irstlm toolkit is used to build ngram language models with modified kneser-ney smoothing .
mikolov et al , 2013a ) proposes skip-gram and continuous bag-of-words models based on a single-layer network architecture .
we estimated 5-gram language models using the sri toolkit with modified kneser-ney smoothing .
in this work , we use vmf as the observational distribution .
we trained a trigram language model on the chinese side , with the srilm toolkit , using the modified kneser-ney smoothing option .
for language model , we use a trigram language model trained with the srilm toolkit on the english side of the training corpus .
our baseline is the smt toolkit moses run over letter strings rather than word strings .
sentiment analysis is a research area in the field of natural language processing .
text segmentation is the task of dividing text into segments , such that each segment is topically coherent , and cutoff points indicate a change of topic ( cite-p-15-1-8 , cite-p-15-3-4 , cite-p-15-1-3 ) .
language models were built with srilm , modified kneser-ney smoothing , default pruning , and order 5 .
to minimize the objective , we use stochastic gradient descent with the diagonal variant of adagrad .
semantic role labeling ( srl ) is the task of identifying the predicate-argument structure of a sentence .
it has been empirically shown that word embeddings can capture semantic and syntactic similarities between words .
sentiment analysis ( sa ) is a field of knowledge which deals with the analysis of people ’ s opinions , sentiments , evaluations , appraisals , attitudes and emotions towards particular entities ( cite-p-17-1-0 ) .
based on the distributional hypothesis , we train a skip-gram model to learn the distributional representations of words in a large corpus .
we use srilm for training a trigram language model on the english side of the training data .
we optimized each system separately using minimum error rate training .
we use the 300-dimensional pre-trained word2vec 3 word embeddings and compare the performance with that of glove 4 embeddings .
we evaluated the translation quality using the bleu-4 metric .
experiments were performed using the publicly available europarl corpora for the english-french language pair .
further , we apply a 4-gram language model trained with the srilm toolkit on the target side of the training corpus .
we use the 300-dimensional pre-trained word2vec 3 word embeddings and compare the performance with that of glove 4 embeddings .
we used glove word embeddings with 300 dimensions pre-trained using commoncrawl to get a vector representation of the evidence sentence .
although coreference resolution is a subproblem of natural language understanding , coreference resolution evaluation metrics have predominately been discussed in terms of abstract entities and hypothetical system errors .
table 1 shows the translation performance by bleu .
for feature extraction , we used the stanford pos tagger .
we use moses , a statistical machine translation system that allows training of translation models .
word sense disambiguation ( wsd ) is formally defined as the task of computationally identifying senses of a word in a context .
the weights of the different feature functions were optimised by means of minimum error rate training on the 2013 wmt test set .
then we use the standard minimum error-rate training to tune the feature weights to maximize the system潞s bleu score .
we obtained parse trees using the stanford parser , and used jacana for word alignment .
we use a morphological analyzer for arabic called madamira .
shen et al proposed a string-to-dependency model , which restricted the target-side of a rule by dependency structures .
we evaluate the translation quality using the case-sensitive bleu-4 metric .
we used the bleu score to evaluate the translation accuracy with and without the normalization .
semantic parsing is the task of mapping natural language sentences into logical forms which can be executed on a knowledge base ( cite-p-18-5-13 , cite-p-18-5-14 , cite-p-18-3-6 , cite-p-18-5-8 , cite-p-18-3-15 , cite-p-18-3-9 ) .
a 4-grams language model is trained by the srilm toolkit .
we used yamcha 1 , which is a general purpose svm-based chunker .
we used the pre-trained word embeddings that were learned using the word2vec toolkit on google news dataset .
in this task , we use the 300-dimensional 840b glove word embeddings .
nenkova et al found that entrainment on high-frequency words was correlated with naturalness , task success , and coordinated turn-taking behavior .
the srilm language modelling toolkit was used with interpolated kneser-ney discounting .
our trigram word language model was trained on the target side of the training corpus using the srilm toolkit with modified kneser-ney smoothing .
language models of order 5 have been built and interpolated with srilm and kenlm .
word alignment is a well-studied problem in natural language computing .
our 5-gram language model is trained by the sri language modeling toolkit .
similarity is a kind of association implying the presence of characteristics in common .
the promt smt system is based on the moses open-source toolkit .
we use the glove vectors of 300 dimension to represent the input words .
the integrated dialect classifier is a maximum entropy model that we train using the liblinear toolkit .
we perform the mert training to tune the optimal feature weights on the development set .
shutova defined metaphor interpretation as a paraphrasing task and presented a method for deriving literal paraphrases for metaphorical expressions from the bnc .
we first use bleu score to perform automatic evaluation .
the treebank data in our experiments are from the conll shared-tasks on dependency parsing .
the wordnet-affect resource was employed for obtaining the affective terms .
coreference resolution is a complex problem , and successful systems must tackle a variety of non-trivial subproblems that are central to the coreference task — e.g. , mention/markable detection , anaphor identification — and that require substantial implementation efforts .
coreference resolution is the task of automatically grouping references to the same real-world entity in a document into a set .
the feature weights for each system were tuned on development sets using the moses implementation of minimum error rate training .
we use the moses statistical mt toolkit to perform the translation .
we use pre-trained word2vec word vectors and vector representations by tilk et al to obtain word-level similarity information .
all word vectors are trained on the skipgram architecture .
translation results are evaluated using the word-based bleu score .
twitter is a microblogging service that has 313 million monthly active users 1 .
our machine translation system is a phrase-based system using the moses toolkit .
the translation quality is evaluated by caseinsensitive bleu-4 metric .
we trained kneser-ney discounted 5-gram language models on each available corpus using the srilm toolkit .
we use the srilm toolkit to compute our language models .
we tune model weights using minimum error rate training on the wmt 2008 test data .
we used a 5-gram language model with modified kneser-ney smoothing , built with the srilm toolkit .
all linear models were trained with the perceptron update rule .
following , we use the na茂ve bayes model implemented in weka for candidate phrase selection .
relation extraction ( re ) is the task of identifying instances of relations , such as nationality ( person , country ) or place of birth ( person , location ) , in passages of natural text .
the language model defined by the expression is named the conditional language model .
we also develop a semantic parser for this corpus .
dependency annotation for hindi is based on paninian framework for building the treebank .
the embeddings were trained over the english wikipedia using word2vec .
part-of-speech ( pos ) tagging is a job to assign a proper pos tag to each linguistic unit such as word for a given sentence .
we estimated 5-gram language models using the sri toolkit with modified kneser-ney smoothing .
motivated by the idea of addressing wce problem as a sequence labeling process , we employ the conditional random fields for our model training , with wapiti toolkit .
we use the cube pruning method to approximately intersect the translation forest with the language model .
semantic role labeling ( srl ) is the task of identifying the semantic arguments of a predicate and labeling them with their semantic roles .
the best model achieved an overall wer improvement of 10 % relative to the 3-gram baseline .
we use srilm for training a trigram language model on the english side of the training corpus .
sentence compression is the task of producing a shorter form of a single given sentence , so that the new form is grammatical and retains the most important information of the original one .
we used the moses toolkit for performing statistical machine translation .
we evaluated the proposed method using four evaluation measures , bleu , nist , wer , and per .
the translation results are evaluated by caseinsensitive bleu-4 metric .
we pretrain word vectors with the word2vec tool on the news dataset released by ding et al , which are fine-tuned during training .
our baseline system is re-implementation of hiero , a hierarchical phrase-based system .
to train our neural algorithm , we apply word embeddings of a look-up from 100-d glove pre-trained on wikipedia and gigaword .
framenet is a lexico-semantic resource focused on semantic frames .
the translation systems were evaluated by bleu score .
we train and evaluate a l2-regularized logistic regression classifier with the liblin-ear solver as implemented in scikit-learn .
neural networks , working on top of conventional n-gram models , have been introduced in as a potential means to improve conventional n-gram language models .
our translation system is an in-house phrasebased system analogous to moses .
this paper presents an unsupervised learning approach to non-english stemming .
dependency parsing is a valuable form of syntactic processing for nlp applications due to its transparent lexicalized representation and robustness with respect to flexible word order languages .
we used a phrase-based smt model as implemented in the moses toolkit .
we use the glove pre-trained word embeddings for the vectors of the content words .
we use the mallet implementation of conditional random fields .
sentiment analysis is a growing research field , especially on web social networks .
our semantic parser is implemented as a neural sequence-to-sequence model with attention .
we implement an in-domain language model using the sri language modeling toolkit .
in this section , we briefly review the hmm alignment model .
language models were built using the sri language modeling toolkit with modified kneser-ney smoothing .
the word embeddings can provide word vector representation that captures semantic and syntactic information of words .
we use the automatic mt evaluation metrics bleu , meteor , and ter , to evaluate the absolute translation quality obtained .
coreference resolution is the problem of partitioning a sequence of noun phrases ( or mentions ) , as they occur in a natural language text , into a set of referential entities .
coreference resolution is the process of linking together multiple expressions of a given entity .
we used trigram language models with interpolated kneser-kney discounting trained using the sri language modeling toolkit .
in recent years , error mining techniques have been developed to help identify the most likely sources of parsing failure ( cite-p-15-3-2 , cite-p-15-3-1 , cite-p-15-1-4 ) .
the matrix is weighted using positive pointwise mutual information .
cohesion is a surface-level property of well-formed texts .
we use a set of 318 english function words from the scikit-learn package .
we use the moses package to train a phrase-based machine translation model .
word sense disambiguation ( wsd ) is a key enabling-technology that automatically chooses the intended sense of a word in context .
coreference resolution is the task of partitioning the set of mentions of discourse referents in a text into classes ( or ‘ chains ’ ) corresponding to those referents ( cite-p-12-3-14 ) .
the model was built using the srilm toolkit with backoff and kneser-ney smoothing .
in addition , we use an english corpus of roughly 227 million words to build a target-side 5-gram language model with srilm in combination with kenlm .
we use the glove vector representations to compute cosine similarity between two words .
these models are usually regarded as features and combined with scaling factors to form a log-linear model .
the ud scheme is built on the google universal part-of-speech tagset , the interset interlingua of morphosyntactic features , and stanford dependencies .
we use a random forest classifier , as implemented in scikit-learn .
we report case-sensitive bleu and ter as the mt evaluation metrics .
a 4-gram language model was trained on the monolingual data by the srilm toolkit .
relation extraction is the task of finding semantic relations between two entities from text .
we used srilm to build a 4-gram language model with interpolated kneser-ney discounting .
we derive 100-dimensional word vectors using word2vec skip-gram model trained over the domain corpus .
kalchbrenner et al proposed a dynamic convolution neural network with multiple layers of convolution and k-max pooling to model a sentence .
the penn discourse treebank is the largest corpus richly annotated with explicit and implicit discourse relations and their senses .
major discourse annotated resources in english include the rst treebank and the penn discourse treebank .
zens and ney show that itg constraints yield significantly better alignment coverage than the constraints used in ibm statistical machine translation models on both german-english and french-english .
semantic role labeling ( srl ) is a major nlp task , providing a shallow sentence-level semantic analysis .
we used a 5-gram language model with modified kneser-ney smoothing , built with the srilm toolkit .
translation results are evaluated using the word-based bleu score .
we use pre-trained 50-dimensional word embeddings vector from glove .
sentiment classification is a well studied problem ( cite-p-13-3-6 , cite-p-13-1-14 , cite-p-13-3-3 ) and in many domains users explicitly provide ratings for each aspect making automated means unnecessary .
the weights of the log-linear interpolation model were optimized via minimum error rate training on the ted development set , using 200 best translations at each tuning iteration .
in this run , we use a sentence vector derived from word embeddings obtained from word2vec .
word sense disambiguation ( wsd ) is the task of assigning sense tags to ambiguous lexical items ( lis ) in a text .
we implemented our method in a phrase-based smt system .
a 3-gram language model is trained on the target side of the training data by the srilm toolkits with modified kneser-ney smoothing .
we used the logistic regression implemented in the scikit-learn library with the default settings .
twitter is a communication platform which combines sms , instant messages and social networks .
we follow previous studies , conducting experiments by using the rst discourse treebank .
continuous representation of words and phrases are proven effective in many nlp tasks .
sentence compression is the task of generating a grammatical and shorter summary for a long sentence while preserving its most important information .
parameter tuning was carried out using both k-best mira and minimum error rate training on a held-out development set .
we used yamcha as a text chunker , which is based on support vector machine .
a knsmoothed 5-gram language model is trained on the target side of the parallel data with srilm .
the statistical significance test is performed using the re-sampling approach .
discourse segmentation is the first step in building a discourse parser .
collobert et al used word embeddings as the input of various nlp tasks , including part-of-speech tagging , chunking , ner , and semantic role labeling .
relation extraction is a subtask of information extraction that finds various predefined semantic relations , such as location , affiliation , rival , etc. , between pairs of entities in text .
mikolov et al showed that the sg algorithm achieves better accuracies in tested cases .
we use a pbsmt model built with the moses smt toolkit .
the targetside 4-gram language model was estimated using the srilm toolkit and modified kneser-ney discounting with interpolation .
we use the weka toolkit and the derived features to train a naive-bayes classifier .
we use minimum error rate training to tune the feature weights of hpb for maximum bleu score on the development set with serval groups of different start weights .
we chose the skip-gram model provided by word2vec tool developed by for training word embeddings .
coreference resolution is the task of partitioning a set of mentions ( i.e . person , organization and location ) into entities .
named entity recognition ( ner ) is the task of identifying named entities in free text—typically personal names , organizations , gene-protein entities , and so on .
otero et al took advantage of the translation equivalents inserted in wikipedia by means of interlanguage links to extract similar articles .
we report the mt performance using the original bleu metric .
we use a cws-oriented model modified from the skip-gram model to derive word embeddings .
socher et al present a compositional model based on a recursive neural network .
we use word2vec as the vector representation of the words in tweets .
coreference resolution is the task of partitioning a set of mentions ( i.e . person , organization and location ) into entities .
we use the automatic mt evaluation metrics bleu , meteor , and ter , to evaluate the absolute translation quality obtained .
the work described in this paper is based on the smt framework of hierarchical phrase-based translation .
a pun is a form of wordplay , which is often profiled by exploiting polysemy of a word or by replacing a phonetically similar sounding word for an intended humorous effect .
the dependency parser we use is an implementation of a transition-based dependency parser .
we trained a 4-gram language model on the xinhua portion of gigaword corpus using the sri language modeling toolkit with modified kneser-ney smoothing .
fast decoding is achieved by using a novel multiple-beam search algorithm .
name tagging is a critical early stage in many natural language processing pipelines .
for both languages , we used the srilm toolkit to train a 5-gram language model using all monolingual data provided .
there has been a substantial amount of work on automatic semantic role labeling , starting with the statistical model of gildea and jurafsky .
specifically , we used wordsim353 , a benchmark dataset , consisting of relatedness judgments for 353 word pairs .
we follow this practice here , and additionally detect person names at decode-time using the stanford named entity recognizer .
to get a dictionary of word embeddings , we use the word2vec tool 2 and train it on the chinese gigaword corpus .
we apply a pretrained glove word embedding on .
we first use bleu score to perform automatic evaluation .
we use the automatic mt evaluation metrics bleu , meteor , and ter , to evaluate the absolute translation quality obtained .
the system used a tri-gram language model built from sri toolkit with modified kneser-ney interpolation smoothing technique .
we use pre-trained word vectors from glove .
a pun is a form of wordplay in which one sign ( e.g. , a word or phrase ) suggests two or more meanings by exploiting polysemy , homonymy , or phonological similarity to another sign , for an intended humorous or rhetorical effect ( aarons , 2017 ; hempelmann and miller , 2017 ) .
the embedded word vectors are trained over large collections of text using variants of neural networks .
our word embeddings is initialized with 100-dimensional glove word embeddings .
to do so , we utilized the popular latent dirichlet allocation , topic modeling method .
all features were log-linearly combined and their weights were optimized by performing minimum error rate training .
our experimental results show that ltagbased features can help improve the performance of srl systems .
we evaluated the translation quality using the case-insensitive bleu-4 metric .
traditionally , a language model is a probabilistic model which assigns a probability value to a sentence or a sequence of words .
to train the models we use the default stochastic gradient descent classifier provided by scikit-learn .
for phrase-based smt translation , we used the moses decoder and its support training scripts .
in addition , a 5-gram lm with kneser-ney smoothing and interpolation was built using the srilm toolkit .
collobert et al employ a cnn-crf structure , which obtains competitive results to statistical models .
we implemented our method in a phrase-based smt system .
goldwater and griffiths employ a bayesian approach to pos tagging and use sparse dirichlet priors to minimize model size .
we obtained parse trees using the stanford parser , and used jacana for word alignment .
the language model was constructed using the srilm toolkit with interpolated kneser-ney discounting .
named entity recognition ( ner ) is the task of identifying and typing phrases that contain the names of persons , organizations , locations , and so on .
popular topic modeling techniques include latent dirichlet allocation and probabilistic latent semantic analysis .
the language models were created using the srilm toolkit on the standard training sections of the ccgbank , with sentenceinitial words uncapitalized .
we use 100-dimension glove vectors which are pre-trained on a large twitter corpus and fine-tuned during training .
semantic role labeling ( srl ) has been defined as a sentence-level natural-language processing task in which semantic roles are assigned to the syntactic arguments of a predicate ( cite-p-14-1-7 ) .
in our implementation , we train a tri-gram language model on each phone set using the srilm toolkit .
we use the glove word vector representations of dimension 300 .
relation extraction is the task of recognizing and extracting relations between entities or concepts in texts .
lexical simplification is a technique that substitutes a complex word or phrase in a sentence with a simpler synonym .
thus , zesch and gurevych semi-automatically created word pairs from domain-specific corpora .
crowdsourcing is a viable mechanism for creating training data for machine translation .
we use the linear svm classifier from scikit-learn .
previous work showed that word clusters derived from an unlabelled dataset can improve the performance of many nlp applications .
word embeddings are initialized with glove 27b trained on tweets and are trainable parameters .
we used 5-gram models , estimated using the sri language modeling toolkit with modified kneser-ney smoothing .
soricut and marcu address the task of parsing discourse structures within the same sentence .
we use the cbow model for the bilingual word embedding learning .
we implemented the different aes models using scikit-learn .
hierarchical phrase-based translation is one of the current promising approaches to statistical machine translation .
the data come from the conll-x and conll 2007 shared tasks .
kim et al and kulkarni et al computed the degree of meaning change by applying neural networks for word representation .
the model parameters of word embedding are initialized using word2vec .
we use a random forest classifier , as implemented in scikit-learn .
ccg is a strongly lexicalized formalism , in which every word is associated with a syntactic category ( similar to an elementary syntactic structure ) indicating its subcategorization potential .
the visargue framework provides a novel visual analytics toolbox for exploratory and confirmatory analyses of multi-party discourse data .
seki et al proposed a probabilistic model for the sub-tasks of anaphoric identification and antecedent identification with the help of a verb dictionary .
discourse parsing is a challenging task and is crucial for discourse analysis .
to generate these trees , we employ the stanford pos tagger 8 and the stack version of the malt parser .
we used kenlm with srilm to train a 5-gram language model based on all available target language training data .
relation extraction is a key step towards question answering systems by which vital structured data is acquired from underlying free text resources .
coreference resolution is the task of clustering a set of mentions in the text such that all mentions in the same cluster refer to the same entity .
the conventional domain adaptation method is fine tuning , in which an out-of-domain model is further trained on indomain data .
in section 4 , we develop a correct , complete and terminating extension of earley 's algorithm for the patr-ii formalism using the restriction notion .
we implemented linear models with the scikit learn package .
in the evaluation , the similarity-model shows lower error rates than both resnik¡¯s wordnet-based model and the em-based clustering model .
distributional semantic models build on the distributional hypothesis which states that the meaning of a word can be modelled by observing the contexts in which it is used .
a 5-gram language model with kneser-ney smoothing was trained with srilm on monolingual english data .
semantic parsing is the task of transducing natural language ( nl ) utterances into formal meaning representations ( mrs ) , commonly represented as tree structures .
we employ the sentiment analyzer in stanford corenlp to do so .
part-of-speech tagging is the problem of determining the syntactic part of speech of an occurrence of a word in context .
we define a conditional random field for this task .
rush et al proposed a sentence summarization framework based on a neural attention model using a supervised sequence-to-sequence neural machine translation model .
most previous works addressing the task of bilingual lexicon extraction from comparable corpora are based on the standard approach .
the mt performance is measured with the widely adopted bleu and ter metrics .
sentiment analysis is the task of identifying the polarity ( positive , negative or neutral ) of review .
indeed , movies that fail the test tend to portray women as less-important and peripheral characters .
for the language model , we used sri language modeling toolkit to train a trigram model with modified kneser-ney smoothing on the 31 , 149 english sentences .
we measure translation performance by the bleu and meteor scores with multiple translation references .
the anaphor is a definite noun phrase and the referent is in focus , that is .
with reference to this system , we implement a data-driven parser with a neural classifier based on long short-term memory .
sentiment classification is a task of predicting sentiment polarity of text , which has attracted considerable interest in the nlp field .
text segmentation is the task of determining the positions at which topics change in a stream of text .
bunescu and mooney designed a kernel along the shortest dependency path between two entities by observing that the relation strongly relies on sdps .
we used trigram language models with interpolated kneser-kney discounting trained using the sri language modeling toolkit .
for parameter training we use conditional random fields as described in .
relation extraction is the task of finding semantic relations between two entities from text .
sentence compression is the task of producing a shorter form of a single given sentence , so that the new form is grammatical and retains the most important information of the original one ( cite-p-15-3-1 ) .
jiang and zhai proposed an instance re-weighting framework that handles both the settings .
we train a 4-gram language model on the xinhua portion of english gigaword corpus by srilm toolkit .
coreference resolution is a field in which major progress has been made in the last decade .
semantic role labeling ( srl ) has been defined as a sentence-level natural-language processing task in which semantic roles are assigned to the syntactic arguments of a predicate ( cite-p-14-1-7 ) .
table 1 shows the performance for the test data measured by case sensitive bleu .
moreover , xing et al incorporated topic words into seq2seq frameworks , where topic words are obtained from a pre-trained l-da model .
in all cases , we used the implementations from the scikitlearn machine learning library .
then , we applied 32k bpe operations , learned jointly over the source and target languages .
meanwhile , we propose a hierarchical attention mechanism for the bilingual lstm network .
we use the sri language model toolkit to train a 5-gram model with modified kneser-ney smoothing on the target-side training corpus .
sentiment analysis is a research area in the field of natural language processing .
dependency parsing is a basic technology for processing japanese and has been the subject of much research .
sentiment analysis ( sa ) is the task of analysing opinions , sentiments or emotions expressed towards entities such as products , services , organisations , issues , and the various attributes of these entities ( cite-p-9-3-3 ) .
conditional random fields are undirected graphical models of a conditional distribution .
translation performance was measured by case-insensitive bleu .
it can process raw text and data conforming to the format of the conll-2012 shared task on coreference resolution .
we employ srilm toolkit to linearly interpolate the target side of the training corpus with the wmt english corpus , optimizing towards the mt tuning set .
all systems are evaluated using case-insensitive bleu .
sentiment analysis is a fundamental problem aiming to give a machine the ability to understand the emotions and opinions expressed in a written text .
we further used a 5-gram language model trained using the srilm toolkit with modified kneser-ney smoothing .
however , these conclusions contradict yamashita claiming that information structure is not crucial for scrambling .
firstly , we built a forward 5-gram language model using the srilm toolkit with modified kneser-ney smoothing .
shen et al proposed a string-to-dependency model , which restricted the target-side of a rule by dependency structures .
we build all the classifiers using the l2-regularized linear logistic regression from the liblinear package .
then we use the standard minimum error-rate training to tune the feature weights to maximize the system潞s bleu score .
supertagging is a widely used speedup technique for lexicalized grammar parsing .
for our spanish experiments , we randomly sample 2 , 000 sentence pairs from the spanish-english europarl v5 parallel corpus .
relation extraction ( re ) is the task of determining semantic relations between entities mentioned in text .
we used a 5-gram language model trained on 126 million words of the xinhua section of the english gigaword corpus , estimated with srilm .
a 5-gram language model with kneser-ney smoothing was trained with srilm on monolingual english data .
ambiguity is a problem in any natural language processing system .
all the feature weights and the weight for each probability factor are tuned on the development set with minimumerror-rate training .
relation extraction is the task of detecting and characterizing semantic relations between entities from free text .
also , we compare our system with the rulebased system proposed by heilman and smith .
in this paper , we propose a novel forest reranking algorithm for dependency parsing .
we use scikitlearn as machine learning library .
we used the srilm toolkit to generate the scores with no smoothing .
we evaluated the translation quality using the bleu-4 metric .
hence , this model is similar to the skip-gram model in word embedding .
we used kenlm with srilm to train a 5-gram language model based on all available target language training data .
a tri-gram language model is estimated using the srilm toolkit .
blitzer et al used structural correspondence learning to train a classifier on source data with new features induced from target unlabeled data .
named entity recognition ( ner ) is a frequently needed technology in nlp applications .
we trained the statistical phrase-based systems using the moses toolkit with mert tuning .
our cdsm feature is based on word vectors derived using a skip-gram model .
we use a bidirectional long short-term memory rnn to encode a sentence .
we trained the embedding vectors with the word2vec tool on the large unlabeled corpus of clinical texts provided by the task organizers .
we use the open-source moses toolkit to build a phrase-based smt system trained on mostly msa data obtained from several ldc corpora including some limited da data .
we used the weka implementation of na茂ve bayes for this baseline nb system .
we used the penn wall street journal treebank as training and test data .
word sense disambiguation ( wsd ) is the nlp task that consists in selecting the correct sense of a polysemous word in a given context .
all language models were trained using the srilm toolkit .
word segmentation is the first step of natural language processing for japanese , chinese and thai because they do not delimit words by whitespace .
meanwhile , we adopt glove pre-trained word embeddings 5 to initialize the representation of input tokens .
the input to net are the pre-trained glove word embeddings of 300d trained on 840b tokens .
in this paper , an oov translation model is established based on the combination pattern of web mining and translation ranking .
like we used support vector machines via the classifier svmlight .
in a second baseline model , we also incorporate 300-dimensional glove word embeddings trained on wikipedia and the gigaword corpus .
twitter is a famous social media platform capable of spreading breaking news , thus most of rumour related research uses twitter feed as a basis for research .
we rely on the stanford parser , a treebank-trained statistical parser , for tokenization , part-of-speech tagging , and phrase-structure parsing .
for all experiments , we used a 4-gram language model with modified kneser-ney smoothing which was trained with the srilm toolkit .
we use the opensource moses toolkit to build a phrase-based smt system .
we used the support vector machine implementation from the liblinear library on the test sets and report the results in table 4 .
we introduce a discriminatively trained , globally normalized , log-linear variant of the lexical translation models proposed by cite-p-17-1-6 .
this model uses multilingual word embeddings trained using fasttext and aligned using muse .
we used the srilm toolkit to train a 4-gram language model on the english side of the training corpus .
our work is inspired by cite-p-31-3-9 who also use freebase as distant supervision source .
coreference resolution is the task of identifying all mentions which refer to the same entity in a document .
we use stanford log-linear partof-speech tagger to produce pos tags for the english side .
for english , we convert the ptb constituency trees to dependencies using the stanford dependency framework .
the well-known phrasebased translation model has significantly advanced the progress of smt by extending translation units from single words to phrases .
many words have multiple meanings , and the process of identifying the correct meaning , or sense of a word in context , is known as word sense disambiguation ( wsd ) .
to that end , we use the state-of-the-art phrase based statistical machine translation system moses .
for english , we convert the ptb constituency trees to dependencies using the stanford dependency framework .
the translation performance was measured using the bleu and the nist mt-eval metrics , and word error rate .
we use the collapsed tree formalism of the stanford dependency parser .
the composite kernel consists of two individual kernels : an entity kernel that allows for entity-related features and a convolution parse tree kernel that models syntactic information of relation examples .
we used the dataset made available by the workshop on statistical machine translation to train a german-english phrase-based system using the moses toolkit in a standard setup .
morpa is a fully implemented parser developed for use in a text-to-speech conversion system .
semantic role labeling ( srl ) is the process of assigning semantic roles to strings of words in a sentence according to their relationship to the semantic predicates expressed in the sentence .
further , we apply a 4-gram language model trained with the srilm toolkit on the target side of the training corpus .
we use the open source moses phrase-based mt system to test the impact of the preprocessing technique on translation quality .
in this paper , we focus on how to integrate glosses into a unified neural wsd system .
we use the word2vec cbow model with a window size of 5 and a minimum frequency of 5 to generate 200-dimensional vectors .
phrase-based statistical machine translation models have achieved significant improvements in translation accuracy over the original ibm word-based model .
wikipedia is a constantly evolving source of detailed information that could facilitate intelligent machines — if they are able to leverage its power .
maximum phrase length is set to 10 words and the parameters in the log-linear model are tuned by mert .
we implemented our method in a phrase-based smt system .
following lample et al , the character-based representation is constructed with a bi-lstm .
the experiments were conducted with the scikit-learn tool kit .
discourse parsing is the task of identifying the presence and the type of the discourse relations between discourse units .
we build all the classifiers using the l2-regularized linear logistic regression from the liblinear package .
a 4-grams language model is trained by the srilm toolkit .
then , we trained word embeddings using word2vec .
phonetic translation across these pairs is called transliteration .
relation extraction is the task of detecting and classifying relationships between two entities from text .
lda is a generative model that learns a set of latent topics for a document collection .
pang and lee use a graph-based technique to identify and analyze only subjective parts of texts .
exact non-projective parsing with such a 2-order model is intractable .
previous works on stance detection have focused on congressional debates , company-internal discussions , and debates in online forums .
the model parameters are trained using minimum error-rate training .
we use the sri language model toolkit to train a 5-gram model with modified kneser-ney smoothing on the target-side training corpus .
we work with the phrase-based smt framework as the baseline system .
we use a standard maximum entropy classifier implemented as part of mallet .
a 5-gram language model built using kenlm was used for decoding .
coreference resolution is a challenging task , that involves identification and clustering of noun phrases mentions that refer to the same real-world entity .
we initialize these word embeddings with glove vectors .
we tried the models with glove and with randomly initialized , learnable word embeddings .
we also used support vector machines and conditional random fields .
we measure the translation quality with automatic metrics including bleu and ter .
we used the svm implementation provided within scikit-learn .
in this paper , we have presented an fdt-based model training approach to smt .
non-interactive algorithms do not use human labels during the learning process .
we split each document into sentences using the sentence tokenizer of the nltk toolkit .
for the tree-based system , we applied a 4-gram language model with kneserney smoothing using srilm toolkit trained on the whole monolingual corpus .
semantic parsing is the problem of mapping natural language strings into meaning representations .
furthermore , we train a 5-gram language model using the sri language toolkit .
barzilay and mckeown extract paraphrases from a monolingual parallel corpus , containing multiple translations of the same source .
lei et al employ three-way tensors to obtain a low-dimensional input representation optimized for parsing performance .
we evaluated translation quality based on the caseinsensitive automatic evaluation score bleu-4 .
ccgs are a linguistically-motivated formalism for modeling a wide range of language phenomena .
sentence compression is the task of producing a shorter form of a single given sentence , so that the new form is grammatical and retains the most important information of the original one ( cite-p-15-3-1 ) .
we utilize minimum error rate training to optimize feature weights of the paraphrasing model according to ndcg .
we extract translation rules from a hypergraph for the hierarchical phrase-based system .
the parameter weights are optimized with minimum error rate training .
wikipedia is a large , multilingual , highly structured , multi-domain encyclopedia , providing an increasingly large wealth of knowledge .
we used a 5-gram language model with modified kneser-ney smoothing implemented using the srilm toolkit .
mikolov et al further proposed continuous bagof-words and skip-gram models , which use a simple single-layer architecture based on inner product between two word vectors .
each translation model is tuned using mert to maximize bleu .
we present the text to the encoder as a sequence of word2vec word embeddings from a word2vec model trained on the hrwac corpus .
in our work , we use latent dirichlet allocation to identify the sub-topics in the given body of texts .
plda is an extension of lda which is an unsupervised machine learning method that models topics of a document collection .
in our implementation , we train a tri-gram language model on each phone set using the srilm toolkit .
we initialize these word embeddings with glove vectors .
davidov et al describe a technique that transforms hashtags and smileys in tweets into sentiments .
negation is a grammatical category which comprises various kinds of devices to reverse the truth value of a proposition ( cite-p-18-3-8 ) .
the word embeddings are pre-trained by skip-gram .
the log linear weights for the baseline systems are optimized using mert provided in the moses toolkit .
shallow semantic representations , bearing a more compact information , could prevent the sparseness of deep structural approaches .
we trained one logistic regression classifier for each emotion class using the liblinear package .
the language model was constructed using the srilm toolkit with interpolated kneser-ney discounting .
the feature weights for each system were tuned on development sets using the moses implementation of minimum error rate training .
the translation quality is evaluated by case-insensitive bleu-4 .
in particular , we show that there are two distinct ways of representing the parse forest .
coreference resolution is the task of automatically grouping references to the same real-world entity in a document into a set .
we use state-of-the-art word embedding methods , namely continuous bag of words and global vectors .
a 4-grams language model is trained by the srilm toolkit .
convolutional neural networks have obtained good results in text classification , which usually consist of convolutional and pooling layers .
in this run , we use a sentence vector derived from word embeddings obtained from word2vec .
a pun is a form of wordplay in which a word suggests two or more meanings by exploiting polysemy , homonymy , or phonological similarity to another word , for an intended humorous or rhetorical effect ( cite-p-15-3-1 ) .
semantic parsing is the problem of translating human language into computer language , and therefore is at the heart of natural language understanding .
we implemented linear models with the scikit learn package .
automatic alignment can be performed using different algorithms such as the em algorithm or using an hmm aligner .
faruqui et al apply post-processing steps to existing word embeddings in order to bring them more in accordance with semantic lexicons such as ppdb and framenet .
we make use of the recently published word embeddings trained on google news .
within this subpart of our ensemble model , we used a svm model from the scikit-learn library .
similarity is a fundamental concept in theories of knowledge and behavior .
morphologically , arabic is a non-concatenative language .
sentiment analysis is a natural language processing ( nlp ) task ( cite-p-10-3-0 ) which aims at classifying documents according to the opinion expressed about a given subject ( federici and dragoni , 2016a , b ) .
sentiment classification is a well studied problem ( cite-p-13-3-6 , cite-p-13-1-14 , cite-p-13-3-3 ) and in many domains users explicitly provide ratings for each aspect making automated means unnecessary .
text segmentation can be defined as the automatic identification of boundaries between distinct textual units ( segments ) in a textual document .
gao et al do a pioneer work by describing a transformation-based converter to transfer a certain word segmentation result to another annotation guideline .
importantly , word embeddings have been effectively used for several nlp tasks , such as named entity recognition , machine translation and part-of-speech tagging .
these word embeddings are learned in advance using a continuous skip-gram model , or other continuous word representation learning methods .
this model shows a significant improvement over the state-of-the-art hierarchical phrase-based system .
we trained the statistical phrase-based systems using the moses toolkit with mert tuning .
we train a 5-gram language model with the xinhua portion of english gigaword corpus and the english side of the training set using the srilm toolkit .
relation extraction is the task of finding semantic relations between two entities from text .
we used the moses toolkit to build an english-hindi statistical machine translation system .
heilman et al continued using language modeling to predict readability for first and second language texts .
word embedding approaches like word2vec or glove are powerful tools for the semantic analysis of natural language .
we use the moses mt framework to build a standard statistical phrase-based mt model using our old-domain training data .
for both languages , we used the srilm toolkit to train a 5-gram language model using all monolingual data provided .
additionally , we compile the model using the adamax optimizer .
al-onaizan and knight proposed a spelling-based model which directly maps english letter sequences into arabic letter sequences .
our nmt is based on an encoderdecoder with attention design , using bidirectional lstm layers for encoding and unidirectional layers for decoding .
twitter is a popular microblogging service , which , among other things , is used for knowledge sharing among friends and peers .
moreover , since event coreference resolution is a complex task that involves exploring a rich set of linguistic features , annotating a large corpus with event coreference information for a new language or domain of interest requires a substantial amount of manual effort .
for the language model , we used sri language modeling toolkit to train a trigram model with modified kneser-ney smoothing on the 31 , 149 english sentences .
the srilm toolkit was used to build the 5-gram language model .
finally , we conduct paired bootstrap sampling to test the significance in bleu scores differences .
the most commonly used word embeddings were word2vec and glove .
we use stanford corenlp for pos tagging and lemmatization .
we obtain word clusters from word2vec k-means word clustering tool .
the language model was a 5-gram model with kneser-ney smoothing trained on the monolingual news corpus with irstlm .
a 5-gram language model with kneser-ney smoothing is trained using s-rilm on the target language .
our word embeddings is initialized with 100-dimensional glove word embeddings .
our phrase-based mt system is trained by moses with standard parameters settings .
central to our approach is a new type-based sampling algorithm for hierarchical pitman-yor models in which we track fractional table counts .
word sense disambiguation ( wsd ) is the task of determining the correct meaning or sense of a word in context .
recent efforts in statistical machine translation have seen promising improvements in output quality , especially the phrase-based models and syntax-based models .
our neural machine translation systems are trained using a modified version of opennmt-py .
for all classifiers , we used the scikit-learn implementation .
kalchbrenner et al show that a cnn for modeling sentences can achieve competitive results in polarity classification .
weights are optimized by the gradient-based adagrad algorithm with a mini-batch .
semantic role labeling ( srl ) is the task of identifying the arguments of lexical predicates in a sentence and labeling them with semantic roles ( cite-p-13-3-3 , cite-p-13-3-11 ) .
the decoding weights were optimized with minimum error rate training .
named entity recognition ( ner ) is a fundamental task in text mining and natural language understanding .
inversion transduction grammar is a synchronous grammar for synchronous parsing of source and target language sentences .
we use the glove vectors of 300 dimension to represent the input words .
conditional random fields are undirected graphical models represented as factor graphs .
relation classification is the task of identifying the semantic relation present between a given pair of entities in a piece of text .
we employ the pretrained word vector , glove , to obtain the fixed word embedding of each word .
our method of morphological analysis comprises a morpheme lexicon .
we train the parameters of the stages separately using adagrad with the perceptron loss function .
transliteration is often defined as phonetic translation ( cite-p-21-3-2 ) .
we use a conditional random field sequence model , which allows for globally optimal training and decoding .
socher et al used recursive neural networks to model sentences for different tasks , including paraphrase detection and sentence classification .
sentiment analysis is a collection of methods and algorithms used to infer and measure affection expressed by a writer .
for this purpose , we used phrase tables learned by the standard statistical mt toolkit moses .
twitter is a popular microblogging service , which , among other things , is used for knowledge sharing among friends and peers .
finally , based on recent results in text classification , we also experiment with a neural network approach which uses a long-short term memory network .
the 5-gram kneser-ney smoothed language models were trained by srilm , with kenlm used at runtime .
collobert et al propose a multi-task learning framework with dnn for various nlp tasks , including part-of-speech tagging , chunking , named entity recognition , and semantic role labelling .
we employ srilm toolkit to linearly interpolate the target side of the training corpus with the wmt english corpus , optimizing towards the mt tuning set .
negation is a complex phenomenon present in all human languages , allowing for the uniquely human capacities of denial , contradiction , misrepresentation , lying , and irony ( cite-p-18-3-7 ) .
xing et al presented topic aware response generation by incorporating topic words obtained from a pre-trained lda model .
this system is a basic encoderdecoder with an attention mechanism .
table 1 shows the performance for the test data measured by case sensitive bleu .
luong and manning also propose an hybrid word-character model to handle the rare word problem .
in all submitted systems , we use the phrase-based moses decoder .
language models used modified kneserney smoothing estimated using kenlm .
neural machine translation using sequence to sequence architectures has become the dominant approach to automatic machine translation .
we used srilm to build a 4-gram language model with interpolated kneser-ney discounting .
relation extraction is the task of detecting and characterizing semantic relations between entities from free text .
wordnet is a general english thesaurus which additionally covers biological terms .
for both languages , we used the srilm toolkit to train a 5-gram language model using all monolingual data provided .
we use the word2vec framework in the gensim implementation to generate the embedding spaces .
previous work consistently reported that the wordbased translation models yielded better performance than the traditional methods for question retrieval .
tanev and magnini proposed a weaklysupervised method that requires as training data a list of terms without context for each category under consideration .
we created 5-gram language models for every domain using srilm with improved kneserney smoothing on the target side of the training parallel corpora .
it has been shown that word embeddings are able to capture to certain semantic and syntactic aspects of words .
we use 300 dimension word2vec word embeddings for the experiments .
to do this , we relied on a neural network with a long short-term memory layer , which is fed from the word embeddings .
lexical simplification is the task of modifying the lexical content of complex sentences in order to make them simpler .
we exploit the svm-light-tk toolkit for kernel computation .
an effective solution for these problems is the long short-term memory architecture .
we used the sri language modeling toolkit to train a fivegram model with modified kneser-ney smoothing .
dependency parsing is a core task in nlp , and it is widely used by many applications such as information extraction , question answering , and machine translation .
for part of speech tagging and dependency parsing of the text , we used the toolset from stanford corenlp .
we use minimal error rate training to maximize bleu on the complete development data .
we trained kneser-ney discounted 5-gram language models on each available corpus using the srilm toolkit .
in addition to that we use pre-trained embeddings , by training word2vec skip-gram model on wikipedia texts .
our model is a first order linear chain conditional random field .
sun and xu enhanced the segmentation results by interpolating the statistics-based features derived from unlabeled data to a crfs model .
we use the adam optimizer and mini-batch gradient to solve this optimization problem .
coreference resolution is the task of clustering a set of mentions in the text such that all mentions in the same cluster refer to the same entity .
for the feature-based system we used logistic regression classifier from the scikit-learn library .
in this paper , we proposed a sentiment aligned topic model ( satm ) for product aspect rating prediction .
results are reported on two standard metrics , nist and bleu , on lower-cased data .
coreference resolution is a set partitioning problem in which each resulting partition refers to an entity .
for all classifiers , we used the scikit-learn implementation .
we used the srilm toolkit to create 5-gram language models with interpolated modified kneser-ney discounting .
we use srilm for training a trigram language model on the english side of the training corpus .
word sense disambiguation ( wsd ) is the task of determining the correct meaning or sense of a word in context .
word alignment is a key component in most statistical machine translation systems .
stemming is a popular way to reduce the size of a vocabulary in natural language tasks by conflating words with related meanings .
relation extraction is the task of recognizing and extracting relations between entities or concepts in texts .
we use the adam optimizer for the gradient-based optimization .
we estimated 5-gram language models using the sri toolkit with modified kneser-ney smoothing .
luong and manning proposed a hybrid scheme that consults character-level information whenever the model encounters an oov word .
table 1 shows the evaluation of all the systems in terms of bleu score with the best score highlighted .
we used the penn treebank wsj corpus to perform empirical experiments on the proposed parsing models .
for this purpose , we used phrase tables learned by the standard statistical mt toolkit moses .
in this paper , we propose a new model that is capable of recognizing overlapping mentions .
bilingual lexicons are fundamental resources in multilingual natural language processing tasks such as machine translation , cross-language information retrieval or computerassisted translation .
for our purpose we use word2vec embeddings trained on a google news dataset and find the pairwise cosine distances for all words .
hassan and menezes proposed an approach based on the random walk algorithm on a contextual similarity bipartite graph , constructed from n-gram sequences on a large unlabeled text corpus .
the model weights of all systems have been tuned with standard minimum error rate training on a concatenation of the newstest2011 and newstest2012 sets .
for this reason , we used glove vectors to extract the vector representation of words .
the quality of translations is evaluated by the case insensitive nist bleu-4 metric .
bengio et al presented a neural network language model where word embeddings are simultaneously learned along with a language model .
the evaluation method is the case insensitive ibm bleu-4 .
li and yarowsky proposed an unsupervised method for extracting the mappings from chinese abbreviations and their full-forms .
evaluation is done using the bleu metric with four references .
dependency parsing is the task of predicting the most probable dependency structure for a given sentence .
we build a 9-gram lm using srilm toolkit with modified kneser-ney smoothing .
transition-based methods have become a popular approach in multilingual dependency parsing because of their speed and performance .
recently , large corpora have been manually annotated with semantic roles in framenet and propbank .
thus , we train a 4-gram language model based on kneser-ney smoothing method using sri toolkit and interpolate it with the best rnnlms by different weights .
richman and schone use article classification knowledge from english wikipedia to produce ne-annotated corpora in other languages .
we use moses , an open source toolkit for training different systems .
we further used a 5-gram language model trained using the srilm toolkit with modified kneser-ney smoothing .
for the cluster- based method , we use word2vec 2 which provides the word vectors trained on the google news corpus .
part-of-speech ( pos ) tagging is a fundamental nlp task , used by a wide variety of applications .
we use srilm for n-gram language model training and hmm decoding .
the trigram language model is implemented in the srilm toolkit .
the continuous bag-of-words approach described by mikolov et al is learned by predicting the word vector based on the context vectors .
high quality word embeddings have been proven helpful in many nlp tasks .
we use the scikit-learn machine learning library to implement the entire pipeline .
in addition , a 5-gram lm with kneser-ney smoothing and interpolation was built using the srilm toolkit .
for the language model , we used sri language modeling toolkit to train a trigram model with modified kneser-ney smoothing on the 31 , 149 english sentences .
neural machine translation is currently the state-of-the art paradigm for machine translation .
we use scikitlearn as machine learning library .
we used moses with the default configuration for phrase-based translation .
event extraction is a particularly challenging type of information extraction ( ie ) .
riedel et al proposed to use multi-instance learning to tolerate noise in the positively-labeled data .
aspect extraction is a task to abstract the common properties of objects from corpora discussing them , such as reviews of products .
word alignment is the task of identifying corresponding words in sentence pairs .
we use glove word embeddings , which are 50-dimension word vectors trained with a crawled large corpus with 840 billion tokens .
we use the 200-dimensional global vectors , pre-trained on 2 billion tweets , covering over 27-billion tokens .
many words have multiple meanings , and the process of identifying the correct meaning , or sense of a word in context , is known as word sense disambiguation ( wsd ) .
coreference resolution is the task of identifying all mentions which refer to the same entity in a document .
for all classifiers , we used the scikit-learn implementation .
we also use a 4-gram language model trained using srilm with kneser-ney smoothing .
a trigram model was built on 20 million words of general newswire text , using the srilm toolkit .
we learn our word embeddings by using word2vec 3 on unlabeled review data .
cussens and pulman describe a symbolic approach which employs inductive logic programming and barg and walther and fouvry follow a unification-based approach .
we use a pbsmt model built with the moses smt toolkit .
relation extraction ( re ) is the task of assigning a semantic relationship between a pair of arguments .
coreference resolution is a task aimed at identifying phrases ( mentions ) referring to the same entity .
in the early part of the last decade , phrase-based machine translation emerged as the preeminent design of statistical mt systems .
we further used a 5-gram language model trained using the srilm toolkit with modified kneser-ney smoothing .
coreference resolution is the process of linking together multiple referring expressions of a given entity in the world .
in the second stage , we use this assumption that a word and its translation tend to appear in similar context across languages .
we use the word2vec vectors with 300 dimensions , pre-trained on 100 billion words of google news .
dependency parsing is a simpler task than constituent parsing , since dependency trees do not have extra non-terminal nodes and there is no need for a grammar to generate them .
for the feature-based system we used logistic regression classifier from the scikit-learn library .
we train our acoustic models by kaldi speech recognition toolkit .
coreference resolution is a challenging task , that involves identification and clustering of noun phrases mentions that refer to the same real-world entity .
zeng et al propose the use of position feature for improving the performance of cnn in relation classification .
a more linguistically-informed approach to n-gram models is the factored language model approach of bilmes and kirchhoff .
word alignment is a critical first step for building statistical machine translation systems .
we use word2vec to train the word embeddings .
we used svm-light-tk , which enables the use of the partial tree kernel .
in all cases , we used the implementations from the scikitlearn machine learning library .
coreference resolution is a task aimed at identifying phrases ( mentions ) referring to the same entity .
translation results are evaluated using the word-based bleu score .
in this paper we proposed a new parsing algorithm based on a branch and bound framework .
in this paper , we propose a novel method that is based on text distortion to compress topic-related information .
we implement an in-domain language model using the sri language modeling toolkit .
mikolov et al showed that meaningful syntactic and semantic regularities can be captured in pre-trained word embedding .
for word-level embedding e w , we utilize pre-trained , 300-dimensional embedding vectors from glove 6b .
the release of the penn discourse treebank has advanced the development of english discourse relation recognition .
we used 100 dimensional glove embeddings for this purpose .
a 4-grams language model is trained by the srilm toolkit .
barman et al addressed the problem of language identification on bengali-hindi-english facebook comments .
the anaphor is a definite noun phrase and the referent is in focus , that is .
for evaluation , we use the dataset from the semeval-2007 lexical substitution task .
the target language model was a trigram language model with modified kneser-ney smoothing trained on the english side of the bitext using the srilm tookit .
incometo select the most fluent path , we train a 5-gram language model with the srilm toolkit on the english gigaword corpus .
in the experiments reported here we use support vector machines through the svm light package .
we used the phrase-based model moses for the experiments with all the standard settings , including a lexicalized reordering model , and a 5-gram language model .
semantic parsing is the task of translating text to a formal meaning representation such as logical forms or structured queries .
we use the moses software package 5 to train a pbmt model .
more recently , neural networks have become prominent in word representation learning .
a pun is the exploitation of the various meanings of a word or words with phonetic similarity but different meanings .
furthermore , we train a 5-gram language model using the sri language toolkit .
we train the model using the adam optimizer with the default hyper parameters .
therefore , word segmentation is a crucial first step for many chinese language processing tasks such as syntactic parsing , information retrieval and machine translation .
we measure the translation quality using a single reference bleu .
for the classification task , we use pre-trained glove embedding vectors as lexical features .
we report decoding speed and bleu score , as measured by sacrebleu .
we calculate cosine similarity using pretrained glove word vectors 7 to find similar words to the seed word .
we also used pre-trained word embeddings , including glove and 300d fasttext vectors .
for this , we utilize the publicly available glove 1 word embeddings , specifically ones trained on the common crawl dataset .
coreference resolution is a set partitioning problem in which each resulting partition refers to an entity .
finally , we construct new subtree-based features for parsing algorithms .
the language model was constructed using the srilm toolkit with interpolated kneser-ney discounting .
lda is a generative model that learns a set of latent topics for a document collection .
we use the cube pruning method to approximately intersect the translation forest with the language model .
we used an in-house implementation of the hierarchical phrase-based decoder as described in chiang .
descriptions are transformed into a vector by adding the corresponding word2vec embeddings .
our empirical results on three nlp tasks show that incorporating and exploiting more information from the target domain through instance weighting is effective .
these supervised learning methods are implemented in scikit-learn toolkit .
the language models were interpolated kneser-ney discounted trigram models , all constructed using the srilm toolkit .
we use pre-trained 100 dimensional glove word embeddings .
we used a 5-gram language model with modified kneser-ney smoothing , built with the srilm toolkit .
word alignment is the task of identifying translational relations between words in parallel corpora , in which a word at one language is usually translated into several words at the other language ( fertility model ) ( cite-p-18-1-0 ) .
we created 5-gram language models for every domain using srilm with improved kneserney smoothing on the target side of the training parallel corpora .
the srilm toolkit was used for training the language models using kneser-ney smoothing .
for the fst representation , we used the the opengrm-ngram language modeling toolkit and used an n-gram order of 4 , with kneser-ney smoothing .
turney and littman determined the polarity of sentiment words by estimating the point-wise mutual information between sentiment words and a set of seed words with strong polarity .
table 4 shows end-to-end translation bleu score results .
we used a 5-gram language model with modified kneser-ney smoothing implemented using the srilm toolkit .
translation performance was measured by case-insensitive bleu .
among these models , neural variants of the conditional random fields model are especially popular .
we use the sri language model toolkit to train a 5-gram model with modified kneser-ney smoothing on the target-side training corpus .
the evaluation metric for the overall translation quality is caseinsensitive bleu4 .
we follow the standard machine translation procedure of evaluation , measuring bleu for every system .
the bleu score measures the precision of n-grams with respect to a reference translation with a penalty for short translations .
we perform named entity tagging using the stanford four-class named entity tagger .
we use pre-trained glove vector for initialization of word embeddings .
sentiment analysis is the task in natural language processing ( nlp ) that deals with classifying opinions according to the polarity of the sentiment they express .
in this paper , we propose a paraphrasing model to address the task of system combination for machine translation .
we use srilm to train a 5-gram language model on the target side of our training corpus with modified kneser-ney discounting .
for cos , we used the cbow model 6 of word2vec .
our 5-gram language model was trained by srilm toolkit .
stance detection is the task of automatically determining from text whether the author of the text is in favor of , against , or neutral towards a proposition or target .
semantic parsing is the task of mapping natural language sentences to complete formal meaning representations .
we used a 5-gram language model with modified kneser-ney smoothing implemented using the srilm toolkit .
language models were estimated using the sri language modeling toolkit with modified kneser-ney smoothing .
we use a standard lstm-based bidirectional encoder-decoder architecture with global attention .
a 4-grams language model is trained by the srilm toolkit .
the language model was a kneser-ney interpolated trigram model generated using the srilm toolkit .
we initialize the embedding weights by the pre-trained word embeddings with 200 dimensional vectors .
sentiment analysis is a natural language processing ( nlp ) task ( cite-p-10-1-14 ) which aims at classifying documents according to the opinion expressed about a given subject ( federici and dragoni , 2016a , b ) .
the language model is a trigram model with modified kneser-ney discounting and interpolation .
a pseudo-word is the concatenation of two words ( e.g . house/car ) .
for classification , we use a maximum entropy model , from the logistic regression package in weka , with all default parameter settings .
lei et al proposed to learn features by representing the cross-products of some primitive units with low-rank tensors for dependency parsing .
the idea of extracting features for nlp using convolutional dnn was previously explored by collobert et al , in the context of pos tagging , chunking , named entity recognition and semantic role labeling .
the srilm toolkit was used to build the 5-gram language model .
we estimated 5-gram language models using the sri toolkit with modified kneser-ney smoothing .
coreference resolution is the task of determining when two textual mentions name the same individual .
we used srilm to build a 4-gram language model with interpolated kneser-ney discounting .
ji and grishman extended the one sense per discourse idea to multiple topically related documents and propagate consistent event arguments across sentences and documents .
relation extraction is the task of finding relations between entities in text , which is useful for several tasks such as information extraction , summarization , and question answering ( cite-p-14-3-7 ) .
the attention strategies have been widely used in machine translation and question answering .
sentiment analysis is a natural language processing ( nlp ) task ( cite-p-10-1-14 ) which aims at classifying documents according to the opinion expressed about a given subject ( federici and dragoni , 2016a , b ) .
coreference resolution is the task of partitioning a set of entity mentions in a text , where each partition corresponds to some entity in an underlying discourse model .
among them , twitter is the most popular service by far due to its ease for real-time sharing of information .
coreference resolution is the task of clustering a set of mentions in the text such that all mentions in the same cluster refer to the same entity .
sentence compression is the task of producing a shorter form of a single given sentence , so that the new form is grammatical and retains the most important information of the original one .
we train skip-gram word embeddings with the word2vec toolkit 1 on a large amount of twitter text data .
this baseline has been previously used as a point of comparison by other unsupervised semantic role induction systems and shown difficult to outperform .
the conll 2008-2009 shared tasks introduced a variant where semantic dependencies are annotated rather than phrasal arguments .
unfortunately , wordnet is a fine-grained resource , encoding sense distinctions that are often difficult to recognize even for human annotators ( cite-p-15-1-6 ) .
more recently , neural networks have become prominent in word representation learning .
we use the logistic regression implementation of liblinear wrapped by the scikit-learn library .
following , we minimize the objective by the diagonal variant of adagrad with minibatchs .
word sense disambiguation ( wsd ) is a problem of finding the relevant clues in a surrounding context .
the log-linear parameter weights are tuned with mert on the development set .
monolingual word embeddings have facilitated advances in many natural language processing tasks , such as natural language understanding , sentiment analysis , and dependency parsing .
faruqui et al apply post-processing steps to existing word embeddings in order to bring them more in accordance with semantic lexicons such as ppdb and framenet .
we use the moses toolkit to train our phrase-based smt models .
we use 5-gram models with modified kneser-ney smoothing and interpolated back-off .
in this paper , we propose a novel and effective approach to sentiment analysis on product reviews .
also called deep learning , such approaches have recently been applied in a number of nlp tasks .
we implement the pbsmt system with the moses toolkit .
a 5-gram language model was created with the sri language modeling toolkit and trained using the gigaword corpus and english sentences from the parallel data .
coreference resolution is the task of identifying all mentions which refer to the same entity in a document .
the language models were interpolated kneser-ney discounted trigram models , all constructed using the srilm toolkit .
we observe that satirical cues are often reflected in certain paragraphs rather than the whole document .
we report decoding speed and bleu score , as measured by sacrebleu .
automatic evaluation results are shown in table 1 , using bleu-4 .
li et al presented a structured perceptron model to detect triggers and arguments jointly .
korhonen et al used verb-frame pairs to cluster verbs relying on the information bottleneck .
for the classifiers we use the scikit-learn machine learning toolkit .
here , we choose the skip-gram model and continuous-bag-of-words model for comparison with the lbl model .
we apply sri language modeling toolkit to train a 4-gram language model with kneser-ney smoothing .
therefore , word segmentation is a preliminary and important preprocess for chinese language processing .
coreference resolution is the task of determining when two textual mentions name the same individual .
for the feature-based system we used logistic regression classifier from the scikit-learn library .
the benchmark model for topic modelling is latent dirichlet allocation , a latent variable model of documents .
for english , we convert the ptb constituency trees to dependencies using the stanford dependency framework .
gram language models are trained over the target-side of the training data , using srilm with modified kneser-ney discounting .
these results demonstrate that this model benefits greatly from the inclusion of long-range dependencies .
in this paper , we propose a broad-coverage normalization system for the social media language without using the human annotations .
hiero is a hierarchical phrase-based statistical mt framework that generalizes phrase-based models by permitting phrases with gaps .
for the mix one , we also train word embeddings of dimension 50 using glove .
simulating the approach reported by , we trained a support vector machine for regression with rbf kernel using scikit-learn with the set of features .
semantic parsing is the task of translating natural language utterances into a machine-interpretable meaning representation .
dependency parsing is a valuable form of syntactic processing for nlp applications due to its transparent lexicalized representation and robustness with respect to flexible word order languages .
coreference resolution is the process of linking together multiple expressions of a given entity .
we obtained parse trees using the stanford parser , and used jacana for word alignment .
we use byte-pair-encoding to achieve openvocabulary translation with a fixed vocabulary of subword symbols .
we use a minibatch stochastic gradient descent algorithm together with an adagrad optimizer .
we use bleu , rouge , and meteor scores as automatic evaluation metrics .
the word vectors of vocabulary words are trained from a large corpus using the glove toolkit .
the weights of the different feature functions were optimised by means of minimum error rate training .
we use sri language model toolkit to train a 5-gram model with modified kneser-ney smoothing on the target-side training corpus .
twitter consists of a massive number of posts on a wide range of subjects , making it very interesting to extract information and sentiments from them .
we use word2vec from as the pretrained word embeddings .
zens and ney show that itg constraints allow a higher flexibility in word ordering for longer sentences than the conventional ibm model .
coreference resolution is a well known clustering task in natural language processing .
for annotation tasks , snow et al showed that crowdsourced annotations are similar to traditional annotations made by experts .
semantic parsing is the problem of deriving a structured meaning representation from a natural language utterance .
we employ the pretrained word vector , glove , to obtain the fixed word embedding of each word .
he and parket attempted to find bursts , periods of elevated occurrence of events as a dynamic phenomenon instead of focusing on arrival rates .
sentiment analysis is a natural language processing task whose aim is to classify documents according to the opinion ( polarity ) they express on a given subject ( cite-p-13-8-14 ) .
phrase-based translation systems prove to be the stateof-the-art as they have delivered translation performance in recent machine translation evaluations .
we use minimum error rate training with nbest list size 100 to optimize the feature weights for maximum development bleu .
we initialize the word embeddings for our deep learning architecture with the 100-dimensional glove vectors .
the standard minimum error rate training algorithm was used for tuning .
we train the word embeddings through using the training and developing sets of each dataset with word2vec tool .
we estimated 5-gram language models using the sri toolkit with modified kneser-ney smoothing .
the target fourgram language model was built with the english part of training data using the sri language modeling toolkit .
for dmcnn , following the settings of previous work , we use the pre-trained word embeddings learned by skip-gram as the initial word embeddings .
we extract the named entities from the web pages using the stanford named entity recognizer .
we used a standard pbmt system built using moses toolkit .
semantic role labeling ( srl ) is the process of assigning semantic roles to strings of words in a sentence according to their relationship to the semantic predicates expressed in the sentence .
coreference resolution is the process of linking together multiple expressions of a given entity .
we use the glove pre-trained word embeddings for the vectors of the content words .
we used the moses decoder , with default settings , to obtain the translations .
taxonomies are useful tools for content organisation , navigation , and retrieval , providing valuable input for semantically intensive tasks such as question answering and textual entailment .
mihalcea et al use both corpusbased and knowledge-based measures of the semantic similarity between words .
our baseline system is based on a hierarchical phrase-based translation model , which can formally be described as a synchronous context-free grammar .
nenkova et al noted that the entrainment score between dialogue partners is higher than the entrainment score between non-partners in dialogue .
for the word-embedding based classifier , we use the glove pre-trained word embeddings .
we apply the rules to each sentence with its dependency tree structure acquired from the stanford parser .
we feed our features to a multinomial naive bayes classifier in scikit-learn .
coreference resolution is the task of identifying all mentions which refer to the same entity in a document .
word sense disambiguation ( wsd ) is a difficult natural language processing task which requires that for every content word ( noun , adjective , verb or adverb ) the appropriate meaning is automatically selected from the available sense inventory 1 .
in 2003 , bengio et al proposed a neural network architecture to train language models which produced word embeddings in the neural network .
a 4-gram language model was trained on the target side of the parallel data using the srilm toolkit .
for training and evaluating the itsg parser , we employ the penn wsj treebank .
we initialize the embedding weights by the pre-trained word embeddings with 200 dimensional vectors .
many words have multiple meanings , and the process of identifying the correct meaning , or sense of a word in context , is known as word sense disambiguation ( wsd ) .
a 4-gram language model was trained on the monolingual data by the srilm toolkit .
the log linear weights for the baseline systems are optimized using mert provided in the moses toolkit .
semantic role labeling ( srl ) is a task of automatically identifying semantic relations between predicate and its related arguments in the sentence .
as a classifier , we employ support vector machines as implemented in svm light .
the continuous bag-of-words approach described by mikolov et al is learned by predicting the word vector based on the context vectors .
for the image labels , we use the representation of the last layer of the vgg neural network .
pitler et al demonstrated that features developed to capture word polarity , verb classes and orientation , as well as some lexical features are strong indicator of the type of discourse relation .
twitter is a popular microblogging service which provides real-time information on events happening across the world .
the phrase-based translation systems rely on language model and lexicalized reordering model to capture lexical dependencies that span phrase boundaries .
for example , turian et al have improved the performance of chunking and named entity recognition by using word embedding also as one of the features in their crf model .
to measure the translation quality , we use the bleu score and the nist score .
for the language model , we used sri language modeling toolkit to train a trigram model with modified kneser-ney smoothing on the 31 , 149 english sentences .
the log-linear feature weights are tuned with minimum error rate training on bleu .
we used a phrase-based smt model as implemented in the moses toolkit .
we apply the 3-phase learning procedure proposed by where we first create word embeddings based on the skip-gram model .
semantic parsing is the task of transducing natural language ( nl ) utterances into formal meaning representations ( mrs ) , commonly represented as tree structures .
dependency parses are obtained from the stanford parser .
we use our reordering model for n-best re-ranking and optimize bleu using minimum error rate training .
this paper describes a simple pattern-matching algorithm for post-processing the output of such parsers to add a wide variety of empty nodes to its parse trees .
translation quality is measured by case-insensitive bleu on newstest13 using one reference translation .
for tuning the feature weights , we applied batch-mira with -safe-hope .
the translation quality is evaluated by case-insensitive bleu-4 .
ji and grishman extended the one sense per discourse idea to multiple topically related documents and propagate consistent event arguments across sentences and documents .
much current work in discourse parsing focuses on the labelling of discourse relations , using data from the penn discourse treebank .
we use the glove pre-trained word embeddings for the vectors of the content words .
we use liblinear logistic regression module to classify document-level embeddings .
the conll dataset is taken form the wall street journal portion of the penn treebank corpus .
semantic parsing is the task of converting natural language utterances into their complete formal meaning representations which are executable for some application .
in addition , we build another word alignment model for l1 and l2 using the small l1-l2 bilingual corpus .
in clark and curran we describe efficient methods for performing the calculations using packed charts .
this task focuses only on the hypernym-hyponym relation extraction from a list of terms collected from various domains and languages .
for the word-embedding based classifier , we use the glove pre-trained word embeddings .
many words have multiple meanings , and the process of identifying the correct meaning , or sense of a word in context , is known as word sense disambiguation ( wsd ) .
document summarization is a task to generate a fluent , condensed summary for a document , and keep important information .
for the language model , we used sri language modeling toolkit to train a trigram model with modified kneser-ney smoothing on the 31 , 149 english sentences .
the core machinery of our system is driven by a latent dirichlet allocation topic model .
relation extraction ( re ) is the task of extracting instances of semantic relations between entities in unstructured data such as natural language text .
word sense disambiguation ( wsd ) is a fundamental task and long-standing challenge in natural language processing ( nlp ) .
the evaluations were performed with scikit-learn using the skll toolkit 6 that makes it easy to run batch scikit-learn experiments .
we used the target side of the parallel corpus and the srilm toolkit to train a 5-gram language model .
grammar induction is the task of learning a grammar from a set of unannotated sentences .
coreference resolution is the task of determining when two textual mentions name the same individual .
furthermore , we train a 5-gram language model using the sri language toolkit .
we use 300-dimensional vectors that were trained and provided by word2vec tool using a part of the google news dataset 4 .
we used the srilm toolkit to create 5-gram language models with interpolated modified kneser-ney discounting .
furthermore , we train a 5-gram language model using the sri language toolkit .
we report decoding speed and bleu score , as measured by sacrebleu .
social media is a rich source of rumours and corresponding community reactions .
transliteration is often defined as phonetic translation ( cite-p-21-3-2 ) .
these features were optimized using minimum error-rate training and the same weights were then used in docent .
dependency parsing is the task of predicting the most probable dependency structure for a given sentence .
word embeddings are critical for high-performance neural networks in nlp tasks .
in this paper we present a new , publicly available corpus for context-dependent semantic parsing .
semantic parsing is the task of translating natural language utterances into a machine-interpretable meaning representation .
as our supervised classification algorithm , we use a linear svm classifier from liblinear , with its default parameter settings .
word sense disambiguation ( wsd ) is the task of assigning sense tags to ambiguous lexical items ( lis ) in a text .
we compute the interannotator agreement in terms of the bleu score .
we applied liblinear via its scikitlearn python interface to train the logistic regression model with l2 regularization .
the decoding weights were optimized with minimum error rate training .
the earliest approach in used edit distance based multiple string alignment to build the confusion networks .
we build all the classifiers using the l2-regularized linear logistic regression from the liblinear package .
word sense disambiguation ( wsd ) is the nlp task that consists in selecting the correct sense of a polysemous word in a given context .
a notable component of our extension is that we introduce a training algorithm for learning a hidden unit crf of maaten et al from partially labeled sequences .
coreference resolution is the process of linking together multiple expressions of a given entity .
semantic parsing is the task of converting a sentence into a representation of its meaning , usually in a logical form grounded in the symbols of some fixed ontology or relational database ( cite-p-21-3-3 , cite-p-21-3-4 , cite-p-21-1-11 ) .
we trained a 4-gram language model on this data with kneser-ney discounting using srilm .
we have used the srilm with kneser-ney smoothing for training a language model for the first stage of decoding .
word sense disambiguation ( wsd ) is a task to identify the intended sense of a word based on its context .
in order to reduce the source vocabulary size translation , the german text was preprocessed by splitting german compound words with the frequencybased method described in .
in this paper , we proposed a way to extend the size of the target vocabulary for neural machine translation .
we apply our model to the english portion of the conll 2012 shared task data , which is derived from the ontonotes corpus .
we used moses , a phrase-based smt toolkit , for training the translation model .
distributed word representations have been shown to improve the accuracy of ner systems .
we used the srilm toolkit and kneser-ney discounting for estimating 5-grams lms .
event coreference resolution is the task of determining which event mentions expressed in language refer to the same real-world event instances .
to do this we examine the dataset created for the english lexical substitution task in semeval .
word embeddings have shown promising results in nlp tasks , such as named entity recognition , sentiment analysis or parsing .
we evaluated the translation quality of the system using the bleu metric .
we used the svd implementation provided in the scikit-learn toolkit .
due to the success of word embeddings in word similarity judgment tasks , this work also makes use of global vector word embeddings .
case-insensitive bleu-4 is our evaluation metric .
qiu et al propose double propagation to expand opinion targets and opinion words lists in a bootstrapping way .
we experimentally evaluate the heldout perplexity of models trained with our various importance sampling distributions .
as the encoder for text we consider convolutional neural networks , gated recurrent units , and long short-term memory networks .
for this model , we use a binary logistic regression classifier implemented in the lib-linear package , coupled with the ovo scheme .
for both languages , we used the srilm toolkit to train a 5-gram language model using all monolingual data provided .
the srilm toolkit was used to build this language model .
relation extraction is a core task in information extraction and natural language understanding .
results are reported using case-insensitive bleu with a single reference .
in this work we study the use of semantic frames for modelling argumentation in speakers ’ discourse .
we set the feature weights by optimizing the bleu score directly using minimum error rate training on the development set .
based on the distributional hypothesis , we train a skip-gram model to learn the distributional representations of words in a large corpus .
named entity recognition ( ner ) is a frequently needed technology in nlp applications .
examples of such schemas include freebase and yago2 .
ner is the task of identifying names in text and assigning them a type ( e.g . person , location , organisation , miscellaneous ) .
the targetside 4-gram language model was estimated using the srilm toolkit and modified kneser-ney discounting with interpolation .
within this subpart of our ensemble model , we used a svm model from the scikit-learn library .
we use moses , an open source toolkit for training different systems .
semi-supervised learning is a broader area of machine learning , focusing on improving the learning process by usage of unlabeled data in conjunction with labeled data .
semantic role labeling ( srl ) is a task of automatically identifying semantic relations between predicate and its related arguments in the sentence .
we use a random forest classifier , as implemented in scikit-learn .
we use srilm for training a trigram language model on the english side of the training data .
we also use a 4-gram language model trained using srilm with kneser-ney smoothing .
barzilay and mckeown identify multi-word paraphrases from a sentence-aligned corpus of monolingual parallel texts .
we have proposed adversarial stability training to improve the robustness of nmt models .
for the language model , we used sri language modeling toolkit to train a trigram model with modified kneser-ney smoothing on the 31 , 149 english sentences .
we use liblinear 9 to solve the lr and svm classification problems .
we used the scikit-learn implementation of svrs and the skll toolkit .
to measure the translation quality , we use the bleu score and the nist score .
we use the stanford parser with stanford dependencies .
kondrak and dorr present a large number of language-independent distance measures in order to predict whether two drug names are confusable or not .
we use a cws-oriented model modified from the skip-gram model to derive word embeddings .
wikipedia is a constantly evolving source of detailed information that could facilitate intelligent machines — if they are able to leverage its power .
for training the translation model and for decoding we used the moses toolkit .
the pre-processed monolingual sentences will be used by srilm or berkeleylm to train a n-gram language model .
sentiment classification is the task of identifying the sentiment polarity of a given text .
relation extraction is the task of detecting and characterizing semantic relations between entities from free text .
our model is a structured conditional random field .
for our logistic regression classifier we use the implementation included in the scikit-learn toolkit 2 .
semantic role labeling ( srl ) is the process of extracting simple event structures , i.e. , “ who ” did “ what ” to “ whom ” , “ when ” and “ where ” .
we use pre-trained 100 dimensional glove word embeddings .
luong et al train a recursive neural network for morphological composition , and show its effectiveness on word similarity task .
we use the word2vec skip-gram model to train our word embeddings .
word embedding approaches like word2vec or glove are powerful tools for the semantic analysis of natural language .
semantic role labeling ( srl ) is the task of identifying semantic arguments of predicates in text .
we perform minimum error rate training to tune various feature weights .
the message-level embeddings are generated using doc2vec .
domain adaptation is a challenge for supervised nlp systems because of expensive and time-consuming manual annotated resources .
we employ conditional random fields to predict the sentiment label for each segment .
the target fourgram language model was built with the english part of training data using the sri language modeling toolkit .
we use the word2vec vectors with 300 dimensions , pre-trained on 100 billion words of google news .
we use both logistic regression with elastic net regularisation and support vector machines with a linear kernel .
the language model was constructed using the srilm toolkit with interpolated kneser-ney discounting .
we use the stanford dependency parser to parse the statement and identify the path connecting the content words in the parse tree .
for all experiments , we used a 4-gram language model with modified kneser-ney smoothing which was trained with the srilm toolkit .
first , we train a vector space representations of words using word2vec on chinese wikipedia .
for the classification task , we use pre-trained glove embedding vectors as lexical features .
in this paper , we aim to generate a more meaningful and informative reply when answering a given question .
language modeling is a fundamental task in natural language processing and is routinely employed in a wide range of applications , such as speech recognition , machine translation , etc ’ .
long short-term memory was introduced by hochreiter and schmidhuber to overcome the issue of vanishing gradients in the vanilla recurrent neural networks .
we optimise the feature weights of the model with minimum error rate training against the bleu evaluation metric .
the language models were built using srilm toolkits .
for the fluency and grammaticality features , we train 4-gram lms using the development dataset with the sri toolkit .
luong et al learn word representations based on morphemes that are obtained from an external morphological segmentation system .
we adapted the moses phrase-based decoder to translate word lattices .
semantic role labeling ( srl ) is a task of automatically identifying semantic relations between predicate and its related arguments in the sentence .
phonetic translation across these pairs is called transliteration .
evaluation is done using the bleu metric with four references .
coreference resolution is the task of determining which mentions in a text refer to the same entity .
keyphrase extraction is the problem of automatically extracting important phrases or concepts ( i.e. , the essence ) of a document .
traditional semantic space models represent meaning on the basis of word co-occurrence statistics in large text corpora .
relation extraction ( re ) is the task of identifying instances of relations , such as nationality ( person , country ) or place of birth ( person , location ) , in passages of natural text .
part-of-speech ( pos ) tagging is a fundamental natural-language-processing problem , and pos tags are used as input to many important applications .
a 4-grams language model is trained by the srilm toolkit .
we used srilm for training the 5-gram language model with interpolated modified kneser-ney discounting , .
in our wok , we have used the stanford log-linear part-of-speech to do pos tagging .
the parameter weights are optimized with minimum error rate training .
semantic role labeling ( srl ) is a task of automatically identifying semantic relations between predicate and its related arguments in the sentence .
coreference resolution is the problem of partitioning a sequence of noun phrases ( or mentions ) , as they occur in a natural language text , into a set of referential entities .
we measure the translation quality with ibm bleu up to 4 grams , using 2 reference translations , bleur2n4 .
1a bunsetsu is a common unit when syntactic structures in japanese are discussed .
we further used a 5-gram language model trained using the srilm toolkit with modified kneser-ney smoothing .
relation extraction is the task of finding relationships between two entities from text .
based on the distributional hypothesis , we train a skip-gram model to learn the distributional representations of words in a large corpus .
relation extraction ( re ) is the task of extracting instances of semantic relations between entities in unstructured data such as natural language text .
we train 300 dimensional word embedding using word2vec on all the training data , and fine-turning during the training process .
the language model component uses the srilm lattice-tool for weight assignment and nbest decoding .
we use corpus-level bleu score to quantitatively evaluate the generated paragraphs .
sentiment analysis is a fundamental problem aiming to give a machine the ability to understand the emotions and opinions expressed in a written text .
coreference resolution is the task of identifying all mentions which refer to the same entity in a document .
sarcasm is a form of verbal irony that is intended to express contempt or ridicule .
as the database of typological features , we used the online edition 2 of the world atlas of language structures .
recently , distant supervision has emerged to be a popular choice for training relation extractors without using manually labeled data .
we use the logistic regression classifier in the skll package , which is based on scikit-learn , optimizing for f 1 score .
information extraction ( ie ) is the task of extracting information from natural language texts to fill a database record following a structure called a template .
additionally , we compile the model using the adamax optimizer .
additionally , coreference resolution is a pervasive problem in nlp and many nlp applications could benefit from an effective coreference resolver that can be easily configured and customized .
we use the stanford named entity recognizer to identify named entities in s and t .
the word embeddings are initialized with pre-trained word vectors using word2vec 2 and other parameters are randomly initialized by sampling from uniform distribution in including character embeddings .
the model parameters will then be estimated using the expectation-maximization algorithm .
sentiment analysis is a recent attempt to deal with evaluative aspects of text .
first , arabic is a morphologically rich language ( cite-p-19-3-7 ) .
dependency parsing is the task to assign dependency structures to a given sentence math-w-4-1-0-14 .
we employ the glove and node2vec to generate the pre-trained word embedding , obtaining two distinct embedding for each word .
case-insensitive 4-gram bleu is used as evaluation metric .
we obtained distributed word representations using word2vec 4 with skip-gram .
coreference resolution is a challenging task , that involves identification and clustering of noun phrases mentions that refer to the same real-world entity .
sentiment analysis is the task in natural language processing ( nlp ) that deals with classifying opinions according to the polarity of the sentiment they express .
sentiment analysis is the study of the subjectivity and polarity ( positive vs. negative ) of a text ( cite-p-7-1-10 ) .
we learn our word embeddings by using word2vec 3 on unlabeled review data .
translation quality is measured by case-insensitive bleu on newstest13 using one reference translation .
according to lakoff and johnson , metaphor is a productive phenomenon that operates at the level of mental processes .
to rerank the candidate texts , we used a 5-gram language model trained on the europarl corpus using kenlm .
word sense disambiguation ( wsd ) is a widely studied task in natural language processing : given a word and its context , assign the correct sense of the word based on a predefined sense inventory ( cite-p-15-3-4 ) .
barzilay and lapata propose an entity-based coherence model which operationalizes some of the intuitions behind the centering model .
we use the moses toolkit to train our phrase-based smt models .
for input representation , we used glove word embeddings .
li et al proposed to use the maximum mutual information as the objective to penalize general responses .
we used sklearn-kittext to build our svm models .
an amr is a graph with nodes representing the concepts of the sentence and edges representing the semantic relations between them .
for the english sts subtask , we used regression models combining a wide array of features including semantic similarity scores obtained from various methods .
we also used support vector machines and conditional random fields .
we use glove vectors with 100 dimensions trained on wikipedia and gigaword as word embeddings .
table 4 shows the bleu scores of the output descriptions .
we trained the five classifiers using the svm implementation in scikit-learn .
word sense disambiguation ( wsd ) is a widely studied task in natural language processing : given a word and its context , assign the correct sense of the word based on a predefined sense inventory ( cite-p-15-3-4 ) .
for the actioneffect embedding model , we use pre-trained glove word embeddings as input to the lstm .
we used treetagger based on the english parameter files supplied with it .
we use the logistic regression implementation of liblinear wrapped by the scikit-learn library .
we further used a 5-gram language model trained using the srilm toolkit with modified kneser-ney smoothing .
in recent years , phrase-based systems for statistical machine translation have delivered state-of-the-art performance on standard translation tasks .
we trained a 5-gram language model on the xinhua portion of gigaword corpus using the srilm toolkit .
we also use a 4-gram language model trained using srilm with kneser-ney smoothing .
thus , we train a 4-gram language model based on kneser-ney smoothing method using sri toolkit and interpolate it with the best rnnlms by different weights .
in recent years , phrase-based systems for statistical machine translation have delivered state-of-the-art performance on standard translation tasks .
we obtain word clusters from word2vec k-means word clustering tool .
we performed mert based tuning using the mira algorithm .
we use the stanford dependency parser with the collapsed representation so that preposition nodes become edges .
the 5-gram target language model was trained using kenlm .
we trained the statistical phrase-based systems using the moses toolkit with mert tuning .
we measure translation quality via the bleu score .
the penn discourse treebank is the largest available discourseannotated resource in english .
named entity ( ne ) transliteration is the process of transcribing a ne from a source language to some target language while preserving its pronunciation in the original language .
the clustering method used in this work is latent dirichlet allocation topic modelling .
we use the moses toolkit to train various statistical machine translation systems .
we evaluated the translation quality using the case-insensitive bleu-4 metric .
language models were built with srilm , modified kneser-ney smoothing , default pruning , and order 5 .
we train 300 dimensional word embedding using word2vec on all the training data , and fine-turning during the training process .
we feed our features to a multinomial naive bayes classifier in scikit-learn .
semantic parsing is the task of mapping natural language to machine interpretable meaning representations .
in all cases , we used the implementations from the scikitlearn machine learning library .
sentiment analysis ( sa ) is a field of knowledge which deals with the analysis of people ’ s opinions , sentiments , evaluations , appraisals , attitudes and emotions towards particular entities ( liu , 2012 ) .
coreference resolution is the task of determining whether two or more noun phrases refer to the same entity in a text .
relation extraction ( re ) has been defined as the task of identifying a given set of semantic binary relations in text .
we obtained both phrase structures and dependency relations for every sentence using the stanford parser .
the penn discourse treebank is a new resource of annotated discourse relations .
entity linking ( el ) is the task of automatically linking mentions of entities such as persons , locations , or organizations to their corresponding entry in a knowledge base ( kb ) .
we use the 300-dimensional pre-trained word2vec 3 word embeddings and compare the performance with that of glove 4 embeddings .
text classification is a fundamental problem in natural language processing ( nlp ) .
coreference resolution is the process of linking together multiple expressions of a given entity .
we report the mt performance using the original bleu metric .
llu铆s et al use a joint arcfactored model that predicts full syntactic paths along with predicate-argument structures via dual decomposition .
discourse parsing is the task of identifying the presence and the type of the discourse relations between discourse units .
relation extraction ( re ) is the process of generating structured relation knowledge from unstructured natural language texts .
support vector machines have been shown to outperform other existing methods in text categorization .
within this subpart of our ensemble model , we used a svm model from the scikit-learn library .
mikolov et al used distributed representations of words to learn a linear mapping between vector spaces of languages and showed that this mapping can serve as a good dictionary between the languages .
we use the stanford dependency parser to extract nouns and their grammatical roles .
choi and cardie developed inference rules to capture compositional effects at the lexical level on phrase-level polarity classification .
we use the popular moses toolkit to build the smt system .
we use pre-trained word vectors from glove .
our framework has made clear advancements with respect to existing structured topic models .
sarcasm is a form of speech in which speakers say the opposite of what they truly mean in order to convey a strong sentiment .
semantic role labeling ( srl ) is the task of identifying the semantic arguments of a predicate and labeling them with their semantic roles .
coreference resolution is a task aimed at identifying phrases ( mentions ) referring to the same entity .
we use srilm toolkit to train a trigram language model with modified kneser-ney smoothing on the target side of training corpus .
the base pcfg uses simplified categories of the stanford pcfg parser .
we evaluated translation quality using uncased bleu and ter .
sentiment classification is the fundamental task of sentiment analysis ( cite-p-15-3-11 ) , where we are to classify the sentiment of a given text .
srilm toolkit is used to build these language models .
we estimated 5-gram language models using the sri toolkit with modified kneser-ney smoothing .
recently , neural networks become popular for natural language processing .
word sense disambiguation ( wsd ) is a natural language processing ( nlp ) task in which the correct meaning ( sense ) of a word in a given context is to be determined .
we used a 5-gram language model with modified kneser-ney smoothing implemented using the srilm toolkit .
for the language model , we used sri language modeling toolkit to train a trigram model with modified kneser-ney smoothing on the 31 , 149 english sentences .
we used the moses decoder , with default settings , to obtain the translations .
in this paper , we adopt continuous bag-of-word in word2vec as our context-based embedding model .
the decoding weights are optimized with minimum error rate training to maximize bleu scores .
we use the skll and scikit-learn toolkits .
we use srilm for training a trigram language model on the english side of the training data .
semantic parsing is the task of mapping natural language to machine interpretable meaning representations .
we conduct experiments on the latest twitter sentiment classification benchmark dataset in semeval 2013 .
and while discourse parsing is a document level task , discourse segmentation is done at the sentence level , assuming that sentence boundaries are known .
the parameter weights are optimized with minimum error rate training .
although coreference resolution is a subproblem of natural language understanding , coreference resolution evaluation metrics have predominately been discussed in terms of abstract entities and hypothetical system errors .
a 4-gram language model which was trained on the entire training corpus using srilm was used to generate responses in conjunction with the phrase-based translation model .
brockett et al use an smt system to correct errors involving mass noun errors .
simulating the approach reported by , we trained a support vector machine for regression with rbf kernel using scikit-learn with the set of features .
sentiment analysis is a growing research field , especially on web social networks .
coreference resolution is the task of partitioning a set of entity mentions in a text , where each partition corresponds to some entity in an underlying discourse model .
we use the brown clustering algorithm to induce our word representations .
our experimental results show that this approach can accurately predict missing topic preferences of users accurately ( 80–94 % ) .
the smt tools are a phrase-based smt toolkit licensed by nict , and moses .
stance detection is the task of assigning stance labels to a piece of text with respect to a topic , i.e . whether a piece of text is in favour of “ abortion ” , neutral , or against .
we measure the translation quality with automatic metrics including bleu and ter .
for this model , we use a binary logistic regression classifier implemented in the lib-linear package , coupled with the ovo scheme .
as a baseline for this comparison , we use morfessor categories-map .
relation extraction is a fundamental task in information extraction .
we used word2vec to learn these dense vectors .
relation extraction is the task of recognizing and extracting relations between entities or concepts in texts .
to address this problem , we propose coverage-based nmt in this paper .
our nmt is based on an encoderdecoder with attention design , using bidirectional lstm layers for encoding and unidirectional layers for decoding .
in this paper , we introduce the novel task of question answering using natural language demonstrations .
luong and manning propose a fine-tuning method , which continues to train the already trained out-of-domain system on the in-domain data .
then we use the standard minimum error-rate training to tune the feature weights to maximize the system潞s bleu score .
the same data was used for tuning the systems with mert .
a multiword expression can be defined as a combination of words for which syntactic or semantic properties of the whole expression can not be obtained from its parts .
luong et al utilized the morpheme segments produced by morfessor and constructed morpheme trees for words to learn morphologically-aware word embeddings by the recursive neural network .
we trained the embedding vectors with the word2vec tool on the large unlabeled corpus of clinical texts provided by the task organizers .
marcu and wong proposed a phrase-based context-free joint probability model for lexical mapping .
sequence labeling is a structured prediction task where systems need to assign the correct label to every token in the input sequence .
the system was trained using moses with default settings , using a 5-gram language model created from the english side of the training corpus using srilm .
furthermore , we train a 5-gram language model using the sri language toolkit .
some researchers used similarity and association measures to build alignment links .
we use the scikit-learn machine learning library to implement the entire pipeline .
conditional random fields are probabilistic models for labelling sequential data .
twitter 1 is a microblogging service , which according to latest statistics , has 284 million active users , 77 % outside the us that generate 500 million tweets a day in 35 different languages .
morphological analysis is a staple of natural language processing for broad languages .
our translation model is implemented as an n-gram model of operations using the srilm toolkit with kneser-ney smoothing .
we used the support vector machine implementation from the liblinear library on the test sets and report the results in table 4 .
we use the sri language model toolkit to train a 5-gram model with modified kneser-ney smoothing on the target-side training corpus .
for the language model we use the corpus of 60,000 simple english wikipedia articles 3 and build a 3-gram language model with kneser-ney smoothing trained with srilm .
we first trained a trigram bnlm as the baseline with interpolated kneser-ney smoothing , using srilm toolkit .
automatic word alignment is a key step in training statistical machine translation systems .
we first use bleu score to perform automatic evaluation .
we then lowercase all data and use all sentences from the modern dutch part of the corpus to train an n-gram language model with the srilm toolkit .
we evaluated the translation quality using the case-insensitive bleu-4 metric .
the 5-gram kneser-ney smoothed language models were trained by srilm , with kenlm used at runtime .
our machine translation system is a phrase-based system using the moses toolkit .
semantic parsing is the problem of translating human language into computer language , and therefore is at the heart of natural language understanding .
we initialize the embedding weights by the pre-trained word embeddings with 200 dimensional vectors .
we choose modified kneser ney as the smoothing algorithm when learning the ngram model .
we used the srilm toolkit to build unpruned 5-gram models using interpolated modified kneser-ney smoothing .
we train a trigram language model with the srilm toolkit .
word sense disambiguation ( wsd ) is the nlp task that consists in selecting the correct sense of a polysemous word in a given context .
coreference resolution is a key task in natural language processing ( cite-p-13-1-8 ) aiming to detect the referential expressions ( mentions ) in a text that point to the same entity .
in this paper , we present a more expressive entity-mention model for coreference resolution .
relation extraction is the task of finding relationships between two entities from text .
hatzivassiloglou and mckeown proposed a method for identifying word polarity of adjectives .
word sense disambiguation ( wsd ) is the task of determining the correct meaning for an ambiguous word from its context .
in the case of bilingual word embedding , mikolov et al propose a method to learn a linear transformation from the source language to the target language for the task of lexicon extraction from bilingual corpora .
we evaluate our models with the standard rouge metric and obtain rouge scores using the pyrouge package .
hatzivassiloglou and mckeown proposed a supervised algorithm to determine the semantic orientation of adjectives .
more recently , mikolov et al propose two log-linear models , namely the skip-gram and cbow model , to efficiently induce word embeddings .
deep neural networks have seen widespread use in natural language processing tasks such as parsing , language modeling , and sentiment analysis .
minimum error rate training is applied to tune the cn weights .
extractive summarization is a sentence selection problem : identifying important summary sentences from one or multiple documents .
dreyer and eisner proposed a log-linear model to identify paradigms .
coreference resolution is a fundamental component of natural language processing ( nlp ) and has been widely applied in other nlp tasks ( cite-p-15-3-9 ) .
our ncpg system is an attention-based bidirectional rnn architecture that uses an encoder-decoder framework .
as for je translation , we use a popular japanese dependency parser to obtain japanese abstraction trees .
wordnet is a key lexical resource for natural language applications .
the targetside 4-gram language model was estimated using the srilm toolkit and modified kneser-ney discounting with interpolation .
the word-embeddings were initialized using the glove 300-dimensions pre-trained embeddings and were kept fixed during training .
the feature weights for each system were tuned on development sets using the moses implementation of minimum error rate training .
their weights are optimized using minimum error-rate training on a held-out development set for each of the experiments .
sentiment analysis ( sa ) is the research field that is concerned with identifying opinions in text and classifying them as positive , negative or neutral .
more importantly , chinese is a language that lacks the morphological clues that help determine the pos tag of a word .
we compute the interannotator agreement in terms of the bleu score .
since coreference resolution is a pervasive discourse phenomenon causing performance impediments in current ie systems , we considered a corpus of aligned english and romanian texts to identify coreferring expressions .
we use corpus-level bleu score to quantitatively evaluate the generated paragraphs .
mihalcea et al propose a method to learn multilingual subjective language via crosslanguage projections .
the 5-gram kneser-ney smoothed language models were trained by srilm , with kenlm used at runtime .
we found that performance improves steadily as the number of available languages increases .
we first train a word2vec model on fr-wikipedia 11 to obtain non contextual word vectors .
experimental results show that our algorithm ( math-w-2-2-5-186 ) can find important feature subset , estimate cluster number and achieve better performance compared with cgd algorithm .
for representing words , we used 100 dimensional pre-trained glove embeddings .
specifically , we used the python scikit-learn module , which interfaces with the widely-used libsvm .
the 5-gram target language model was trained using kenlm .
our baseline system is based on a hierarchical phrase-based translation model , which can formally be described as a synchronous context-free grammar .
in this paper , we propose guided learning , a new learning framework for bidirectional sequence classification .
sentiment analysis is the task of identifying the polarity ( positive , negative or neutral ) of review .
we use the word2vec skip-gram model to train our word embeddings .
social media is a popular public platform for communicating , sharing information and expressing opinions .
after standard preprocessing of the data , we train a 3-gram language model using kenlm .
we use sri language model toolkit to train a 5-gram model with modified kneser-ney smoothing on the target-side training corpus .
for other methods , we used the mstparser as the underlying dependency parsing tool .
later , xue et al combined the language model and translation model to a translation-based language model and observed better performance in question retrieval .
huang et al further extended this context clustering method and incorporated global context to learn multi-prototype representation vectors .
we use glove word embeddings , which are 50-dimension word vectors trained with a crawled large corpus with 840 billion tokens .
in this paper , we present a comprehensive analysis of the relationship between personal traits and brand preferences .
semantic similarity is a core technique for many topics in natural language processing such as textual entailment ( cite-p-22-1-7 ) , semantic role labeling ( cite-p-22-1-19 ) , and question answering ( cite-p-22-3-26 ) .
we implemented the algorithms in python using the stochastic gradient descent method for nmf from the scikit-learn package .
mikolov et al and mikolov et al further observe that the semantic relationship of words can be induced by performing simple algebraic operations with word vectors .
word segmentation is a prerequisite for many natural language processing ( nlp ) applications on those languages that have no explicit space between words , such as arabic , chinese and japanese .
we use the automatic mt evaluation metrics bleu , meteor , and ter , to evaluate the absolute translation quality obtained .
hearst examined extracting hyponym data by taking advantage of lexical patterns in text .
we use the adaptive moment estimation for the optimizer .
we trained a 4-gram language model with kneser-ney smoothing and unigram caching using the sri-lm toolkit .
we use the adam optimizer and mini-batch gradient to solve this optimization problem .
for lm training and interpolation , the srilm toolkit was used .
we further used a 5-gram language model trained using the srilm toolkit with modified kneser-ney smoothing .
automatic semantic role labeling was first introduced by gildea and jurafsky .
we used stanford corenlp for sentence splitting , part-of-speech tagging , named entity recognition , co-reference resolution and dependency parsing .
zou et al learn bilingual word embeddings by designing an objective function that combines unsupervised training with bilingual constraints based on word alignments .
mikolov et al proposed a computationally efficient method for learning distributed word representation such that words with similar meanings will map to similar vectors .
using this similarity function in query expansion can significantly improve the retrieval performance .
we apply the 3-phase learning procedure proposed by where we first create word embeddings based on the skip-gram model .
our phrase-based system is similar to the alignment template system described by och and ney .
mikolov et al further proposed continuous bagof-words and skip-gram models , which use a simple single-layer architecture based on inner product between two word vectors .
framenet is an expert-built lexical-semantic resource incorporating the theory of frame-semantics .
dependency parsing is a way of structurally analyzing a sentence from the viewpoint of modification .
we used l2-regularized logistic regression classifier as implemented in liblinear .
in order to build the englishfrench parallel corpus with discourse annotations , we used the europarl corpus .
coreference resolution is a set partitioning problem in which each resulting partition refers to an entity .
we evaluate our models using the standard bleu metric 2 on the detokenized translations of the test set .
word sense disambiguation is the process of determining which sense of a homograph is correct in a given context .
pang and lee propose a graph-based method which finds minimum cuts in a document graph to classify the sentences into subjective or objective .
for the tokenization process , our system used tweettokenizer from nltk .
semantic parsing is the task of translating natural language utterances into a machine-interpretable meaning representation .
for evaluation , caseinsensitive nist bleu is used to measure translation performance .
each candidate property is generated from just one component of the simile .
we propose a grouping-based ordering framework that integrates local and global coherence concerns .
we used moses to train an alignment model on the created paraphrase dataset .
we use skip-gram with negative sampling for obtaining the word embeddings .
to train the models we use the default stochastic gradient descent classifier provided by scikit-learn .
in this paper , we propose the first approach for applying distant supervision to cross-sentence relation extraction .
we perform pre-training using the skipgram nn architecture available in the word2vec tool .
our baseline is an in-house phrase-based statistical machine translation system very similar to moses .
we use the logistic regression implementation of liblinear wrapped by the scikit-learn library .
relation extraction is a core task in information extraction and natural language understanding .
in this work , we address the technical difficulty of leveraging implicit supervision in learning an algebra word problem solver .
as a classifier , we employ support vector machines as implemented in svm light .
for the evaluation of the results we use the bleu score .
feature weights were set with minimum error rate training on a development set using bleu as the objective function .
we used the bleu score to evaluate the translation accuracy with and without the normalization .
vaswani et al extend the dot product attention described in luong et al to consider these vectors .
we evaluated the system using bleu score on the test set .
our machine translation system is a phrase-based system using the moses toolkit .
gabrilovich and markovitch utilized wikipedia-based concepts as the basis for a high-dimensional meaning representation space .
because the system is incremental , it should be straightforward to apply it to unsegmented text .
semantic parsing is the task of translating natural language utterances into a machine-interpretable meaning representation ( mr ) .
within this subpart of our ensemble model , we used a svm model from the scikit-learn library .
for simplicity , we use the well-known conditional random fields for sequential labeling .
the parsing algorithm is extended to handle translation candidates and to incorporate language model scores via cube pruning .
we generate dependency structures from the ptb constituency trees using the head rules of yamada and matsumoto .
the skip-gram model is a very popular technique for learning embeddings that scales to huge corpora and can capture important semantic and syntactic properties of words .
to evaluate segment translation quality , we use corpus level bleu .
we use both logistic regression with elastic net regularisation and support vector machines with a linear kernel .
language models were built using the sri language modeling toolkit with modified kneser-ney smoothing .
gu et al , cheng and lapata , and nallapati et al also utilized seq2seq based framework with attention modeling for short text or single document summarization .
we use the linearsvc classifier as implemented in scikit-learn package 17 with the default parameters .
yarowsky proposes a method for word sense disambiguation , which is based on monolingual bootstrapping .
the grammatical relations are all the collapsed dependencies produced by the stanford dependency parser .
gamon et al train a decision tree model and a language model to correct errors in article and preposition usage .
we initialize the word embeddings for our deep learning architecture with the 100-dimensional glove vectors .
dependency parsing is the task of building dependency links between words in a sentence , which has recently gained a wide interest in the natural language processing community .
for language models , we use the srilm linear interpolation feature .
in particular , socher et al obtain good parsing performance by building compositional representations from word vectors .
semantic role labeling ( srl ) is defined as the task to recognize arguments for a given predicate and assign semantic role labels to them .
we further used a 5-gram language model trained using the srilm toolkit with modified kneser-ney smoothing .
our translation model is implemented as an n-gram model of operations using the srilm toolkit with kneser-ney smoothing .
in order to reduce the amount of annotated data to train a dependency parser , koo et al used word clusters computed from unlabelled data as features for training a parser .
the weights associated to feature functions are optimally combined using the minimum error rate training .
coreference resolution is a challenging task , that involves identification and clustering of noun phrases mentions that refer to the same real-world entity .
named entity recognition ( ner ) is the task of identifying named entities in free text—typically personal names , organizations , gene-protein entities , and so on .
berger and lafferty proposed the use of translation models for document retrieval .
the quality of translations is evaluated by the case insensitive nist bleu-4 metric .
coreference resolution is the process of linking multiple mentions that refer to the same entity .
we use the attention-based nmt model introduced by bahdanau et al as our text-only nmt baseline .
we apply sri language modeling toolkit to train a 4-gram language model with kneser-ney smoothing .
named entity recognition ( ner ) is the task of finding rigid designators as they appear in free text and classifying them into coarse categories such as person or location ( cite-p-24-4-6 ) .
we update the gradient with adaptive moment estimation .
lexical co-occurrences have previously been shown to be useful for discourse level learning tasks .
we performed paired bootstrap sampling to test the significance in bleu score differences .
we initialize the embedding weights by the pre-trained word embeddings with 200 dimensional vectors .
semantic parsing is the task of automatically translating natural language text to formal meaning representations ( e.g. , statements in a formal logic ) .
we implement the weight tuning component according to the minimum error rate training method .
the srilm toolkit was used to build the 5-gram language model .
the target fourgram language model was built with the english part of training data using the sri language modeling toolkit .
our word embeddings is initialized with 100-dimensional glove word embeddings .
srilm toolkit was used to create up to 5-gram language models using the mentioned resources .
our smt system is a phrase-based system based on the moses smt toolkit .
semantic parsing is the problem of translating human language into computer language , and therefore is at the heart of natural language understanding .
relation extraction ( re ) is the task of extracting semantic relationships between entities in text .
for assessing significance , we apply the approximate randomization test .
the universal dependencies project seeks to develop cross-linguistically consistent treebank annotation of morphology and syntax for many languages .
in this paper , we propose a novel cascade model , which can capture both the latent semantics and latent similarity by modeling mooc data .
table 2 shows the translation quality measured in terms of bleu metric with the original and universal tagset .
zhou et al further propose context-sensitive spt , which can dynamically determine the tree span by extending the necessary predicate-linked path information outside spt .
stance detection is the task of determining whether the author of a text is in favor or against a given topic , while rejecting texts in which neither inference is likely .
relation extraction ( re ) is the task of assigning a semantic relationship between a pair of arguments .
unpruned language models were trained using lmplz which employs modified kneser-ney smoothing .
collobert et al showed that a neural model could achieve close to state-of-the-art results in part of speech tagging and chunking by relying almost only on word embeddings learned with a language model .
twitter is a widely used microblogging platform , where users post and interact with messages , “ tweets ” .
the srilm toolkit was used to build this language model .
the target-side language models were estimated using the srilm toolkit .
we used the support vector machine implementation from the liblinear library on the test sets and report the results in table 4 .
for input representation , we used glove word embeddings .
for the training of the smt model , including the word alignment and the phrase translation table , we used moses , a toolkit for phrase-based smt models .
relation extraction is a fundamental task that enables a wide range of semantic applications from question answering ( cite-p-13-3-12 ) to fact checking ( cite-p-13-3-10 ) .
part-of-speech ( pos ) tagging is a crucial task for natural language processing ( nlp ) tasks , providing basic information about syntax .
language models were built using the sri language modeling toolkit with modified kneser-ney smoothing .
we used trigram language models with interpolated kneser-kney discounting trained using the sri language modeling toolkit .
the decoder uses a cky-style parsing algorithm and cube pruning to integrate the language model scores .
the n-gram language models are trained using the srilm toolkit or similar software developed at hut .
to minimize the objective , we use stochastic gradient descent with the diagonal variant of adagrad .
translation performances are measured with case-insensitive bleu4 score .
we used 4-gram language models , trained using kenlm .
we evaluate our semantic parser on the webques-tions dataset , which contains 5,810 question-answer pairs .
the framenet corpus is a collection of semantic frames , together with a corpus of documents annotated with these frames .
we built a trigram language model with kneser-ney smoothing using kenlm toolkit .
to the best of our knowledge , this is the first time that very deep convolutional nets have been applied to text processing .
language models were built using the sri language modeling toolkit with modified kneser-ney smoothing .
one of the very few available discourse annotated corpora is the penn discourse treebank in english .
we evaluated the system using bleu score on the test set .
in this paper , we model our problem in the framework of posterior regularization .
for all three classifiers , we used the word2vec 300d pre-trained embeddings as features .
miwa and bansal adopted a bidirectional tree lstm model to jointly extract named entities and relations under a dependency tree structure .
for j-e translation , we used the cabocha parser to analyze the context document .
we trained two 5-gram language models on the entire target side of the parallel data , with srilm .
the language models were built using srilm toolkits .
we use srilm to train a 5-gram language model on the target side of our training corpus with modified kneser-ney discounting .
the language model is a 3-gram language model trained using the srilm toolkit on the english side of the training data .
sarcasm is a form of verbal irony that is intended to express contempt or ridicule .
we used the svm implementation provided within scikit-learn .
the weights of the different feature functions were optimised by means of minimum error rate training on the 2013 wmt test set .
the case insensitive nist bleu-4 metric is adopted for evaluation .
semantic role labeling ( srl ) is a form of shallow semantic parsing whose goal is to discover the predicate-argument structure of each predicate in a given input sentence .
for training the translation model and for decoding we used the moses toolkit .
we use a pbsmt model where the language model is a 5-gram lm with modified kneser-ney smoothing .
we automatically produced training data from the penn treebank .
this approach benefits from large unsupervised corpora , that can be used to learn effective word embeddings .
badjatiya et al presented a gradient boosted lstm model with random embeddings to outperform state of the art hate speech detection techniques .
there is no data like more data , performance improves log-linearly with the number of parameters ( unique n-grams ) .
we implement our approach in the framework of phrase-based statistical machine translation .
the annotation is based on the google universal part-ofspeech tags and the stanford dependencies , adapted and harmonized across languages .
parsing is the task of reconstructing the syntactic structure from surface text .
with english gigaword corpus , we use the skip-gram model as implemented in word2vec 3 to induce embeddings .
we used a phrase-based smt model as implemented in the moses toolkit .
the feature weights for each system were tuned on development sets using the moses implementation of minimum error rate training .
we train the parameters of the stages separately using adagrad with the perceptron loss function .
morphological analysis is the basis for many nlp applications , including syntax parsing , machine translation and automatic indexing .
the existing methods use only the information in either language side .
we trained a standard 5-gram language model with modified kneser-ney smoothing using the kenlm toolkit on 4 billion running words .
li et al used a latent dirichlet allocation model to generate topic distribution features as the news representations .
in our work , we use latent dirichlet allocation to identify the sub-topics in the given body of texts .
word embedding approaches like word2vec or glove are powerful tools for the semantic analysis of natural language .
for this task , we use the widely-used bleu metric .
the language model is a 3-gram language model trained using the srilm toolkit on the english side of the training data .
word sense disambiguation ( wsd ) is a problem long recognised in computational linguistics ( yngve 1955 ) and there has been a recent resurgence of interest , including a special issue of this journal devoted to the topic ( cite-p-27-8-11 ) .
the component features are weighted to minimize a translation error criterion on a development set .
twitter 1 is a microblogging service , which according to latest statistics , has 284 million active users , 77 % outside the us that generate 500 million tweets a day in 35 different languages .
named entity disambiguation is the task of linking entity mentions to their intended referent , as represented in a knowledge base , usually derived from wikipedia .
for our experiments reported here , we obtained word vectors using the word2vec tool and the text8 corpus .
we used srilm to build a 4-gram language model with interpolated kneser-ney discounting .
semantic role labeling ( srl ) has been defined as a sentence-level natural-language processing task in which semantic roles are assigned to the syntactic arguments of a predicate ( cite-p-14-1-7 ) .
the language models were trained using srilm toolkit .
word segmentation is the foremost obligatory task in almost all the nlp applications where the initial phase requires tokenization of input into words .
all word vectors are trained on the skipgram architecture .
we report decoding speed and bleu score , as measured by sacrebleu .
socher et al present a model for compositionality based on recursive neural networks .
traditional semantic space models represent meaning on the basis of word co-occurrence statistics in large text corpora .
turian et al , for example , used embeddings from existing language models as unsupervised lexical features to improve named entity recognition and chunking .
word sense disambiguation ( wsd ) is the task of assigning sense tags to ambiguous lexical items ( lis ) in a text .
semantic parsing is the problem of translating human language into computer language , and therefore is at the heart of natural language understanding .
some researchers use similarity and association measures to build alignment links .
sentiment analysis is the task of automatically identifying the valence or polarity of a piece of text .
sentiment analysis is a much-researched area that deals with identification of positive , negative and neutral opinions in text .
the smt system was tuned on the development set newstest10 with minimum error rate training using the bleu error rate measure as the optimization criterion .
the language models were interpolated kneser-ney discounted trigram models , all constructed using the srilm toolkit .
we also used pre-trained word embeddings , including glove and 300d fasttext vectors .
we use the pre-trained 300-dimensional word2vec vectors by mikolov et al and mikolov et al .
sentiment analysis is the task of automatically identifying the valence or polarity of a piece of text .
we use the 100-dimensional pre-trained word embeddings trained by word2vec 2 and the 100-dimensional randomly initialized pos tag embeddings .
xiao et al propose a topic-based similarity model for rule selection in hierarchical phrasebased translation .
we use the open source moses phrase-based mt system to test the impact of the preprocessing technique on translation quality .
the annotation was performed using the brat 2 tool .
the syntax tree features were calculated using the stanford parser trained using the english caseless model .
bengio et al proposed a probabilistic neural network language model for word representations .
all language models are created with the srilm toolkit and are standard 4-gram lms with interpolated modified kneser-ney smoothing .
the decoding weights were optimized with minimum error rate training .
part-of-speech ( pos ) tagging is the task of assigning each of the words in a given piece of text a contextually suitable grammatical category .
we used the svd implementation provided in the scikit-learn toolkit .
for english , we use the pre-trained glove vectors .
the language model was constructed using the srilm toolkit with interpolated kneser-ney discounting .
we use pre-trained 50 dimensional glove vectors 4 for word embeddings initialization .
in addition , we use an english corpus of roughly 227 million words to build a target-side 5-gram language model with srilm in combination with kenlm .
coreference resolution is a complex problem , and successful systems must tackle a variety of non-trivial subproblems that are central to the coreference task — e.g. , mention/markable detection , anaphor identification — and that require substantial implementation efforts .
we use the sdsl library to implement all our structures and compare our indexes to srilm .
word sense disambiguation ( wsd ) is a particular problem of computational linguistics which consists in determining the correct sense for a given ambiguous word .
unpruned language models were trained using lmplz which employs modified kneser-ney smoothing .
coreference resolution is a well known clustering task in natural language processing .
the meta-net project aims to ensure equal access to information by all european citizens .
as described herein , we proposed a new automatic evaluation method for machine translation .
we adopt berkeley parser 1 to train our sub-models .
we trained two 5-gram language models on the entire target side of the parallel data , with srilm .
experiments on nlp & cc 2013 clsc dataset show that our approach outperforms the state-of-the-art systems .
pereira , curran and lin use syntactic features in the vector definition .
parameter tuning was carried out using both k-best mira and minimum error rate training on a held-out development set .
taxonomies , which serve as backbones for structured knowledge , are useful for many nlp applications such as question answering and document clustering .
the lstm addresses the problem by re-parameterizing the rnn model .
ccg is a strongly lexicalized formalism , in which every word is associated with a syntactic category ( similar to an elementary syntactic structure ) indicating its subcategorization potential .
we used the srilm toolkit to train a 4-gram language model on the xinhua portion of the gigaword corpus , which contains 238m english words .
we used minimum error rate training to tune the feature weights for maximum bleu on the development set .
negation is a grammatical category that comprises devices used to reverse the truth value of propositions .
these nlp tools have the potential to make a marked difference for gun violence researchers .
we also use a 4-gram language model trained using srilm with kneser-ney smoothing .
we evaluate our semantic parser on the webques-tions dataset , which contains 5,810 question-answer pairs .
we used a 4-gram language model which was trained on the xinhua section of the english gigaword corpus using the srilm 4 toolkit with modified kneser-ney smoothing .
we tokenized , cleaned , and truecased our data using the standard tools from the moses toolkit .
kalchbrenner et al , 2014 ) proposes a cnn framework with multiple convolution layers , with latent , dense and low-dimensional word embeddings as inputs .
coreference resolution is a challenging task , that involves identification and clustering of noun phrases mentions that refer to the same real-world entity .
we use the automatic mt evaluation metrics bleu , meteor , and ter , to evaluate the absolute translation quality obtained .
rosa et al and mare膷ek et al applied a rule-based approach to ape of english-czech mt outputs on the morphological level .
we trained a 5-gram language model on the xinhua portion of gigaword corpus using the srilm toolkit .
we trained two 5-gram language models on the entire target side of the parallel data , with srilm .
italwordnet is a lexical semantic database based on eurowordnet lexical model which , in its turn , is inspired from princeton wordnet .
we used a 5-gram language model with modified kneser-ney smoothing implemented using the srilm toolkit .
a back-off 2-gram model with good-turing discounting and no lexical classes was also created from the training set , using the srilm toolkit , .
the smt system was tuned on the development set newstest10 with minimum error rate training using the bleu error rate measure as the optimization criterion .
we use word embeddings of dimension 100 pretrained using word2vec on the training dataset .
coreference resolution is a challenging task , that involves identification and clustering of noun phrases mentions that refer to the same real-world entity .
semantic role labeling ( srl ) is the process of extracting simple event structures , i.e. , “ who ” did “ what ” to “ whom ” , “ when ” and “ where ” .
in this run , we use a sentence vector derived from word embeddings obtained from word2vec .
as a strong baseline , we trained the skip-gram model of mikolov et al using the publicly available word2vec 5 software .
mihalcea et al proposed a method to measure the semantic similarity of words or short texts , considering both corpus-based and knowledge-based information .
sentiment analysis is a research area in the field of natural language processing .
for the word-embedding based classifier , we use the glove pre-trained word embeddings .
a lexicalized reordering model was trained with the msd-bidirectional-fe option .
we implement an in-domain language model using the sri language modeling toolkit .
twitter is a microblogging service that has 313 million monthly active users 1 .
for regularization , dropout is applied to each layer .
this scenario posits new challenges to active learning .
reading comprehension ( rc ) is a language understanding task similar to question answering , where a system is expected to read a given passage of text and answer questions about it .
we used bleu and meteor for extrinsic evaluation .
word embedding models are aimed at learning vector representations of word meaning .
coreference resolution is the task of clustering referring expressions in a text so that each resulting cluster represents an entity .
we use the stanford dependency parser with the collapsed representation so that preposition nodes become edges .
coreference resolution is the task of determining which mentions in a text refer to the same entity .
lexical substitution is a more natural task , enables us to evaluate meaning composition at the level of individual words , and provides a common ground to compare cdsms with dedicated lexical substitution models .
dredze et al , show that domain adaptation is hard for dependency parsing based on results in the conll 2007 shared task .
we used datasets distributed for the 2006 and 2007 conll shared tasks .
we train randomly initialized word embeddings of size 500 for the dialog model and use 300 dimentional glove embeddings for reranking classifiers .
we use liblinear logistic regression module to classify document-level embeddings .
we report bleu and ter evaluation scores .
we trained a 5-gram language model on the xinhua portion of gigaword corpus using the srilm toolkit .
peng et al achieved better results by using a conditional random field model .
the smt system is implemented using moses and the nmt system is built using the fairseq toolkit .
after standard preprocessing of the data , we train a 3-gram language model using kenlm .
we model the generative architecture with a recurrent language model based on a recurrent neural network .
the language model was a kneser-ney interpolated trigram model generated using the srilm toolkit .
recently , mikolov et al proposed novel model architectures to compute continuous vector representations of words obtained from very large data sets .
continuous representation of words and phrases are proven effective in many nlp tasks .
we use the aligned english and german sentences in europarl for our experiments .
part-of-speech tagging is the act of assigning each word in a sentence a tag that describes how that word is used in the sentence .
we are also interested in using long short-term memory neural networks to better model the locality of propagated information from the stack and queue .
the skip-gram model is a very popular technique for learning embeddings that scales to huge corpora and can capture important semantic and syntactic properties of words .
we evaluate the translation quality using the case-sensitive bleu-4 metric .
we work with the phrase-based smt framework as the baseline system .
for the semantic language model , we used the srilm package and trained a tri-gram language model with the default goodturing smoothing .
transliteration is a key building block for multilingual and cross-lingual nlp since it is useful for user-friendly input methods and applications like machine translation and cross-lingual information retrieval .
recently , le and mikolov exploit neural networks to learn continuous document representation from data .
named entity recognition ( ner ) is the process by which named entities are identified and classified in an open-domain text .
the data sets used are taken from the conll-x shared task on multilingual dependency parsing .
to see whether an improvement is statistically significant , we also conduct significance tests using the paired bootstrap approach .
we used the malt parser to obtain source english dependency trees and the stanford parser for arabic .
conditional random fields are discriminatively-trained undirected graphical models that find the globally optimal labeling for a given configuration of random variables .
we use our reordering model for n-best re-ranking and optimize bleu using minimum error rate training .
named entity recognition ( ner ) is the process by which named entities are identified and classified in an open-domain text .
an early attempt can be found in nepveu et al , where dynamic adaptation of an imt system via cache-based model extensions to language and translation models is proposed .
for the semantic language model , we used the srilm package and trained a tri-gram language model with the default goodturing smoothing .
luong et al , 2013 ) utilized recursive neural networks in which inputs are morphemes of words .
specifically , we adopt linear-chain conditional random fields as the method for sequence labeling .
word sense disambiguation ( wsd ) is a key enabling technology that automatically chooses the intended sense of a word in context .
relation extraction ( re ) is the process of generating structured relation knowledge from unstructured natural language texts .
we use the glove word vector representations of dimension 300 .
word sense disambiguation ( wsd ) is a key enabling-technology that automatically chooses the intended sense of a word in context .
existing works are based on two basic models , plsa and lda .
relation extraction ( re ) is the task of identifying instances of relations , such as nationality ( person , country ) or place of birth ( person , location ) , in passages of natural text .
we use sri language model toolkit to train a 5-gram model with modified kneser-ney smoothing on the target-side training corpus .
as described by joshi , bhattacharyya , and carman , irony modeling approaches can roughly be classified into rule-based and machine learning methods .
for the word-embedding based classifier , we use the glove pre-trained word embeddings .
sentiment analysis is the process of identifying and extracting subjective information using natural language processing ( nlp ) .
we learn our word embeddings by using word2vec 3 on unlabeled review data .
we used srilm for training the 5-gram language model with interpolated modified kneser-ney discounting , .
if the anaphor is a definite noun phrase and the referent is in focus ( i.e . in the cache ) , anaphora resolution will be hindered .
for improving the word alignment , we use the word-classes that are trained from a monolingual corpus using the srilm toolkit .
morfessor 2.0 is a rewrite of the original , widely-used morfessor 1.0 software , with well documented command-line tools and library interface .
we trained a 4-gram language model on this data with kneser-ney discounting using srilm .
coreference resolution is a field in which major progress has been made in the last decade .
we trained kneser-ney discounted 5-gram language models on each available corpus using the srilm toolkit .
dependency parsing is a crucial component of many natural language processing ( nlp ) systems for tasks such as relation extraction ( cite-p-15-1-5 ) , statistical machine translation ( cite-p-15-5-7 ) , text classification ( o ? zgu ? r and gu ? ngo ? r , 2010 ) , and question answering ( cite-p-15-3-0 ) .
huang et al further extended this context clustering method and incorporated global context to learn multi-prototype representation vectors .
zeng et al proposed an approach for relation classification where sentence-level features are learned through a cnn , which has word embedding and position features as its input .
distant supervision as a learning paradigm was introduced by mintz et al for relation extraction in general domain .
the language model is a 5-gram with interpolation and kneserney smoothing .
we use the pre-trained 300-dimensional word2vec embeddings trained on google news 1 as input features .
our word embeddings is initialized with 100-dimensional glove word embeddings .
we use conditional random fields for sequence labelling .
coreference resolution is a task aimed at identifying phrases ( mentions ) referring to the same entity .
for the actioneffect embedding model , we use pre-trained glove word embeddings as input to the lstm .
as a classifier , we choose a first-order conditional random field model .
we build a 9-gram lm using srilm toolkit with modified kneser-ney smoothing .
relation extraction is a fundamental task in information extraction .
for the classification task , we use pre-trained glove embedding vectors as lexical features .
we used a 5-gram language model with modified kneser-ney smoothing , built with the srilm toolkit .
due to their ability to capture syntactic and semantic information of words from large scale unlabeled texts , we pre-train the word embeddings from the given training dataset by word2vec toolkit .
coreference resolution is the task of partitioning a set of entity mentions in a text , where each partition corresponds to some entity in an underlying discourse model .
cite-p-20-1-16 extended the above model to handle other types of non-standard words .
coreference resolution is the task of clustering referring expressions in a text so that each resulting cluster represents an entity .
table 4 shows the bleu scores of the output descriptions .
in our experiments , we choose to use the published glove pre-trained word embeddings .
tai et al , and le and zuidema extended sequential lstms to tree-structured lstms by adding branching factors .
marcu and wong proposed a phrase-based context-free joint probability model for lexical mapping .
we use the rouge 1 to evaluate our framework , which has been widely applied for summarization evaluation .
coreference resolution is a complex problem , and successful systems must tackle a variety of non-trivial subproblems that are central to the coreference task — e.g. , mention/markable detection , anaphor identification — and that require substantial implementation efforts .
coreference resolution is the process of linking multiple mentions that refer to the same entity .
we utilize the google news dataset created by mikolov et al , which consists of 300-dimensional vectors for 3 million words and phrases .
specifically , we adopt linear-chain conditional random fields as the method for sequence labeling .
a sentiment lexicon is a list of words and phrases , such as excellent , awful and not bad , each is being assigned with a positive or negative score reflecting its sentiment polarity .
we trained a 4-gram language model on the xinhua portion of gigaword corpus using the sri language modeling toolkit with modified kneser-ney smoothing .
srilm toolkit was used to create up to 5-gram language models using the mentioned resources .
word sense disambiguation ( wsd ) is the task to identify the intended sense of a word in a computational manner based on the context in which it appears ( cite-p-13-3-4 ) .
a 5-gram language model with kneser-ney smoothing was trained with srilm on monolingual english data .
qiu et al propose double propagation to expand opinion targets and opinion words lists in a bootstrapping way .
we used the treetagger tool to extract part-of-speech from each given text , then tokenize and lemmatize it .
we used the phrase-based smt model , as implemented in the moses toolkit , to train an smt system translating from english to arabic .
we begin by building two word alignment models using the berkeley aligner , a state-of-the-art word alignment package that relies on ibm mixture models 1 and 2 and an hmm .
matsuo et al presented a graph cluster-ing algorithm for word clustering based on word similarity measures by web counts .
unsupervised word embeddings trained from large amounts of unlabeled data have been shown to improve many nlp tasks .
with english gigaword corpus , we use the skip-gram model as implemented in word2vec 3 to induce embeddings .
translation performances are measured with case-insensitive bleu4 score .
specifically , we adopt linear-chain conditional random fields as the method for sequence labeling .
we added part of speech and dependency triple annotations to this data using the stanford parser .
for feature building , we use word2vec pre-trained word embeddings .
the srilm toolkit was used to build the 5-gram language model .
recent studies focuses on learning word embeddings for specific tasks , such as sentiment analysis and dependency parsing .
yarowsky proposes a method for word sense disambiguation , which is based on monolingual bootstrapping .
word2vec is the method to obtain distributed representations for a word by using neural networks with one hidden layer .
in particular , neural language models have demonstrated impressive performance at the task of language modeling .
in the first step , we pose a variant of sequential pattern mining problem to identify sequential word patterns that are more common among student answers .
the word embeddings are identified using the standard glove representations .
relation extraction is the task of detecting and classifying relationships between two entities from text .
the subtask of aspect category detection obtains the best performance when applying the boosting method on maxent .
dependency parsing is a fundamental task for language processing which has been investigated for decades .
barzilay and mckeown extracted both single-and multiple-word paraphrases from a sentence-aligned corpus for use in multi-document summarization .
using these representations as features , bansal et al obtained improvements in dependency recovery in the mst parser .
we used the mallet toolkit for generating topic distribution vectors and the weka package for the classification tasks .
relation extraction is the task of finding relationships between two entities from text .
also , while in previous approaches , the features are collected from corpora , those we make use of are retrieved from the lexicon entries .
we estimated 5-gram language models using the sri toolkit with modified kneser-ney smoothing .
sentiment analysis ( sa ) is the determination of the polarity of a piece of text ( positive , negative , neutral ) .
furthermore , xu et al correct false negative instances by using pseudo-relevance feedback to expand the origin knowledge base .
the language model was constructed using the srilm toolkit with interpolated kneser-ney discounting .
the classifier we use in this paper is support vector machines in the implementation of svm light .
we used adam for optimization of the neural models .
the translation quality is evaluated by case-insensitive bleu-4 .
named entity recognition ( ner ) is a challenging learning problem .
we also used word2vec to generate dense word vectors for all word types in our learning corpus .
shen et al extended the hmm-based approach to make it discriminative by making use of conditional random fields .
an effective solution for these problems is the long short-term memory architecture .
the annotation scheme is based on an evolution of stanford dependencies , google universal part-ofspeech tags , and the interset interlingua for morphosyntactic tagsets .
we apply sri language modeling toolkit to train a 4-gram language model with kneser-ney smoothing .
we used a 5-gram language model with modified kneser-ney smoothing implemented using the srilm toolkit .
morphological disambiguation is the task of selecting the correct morphological parse for a given word in a given context .
sentiment analysis is a research area in the field of natural language processing .
we used the svm implementation of scikit learn .
we used l2-regularized logistic regression classifier as implemented in liblinear .
the smt weighting parameters were tuned by mert using the development data .
we build a trigram language model per prompt for the english data using the srilm toolkit and measure the perplexity of translated german answers under that language model .
we use srilm with its default parameters for this purpose .
we employ the pretrained word vector , glove , to obtain the fixed word embedding of each word .
extensive experiments on benchmark datasets show that our approach can train accurate sentiment classifier with less labeled samples .
an hierarchical phrase-based model is a powerful method to cover any format of translation pairs by using synchronous context free grammar .
table 1 summarizes test set performance in bleu , nist and ter .
we use byte pair encoding with 45k merge operations to split words into subwords .
we use the svm implementation from scikit-learn , which in turn is based on libsvm .
katiyar and cardie proposed a neural network-based approach that learns hypergraph representation for nested entities using features extracted from a recurrent neural network .
we have used the srilm with kneser-ney smoothing for training a language model for the first stage of decoding .
pereira et al cluster nouns according to their distribution as direct objects of verbs , using information-theoretic tools .
as a baseline system for our experiments we use the syntax-based component of the moses toolkit .
we apply the 3-phase learning procedure proposed by where we first create word embeddings based on the skip-gram model .
we evaluate the translation quality using the case-sensitive bleu-4 metric .
for the semantic language model , we used the srilm package and trained a tri-gram language model with the default goodturing smoothing .
sentiment classification is a task to predict a sentiment label , such as positive/negative , for a given text and has been applied to many domains such as movie/product reviews , customer surveys , news comments , and social media .
guo et al , 2014 ) considers bilingual datasets to learn sense-specific word representations .
the process of identifying the correct meaning , or sense of a word in context , is known as word sense disambiguation ( wsd ) .
wordnet is a key lexical resource for natural language applications .
we used the moses toolkit to train the phrase tables and lexicalized reordering models .
we used the srilm toolkit to generate the scores with no smoothing .
our translation model is implemented as an n-gram model of operations using srilm-toolkit with kneser-ney smoothing .
for learning coreference decisions , we used a maximum entropy model .
this type of features are based on a trigram model with kneser-ney smoothing .
we use case-sensitive bleu-4 to measure the quality of translation result .
here , we present an effective , expandable , and tractable new approach to comprehensive multiword lexicon acquisition .
we used the srilm toolkit to create 5-gram language models with interpolated modified kneser-ney discounting .
the system was trained using moses with default settings , using a 5-gram language model created from the english side of the training corpus using srilm .
the standard minimum error rate training algorithm was used for tuning .
the language model component uses the srilm lattice-tool for weight assignment and nbest decoding .
sagae and tsujii applied the standard co-training method for dependency parsing .
adding subjectivity labels to wordnet could also support automatic subjectivity analysis .
it is a standard phrasebased smt system built using the moses toolkit .
we implement the pbsmt system with the moses toolkit .
various recent attempts have been made to include non-local features into graph-based dependency parsing .
for both languages , we used the srilm toolkit to train a 5-gram language model using all monolingual data provided .
we used the google news pretrained word2vec word embeddings for our model .
the log-linear parameter weights are tuned with mert on the development set .
the experimental results demonstrate that our approach outperforms the template extraction based approaches .
we apply standard tuning with mert on the bleu score .
relation extraction is a subtask of information extraction that finds various predefined semantic relations , such as location , affiliation , rival , etc. , between pairs of entities in text .
continuous-valued vector representation of words has been one of the key components in neural architectures for natural language processing .
semantic textual similarity is the task of judging the similarity of a pair of sentences on a scale from 1 to 5 .
in this paper , we describe an approach which overcomes this problem using dictionary definitions .
relation extraction is the task of predicting semantic relations over entities expressed in structured or semi-structured text .
our word embeddings is initialized with 100-dimensional glove word embeddings .
duh et al employed the method of and further explored neural language model for data selection rather than the conventional n-gram language model .
sentiment classification is the fundamental task of sentiment analysis ( cite-p-15-3-11 ) , where we are to classify the sentiment of a given text .
for this task , we used the svm implementation provided with the python scikit-learn module .
coreference resolution is the task of determining when two textual mentions name the same individual .
the probabilistic language model is constructed on google web 1t 5-gram corpus by using the srilm toolkit .
all language models were trained using the srilm toolkit .
the decoding weights are optimized with minimum error rate training to maximize bleu scores .
the context clustering approach was pioneered by sch眉tze who used second order co-occurrences to construct the context embedding .
text categorization is the problem of automatically assigning predefined categories to free text documents .
for training the trigger-based lexicon model , we apply the expectation-maximization algorithm .
generative models like lda and plsa have been proved to be very successful in modeling topics and other textual information in an unsupervised manner .
luong et al learn word representations based on morphemes that are obtained from an external morphological segmentation system .
these language models were built up to an order of 5 with kneser-ney smoothing using the srilm toolkit .
sentiment analysis is a ‘ suitcase ’ research problem that requires tackling many nlp subtasks , e.g. , aspect extraction ( cite-p-26-3-15 ) , named entity recognition ( cite-p-26-3-6 ) , concept extraction ( cite-p-26-3-20 ) , sarcasm detection ( cite-p-26-3-16 ) , personality recognition ( cite-p-26-3-7 ) , and more .
we use the selectfrommodel 4 feature selection method as implemented in scikit-learn .
relation extraction is a core task in information extraction and natural language understanding .
coreference resolution is a complex problem , and successful systems must tackle a variety of non-trivial subproblems that are central to the coreference task — e.g. , mention/markable detection , anaphor identification — and that require substantial implementation efforts .
we use the l2-regularized logistic regression of liblinear as our term candidate classifier .
a knsmoothed 5-gram language model is trained on the target side of the parallel data with srilm .
in addition , we describe an approach to crowdsourcing ideological bias annotations .
callison-burch et al used pivot languages for paraphrase extraction to handle the unseen phrases for phrase-based smt .
the parameters are initialized by the techniques described in .
unpruned language models were trained using lmplz which employs modified kneser-ney smoothing .
kim et al and kulkarni et al computed the degree of meaning change by applying neural networks for word representation .
finally , we combine all the above features using a support vector regression model which is implemented in scikit-learn .
for monolingual treebank data we relied on the conll-x and conll-2007 shared tasks on dependency parsing .
we do perform word segmentation in this work , using the stanford tools .
we use srilm for training a trigram language model on the english side of the training data .
lexical simplification is a technique that substitutes a complex word or phrase in a sentence with a simpler synonym .
negation is a grammatical category that comprises devices used to reverse the truth value of propositions .
word alignment is a critical first step for building statistical machine translation systems .
the language model is a 5-gram lm with modified kneser-ney smoothing .
for language modeling , we use the english gigaword corpus with 5-gram lm implemented with the kenlm toolkit .
collobert et al used word embeddings as input to a deep neural network for multi-task learning .
these results show that phrase structure trees , when viewed in certain ways , have much more descriptive power than one would have thought .
the trec documents were converted from html to raw text , and both collections were tokenised using bio-specific nlp tools .
in this paper , we propose a novel neural system combination framework for machine translation .
hatzivassiloglou and mckeown proposed a method for identifying word polarity of adjectives .
for our implementation we use 300-dimensional part-of-speech-specific word embeddings v i generated using the gensim word2vec package .
crowdsourcing is a viable mechanism for creating training data for machine translation .
the target language model was a trigram language model with modified kneser-ney smoothing trained on the english side of the bitext using the srilm tookit .
as a countbased baseline , we use modified kneser-ney as implemented in kenlm .
we use the word2vec skip-gram model to learn initial word representations on wikipedia .
lexical simplification is a popular task in natural language processing and it was the topic of a successful semeval task in 2012 ( cite-p-14-1-9 ) .
chapman et al developed negex , a simple regular expression-based algorithm to determine whether a finding or disease mentioned within medical reports was present or absent .
we used a logistic regression classifier provided by the liblinear software .
firstly , at word-level alignment , luong et al extend the skip-gram model to learn efficient bilingual word embeddings .
in this paper , we propose a cache-based approach to document-level translation .
we used the target side of the parallel corpus and the srilm toolkit to train a 5-gram language model .
the grammatical relations are all the collapsed dependencies produced by the stanford dependency parser .
there has been a substantial amount of work on automatic semantic role labeling , starting with the statistical model of gildea and jurafsky .
we use the sri language model toolkit to train a 5-gram model with modified kneser-ney smoothing on the target-side training corpus .
we apply sri language modeling toolkit to train a 4-gram language model with kneser-ney smoothing .
the translation systems were evaluated by bleu score .
all language models are created with the srilm toolkit and are standard 4-gram lms with interpolated modified kneser-ney smoothing .
to minimize the objective , we use stochastic gradient descent with the diagonal variant of adagrad .
models are evaluated in terms of bleu , meteor and ter on tokenized , cased test data .
the experiments were carried out using the chinese-english datasets provided within the iwslt 2006 evaluation campaign , extracted from the basic travel expression corpus .
a pun is a form of wordplay in which one sign ( e.g. , a word or phrase ) suggests two or more meanings by exploiting polysemy , homonymy , or phonological similarity to another sign , for an intended humorous or rhetorical effect ( aarons , 2017 ; hempelmann and miller , 2017 ) .
using espac medlineplus , we trained an initial phrase-based moses system .
relation extraction is the task of extracting semantic relationships between entities in text , e.g . to detect an employment relationship between the person larry page and the company google in the following text snippet : google ceo larry page holds a press announcement at its headquarters in new york on may 21 , 2012 .
recent studies have also shown that the capability to automatically identify problematic situations during interaction can significantly improve the system performance .
the translation results are evaluated by caseinsensitive bleu-4 metric .
a trigram language model with modified kneser-ney discounting and interpolation was used as produced by the srilm toolkit .
all english data are pos tagged and lemmatised using the treetagger .
we use moses , an open source toolkit for training different systems .
we present the text to the encoder as a sequence of word2vec word embeddings from a word2vec model trained on the hrwac corpus .
we use the glove vector representations to compute cosine similarity between two words .
we used the case-insensitive bleu-4 to evaluate translation quality and run mert three times .
to this end , we use first-and second-order conditional random fields .
itspoke is a speech-enabled version of the text-based why2-atlas conceptual physics tutoring system .
word sense disambiguation ( wsd ) is the nlp task that consists in selecting the correct sense of a polysemous word in a given context .
our phrase-based system is similar to the alignment template system described by och and ney .
the target language model is built on the target side of the parallel data with kneser-ney smoothing using the irstlm tool .
we use the moses software package 5 to train a pbmt model .
collobert and weston , 2008 , proposed a multitask neural network trained jointly on the relevant tasks using weight-sharing .
we used a 5-gram language model with modified kneser-ney smoothing , built with the srilm toolkit .
the penn discourse treebank is the largest available discourseannotated resource in english .
table 1 summarizes test set performance in bleu , nist and ter .
mcclosky et al presented a successful instance of parsing with self-training by using a reranker .
we use the glove word vector representations of dimension 300 .
this task is called sentence compression .
bleu is an established and the most widely used automatic metric for evaluation of mt quality .
word sense disambiguation ( wsd ) is the task of determining the correct meaning for an ambiguous word from its context .
the target language model was a standard ngram language model trained by the sri language modeling toolkit .
tomanek et al utilised eye-tracking data to evaluate the degree of difficulty in annotating named entities .
we initialize the embedding weights by the pre-trained word embeddings with 200 dimensional vectors .
sentiment analysis ( sa ) is the task of determining the sentiment of a given piece of text .
we used the srilm toolkit to train a 4-gram language model on the english side of the training corpus .
socher et al train a composition function using a neural network-however their method requires annotated data .
the srilm toolkit is used to train 5-gram language model .
we use a bidirectional long short-term memory rnn to encode a sentence .
previous work has shown that unlabeled text can be used to induce unsupervised word clusters which can improve the performance of many supervised nlp tasks .
in section 4 , through experiments on multiple real-world datasets , we observe that sictf is not only more accurate than kb-lda but also significantly faster with a speedup of 14x .
we build an open-vocabulary language model with kneser-ney smoothing using the srilm toolkit .
incometo select the most fluent path , we train a 5-gram language model with the srilm toolkit on the english gigaword corpus .
luong et al , 2013 ) utilized recursive neural networks in which inputs are morphemes of words .
for word representation , we train the skip-gram word embedding on each dataset separately to initialize the word vectors .
when parsers are trained on ptb , we use the stanford pos tagger .
we used the scikit-learn implementation of svrs and the skll toolkit .
we used the phrasebased translation system in moses 5 as a baseline smt system .
for this task , we use the widely-used bleu metric .
coreference resolution is the task of automatically grouping references to the same real-world entity in a document into a set .
lstms were introduced by hochreiter and schmidhuber in order to mitigate the vanishing gradient problem .
the language model was constructed using the srilm toolkit with interpolated kneser-ney discounting .
we evaluate text generated from gold mr graphs using the well-known bleu measure .
we conduct experiments using these word embeddings with maltparser and maltoptimizer .
grammar induction is the task of learning grammatical structure from plain text without human supervision .
the selected plain sentence pairs are further parsed by stanford parser on both the english and chinese sides .
the language models were built using srilm toolkits .
we use glove vectors with 100 dimensions trained on wikipedia and gigaword as word embeddings .
in this work , we use the expectation-maximization algorithm .
we used a 5-gram language model with modified kneser-ney smoothing , built with the srilm toolkit .
abstract meaning representation is a compact , readable , whole-sentence semantic annotation .
according to lakoff and johnson , metaphor is a productive phenomenon that operates at the level of mental processes .
bunescu and mooney propose a shortest path dependency kernel for relation extraction .
we train a 4-gram language model on the xinhua portion of english gigaword corpus by srilm toolkit .
pitler and nenkova show that discourse coherence features are more informative than other features for ranking texts with respect to their readability .
we use two standard evaluation metrics bleu and ter , for comparing translation quality of various systems .
we use sri language model toolkit to train a 5-gram model with modified kneser-ney smoothing on the target-side training corpus .
for the semantic language model , we used the srilm package and trained a tri-gram language model with the default goodturing smoothing .
mert was used to tune development set parameter weights and bleu was used on test sets to evaluate the translation performance .
we first train a word2vec model on fr-wikipedia 11 to obtain non contextual word vectors .
the weights of these features are then learned using a discriminative training algorithm .
for improving the word alignment , we use the word-classes that are trained from a monolingual corpus using the srilm toolkit .
moreover , since event coreference resolution is a complex task that involves exploring a rich set of linguistic features , annotating a large corpus with event coreference information for a new language or domain of interest requires a substantial amount of manual effort .
we further used a 5-gram language model trained using the srilm toolkit with modified kneser-ney smoothing .
in this work , we propose a multi-space variational encoder-decoder framework for labeled sequence transduction problem .
the system was trained using moses with default settings , using a 5-gram language model created from the english side of the training corpus using srilm .
we use the 300-dimensional skip-gram word embeddings built on the google-news corpus .
relation extraction is the task of predicting semantic relations over entities expressed in structured or semi-structured text .
aspect extraction is a central problem in sentiment analysis .
in this work , we are interested in selective sampling for pool-based active learning , and focus on uncertainty sampling .
sentiment analysis is a much-researched area that deals with identification of positive , negative and neutral opinions in text .
for this task , we use glove pre-trained word embedding trained on common crawl corpus .
twitter is a widely used microblogging environment which serves as a medium to share opinions on various events and products .
the language model component uses the srilm lattice-tool for weight assignment and nbest decoding .
a knsmoothed 5-gram language model is trained on the target side of the parallel data with srilm .
the parse trees for sentences in the test set were obtained using the stanford parser .
specifically , we adopt linear-chain conditional random fields as the method for sequence labeling .
we use the adaptive moment estimation for the optimizer .
word embeddings are initialized with pretrained glove vectors 2 , and updated during the training .
we use word embedding pre-trained on newswire with 300 dimensions from word2vec .
multi-task joint learning can transfer knowledge between tasks by sharing task-invariant layers .
word sense disambiguation ( wsd ) is the task to identify the intended sense of a word in a computational manner based on the context in which it appears ( cite-p-13-3-4 ) .
we use glove vectors with 200 dimensions as pre-trained word embeddings , which are tuned during training .
we use stanford corenlp for chinese word segmentation and pos tagging .
finally , we construct new subtree-based features for parsing algorithms .
dependency parsing is a core task in nlp , and it is widely used by many applications such as information extraction , question answering , and machine translation .
we use 4-gram language models in both tasks , and conduct minimumerror-rate training to optimize feature weights on the dev set .
in this run , we use a sentence vector derived from word embeddings obtained from word2vec .
koo et al and suzuki et al use unsupervised wordclusters as features in a dependency parser to get lexical dependencies .
word alignment is a key component in most statistical machine translation systems .
marcu and wong , 2002 ) presents a joint probability model for phrase-based translation .
we used the penn treebank wsj corpus to perform empirical experiments on the proposed parsing models .
we use the pre-trained glove vectors to initialize word embeddings .
the phrase structure trees produced by the parser are further processed with the stanford conversion tool to create dependency graphs .
relation extraction ( re ) is the task of assigning a semantic relationship between a pair of arguments .
our approach relies on long short-term memory networks .
coreference resolution is the task of clustering a set of mentions in the text such that all mentions in the same cluster refer to the same entity .
as a learning algorithm for our classification model , we used maximum entropy .
the primary contribution of this paper is a novel technique— cube summing—for approximate summing over discrete structures with non-local features , which we relate to cube pruning ( §4 ) .
for estimating monolingual word vector models , we use the cbow algorithm as implemented in the word2vec package using a 5-token window .
we use 300-dimensional glove vectors trained on 6b common crawl corpus as word embeddings , setting the embeddings of outof-vocabulary words to zero .
the bleu metric was used to automatically evaluate the quality of the translations .
models were built and interpolated using srilm with modified kneser-ney smoothing and the default pruning settings .
word embeddings have proven to be effective models of semantic representation of words in various nlp tasks .
coreference resolution is the task of clustering referring expressions in a text so that each resulting cluster represents an entity .
for this task , we used the svm implementation provided with the python scikit-learn module .
we use bleu scores to measure translation accuracy .
semantic parsing is the problem of deriving a structured meaning representation from a natural language utterance .
marcu and wong , 2002 ) presents a joint probability model for phrase-based translation .
relation extraction is a subtask of information extraction that finds various predefined semantic relations , such as location , affiliation , rival , etc. , between pairs of entities in text .
as a strong baseline , we trained the skip-gram model of mikolov et al using the publicly available word2vec 5 software .
for parameter training we use conditional random fields as described in .
we used moses as the phrase-based machine translation system .
our word embeddings is initialized with 100-dimensional glove word embeddings .
we evaluated the intermediate outputs using bleu against human references as in table 3 .
we used a 5-gram language model with modified kneser-ney smoothing implemented using the srilm toolkit .
we used the pre-trained word embeddings that were learned using the word2vec toolkit on google news dataset .
information extraction ( ie ) is the process of finding relevant entities and their relationships within textual documents .
clark and curran evaluate a number of log-linear parsing models for ccg .
abstract meaning representation is a semantic formalism which represents sentence meaning in a form of a rooted directed acyclic graph .
transition-based dependency parsers scan an input sentence from left to right , performing a sequence of transition actions to predict its parse tree .
we used srilm to build a 4-gram language model with interpolated kneser-ney discounting .
we used the sri language modeling toolkit for this purpose .
evaluation was performed using the bleu metric .
named entity recognition ( ner ) is a key technique for ie and other natural language processing tasks .
relation extraction is the task of finding semantic relations between two entities from text .
we employ conditional random fields to predict the sentiment label for each segment .
we use three common evaluation metrics including bleu , me-teor , and ter .
sentiment analysis ( sa ) is a field of knowledge which deals with the analysis of people ’ s opinions , sentiments , evaluations , appraisals , attitudes and emotions towards particular entities ( liu , 2012 ) .
we use the perplexity computation method of mikolov et al suitable for skip-gram models .
we used the svd implementation provided in the scikit-learn toolkit .
we propose a novel geolocation prediction model using a complex neural network .
instead , we compute the relatedness of two words based on their distributed representations , which are learned using the word2vec toolkit .
for representing words , we used 100 dimensional pre-trained glove embeddings .
we report the findings of the complex word identification task of semeval 2016 .
we also use a 4-gram language model trained using srilm with kneser-ney smoothing .
however , dependency parsing , which is a popular choice for japanese , can incorporate only shallow syntactic information , i.e. , pos tags , compared with the richer syntactic phrasal categories in constituency parsing .
we apply standard tuning with mert on the bleu score .
all language models were trained using the srilm toolkit .
we use the long short-term memory architecture for recurrent layers .
we use 300-dimensional word embeddings from glove to initialize the model .
these features were optimized using minimum error-rate training and the same weights were then used in docent .
relation classification is the task of assigning sentences with two marked entities to a predefined set of relations .
the translation quality is evaluated by case-insensitive bleu-4 .
translation performance is measured using the automatic bleu metric , on one reference translation .
the input to the network is the embeddings of words , and we use the pre-trained word embeddings by using word2vec on the wikipedia corpus whose size is over 11g .
framenet is an expert-built lexical-semantic resource incorporating the theory of frame-semantics .
in this paper , we propose a novel approach for disfluency detection .
sentiment analysis is a collection of methods and algorithms used to infer and measure affection expressed by a writer .
we use the automatic mt evaluation metrics bleu , meteor , and ter , to evaluate the absolute translation quality obtained .
for our purpose we use word2vec embeddings trained on a google news dataset and find the pairwise cosine distances for all words .
word embeddings for english and hindi have been trained using word2vec 1 tool .
our results show a consistent improvement over a state-of-the-art baseline in terms of bleu and a manual error analysis .
information extraction ( ie ) is a main nlp aspects for analyzing scientific papers , which includes named entity recognition ( ner ) and relation extraction ( re ) .
bilingual lexicons serve as an indispensable source of knowledge for various cross-lingual tasks such as cross-lingual information retrieval or statistical machine translation .
state of the art statistical parsers are trained on manually annotated treebanks that are highly expensive to create .
for both languages , we used the srilm toolkit to train a 5-gram language model using all monolingual data provided .
dependency parsing is a way of structurally analyzing a sentence from the viewpoint of modification .
transition-based dependency parsing was originally introduced by yamada and matsumoto and nivre .
each system is optimized using mert with bleu as an evaluation measure .
we use the svm implementation from scikit-learn , which in turn is based on libsvm .
furthermore , the objective function for our simplest model is concave , guaranteeing convergence to a global optimum .
for the mix one , we also train word embeddings of dimension 50 using glove .
coreference resolution is the problem of identifying which mentions ( i.e. , noun phrases ) refer to which real-world entities .
chandar a p et al and zhou et al use the autoencoder to model the connections between bilingual sentences .
the language models were trained using srilm toolkit .
one of the very few available discourse annotated corpora is the penn discourse treebank in english .
we argue that relevance for satisfaction , contrastive weight clues , and certain adverbials work to affect the polarity , as evidenced by the statistical analysis .
for the fluency and grammaticality features , we train 4-gram lms using the development dataset with the sri toolkit .
we evaluated the translation quality using the case-insensitive bleu-4 metric .
word alignment is a central problem in statistical machine translation ( smt ) .
in our experiments , we choose to use the published glove pre-trained word embeddings .
in this paper , we proposed a novel framework to tackle the problem of list-only entity linking .
in our work , we use lda to identify the subtopics in the given body of texts .
we used svm multiclass from svm-light toolkit as the classifier .
berland and charniak used similar pattern-based techniques and other heuristics to extract meronymy relations .
a hybrid model of the word-based and the character-based model has also been proposed by luong and manning .
recently , methods inspired by neural language modeling received much attentions for representation learning .
hochreiter and schmidhuber developed long short-term memory to overcome the long term dependency problem .
aspect extraction is a central problem in sentiment analysis .
soricut and echihabi proposed document-aware features in order to rank machine translated documents .
we use srilm to train a 5-gram language model on the target side of our training corpus with modified kneser-ney discounting .
we train a kn-smoothed 5-gram language model on the target side of the parallel training data with srilm .
the binary syntactic features were automatically extracted using the stanford parser .
word sense disambiguation is the process of determining which sense of a word is used in a given context .
language models were built with srilm , modified kneser-ney smoothing , default pruning , and order 5 .
our transition-based parser is based on a study by zhu et al , which adopts the shift-reduce parsing of sagae and lavie and zhang and clark .
in addition , we use an english corpus of roughly 227 million words to build a target-side 5-gram language model with srilm in combination with kenlm .
gram language model with modified kneser-ney smoothing is trained with the srilm toolkit on the epps , ted , newscommentary , and the gigaword corpora .
bannard and callison-burch introduced the pivot approach to extracting paraphrase phrases from bilingual parallel corpora .
to train the models we use the default stochastic gradient descent classifier provided by scikit-learn .
we use the glove word vector representations of dimension 300 .
to measure the translation quality , we use the bleu score and the nist score .
the language model was constructed using the srilm toolkit with interpolated kneser-ney discounting .
we use the 300-dimensional pre-trained word2vec 3 word embeddings and compare the performance with that of glove 4 embeddings .
chen et al extracted different types of subtrees from the auto-parsed data and used them as new features in standard learning methods .
we use 300 dimension word2vec word embeddings for the experiments .
the probabilistic language model is constructed on google web 1t 5-gram corpus by using the srilm toolkit .
we use skip-gram representation for the training of word2vec tool .
sentiment analysis is a natural language processing task whose aim is to classify documents according to the opinion ( polarity ) they express on a given subject ( cite-p-13-8-14 ) .
to tune feature weights minimum error rate training is used , optimized against the neva metric .
al-onaizan and knight proposed a spelling-based model which directly maps english letter sequences into arabic letter sequences .
relation extraction is a crucial task in the field of natural language processing ( nlp ) .
the fundamental work for the pattern-based approaches is that of hearst .
parameter optimisation is done by mini-batch stochastic gradient descent where back-propagation is performed using adadelta update rule .
we used a 5-gram language model with modified kneser-ney smoothing , built with the srilm toolkit .
for language modeling , we use the english gigaword corpus with 5-gram lm implemented with the kenlm toolkit .
an effective alternative , which however only delivers unnormalized scores , is to train the network using the noise contrastive estimation denoted by nce in the rest of the paper .
we then evaluate the effect of word alignment on machine translation quality using the phrase-based translation system moses .
the basic model of the our system is a log-linear model .
accordingly , we use an adaptive recurrence mechanism to learn a dynamic node representation through attention structure .
our trigram word language model was trained on the target side of the training corpus using the srilm toolkit with modified kneser-ney smoothing .
we use mini-batch update and adagrad to optimize the parameter learning .
we initialize our word vectors with 300-dimensional word2vec word embeddings .
pun is a figure of speech that consists of a deliberate confusion of similar words or phrases for rhetorical effect , whether humorous or serious .
in this paper , we explore an implicit content-introducing method for generative short-text conversation system .
all the feature weights were trained using our implementation of minimum error rate training .
we first train a word2vec model on fr-wikipedia 11 to obtain non contextual word vectors .
incometo select the most fluent path , we train a 5-gram language model with the srilm toolkit on the english gigaword corpus .
we trained a 5-grams language model by the srilm toolkit .
we used google pre-trained word embedding with 300 dimensions .
the 'grammar ' consists of a lexicon where each lexical item is associated with a finite number of structures for which that item is the 'head ' .
word sense disambiguation is the process of determining which sense of a word is used in a given context .
we measured the overall translation quality with 4-gram bleu , which was computed on tokenized and lowercased data for all systems .
parameter tuning was carried out using both k-best mira and minimum error rate training on a held-out development set .
coreference resolution is the problem of partitioning a sequence of noun phrases ( or mentions ) , as they occur in a natural language text , into a set of referential entities .
our cdsm feature is based on word vectors derived using a skip-gram model .
we used trigram language models with interpolated kneser-kney discounting trained using the sri language modeling toolkit .
twitter is a widely used microblogging platform , where users post and interact with messages , “ tweets ” .
for word embeddings , we trained a skip-gram model over wikipedia , using word2vec .
we use the long short-term memory architecture for recurrent layers .
for english posts , we used the 200d glove vectors as word embeddings .
word sense disambiguation ( wsd ) is a particular problem of computational linguistics which consists in determining the correct sense for a given ambiguous word .
we tokenized and part-of-speech tagged the tweets with the carnegie mellon university twitter nlp tool .
semantic role labeling ( srl ) is the process of producing such a markup .
finally , we combine all the above features using a support vector regression model which is implemented in scikit-learn .
we use the pre-trained 300-dimensional word2vec embeddings trained on google news 1 as input features .
we adapt the models of mikolov et al and mikolov et al to infer feature embeddings .
stance detection has been defined as automatically detecting whether the author of a piece of text is in favor of the given target or against it .
we used the moses toolkit to build mt systems using various alignments .
socher et al introduced a family of recursive neural networks to represent sentence-level semantic composition .
finkel and manning propose a discriminative parsingbased method for nested named entity recognition , employing crfs as its core .
for generating the translations from english into german , we used the statistical translation toolkit moses .
socher et al introduce a family of recursive neural networks for sentence-level semantic composition .
we trained a linear log-loss model using stochastic gradient descent learning as implemented in the scikit learn library .
we used glove word embeddings with 300 dimensions pre-trained using commoncrawl to get a vector representation of the evidence sentence .
we first train a word2vec model on fr-wikipedia 11 to obtain non contextual word vectors .
dependency parsing is the task to assign dependency structures to a given sentence math-w-4-1-0-14 .
here we adopt the greedy feature selection algorithm as described in jiang and ng to select useful features empirically and incrementally according to their contributions on the development data .
we used moses as the phrase-based machine translation system .
incometo select the most fluent path , we train a 5-gram language model with the srilm toolkit on the english gigaword corpus .
once again , segmentation is the part of the process where the automatic algorithms most seriously underperform .
erbach , barg and walther and fouvry followed a unification-based symbolic approach to unknown word processing for constraint-based grammars .
readability is used to provide users with high-quality service in text recommendation or text visualization .
we used the svm implementation provided within scikit-learn .
in this paper we present a method for using lsa analysis to initialize a plsa model .
morphological tagging is the task of assigning a morphological analysis to a token in context .
the evaluation metric is the case-insensitive bleu4 .
nallapati et al also employed the typical attention modeling based seq2seq framework , but utilized a trick to control the vocabulary size to improve the training efficiency .
word sense disambiguation ( wsd ) is the task of determining the correct meaning for an ambiguous word from its context .
to implement svm algorithm , we have used the publicly available python based scikit-learn package .
segmentation is the task of dividing a stream of data ( text or other media ) into coherent units .
coreference resolution is the task of determining which mentions in a text refer to the same entity .
shen et al proposed a target dependency language model for smt to employ target-side structured information .
word embeddings have shown promising results in nlp tasks , such as named entity recognition , sentiment analysis or parsing .
we employ word2vec as the unsupervised feature learning algorithm , based on a raw corpus of over 90 million messages extracted from chinese weibo platform .
relation extraction is the task of predicting semantic relations over entities expressed in structured or semi-structured text .
twitter is a popular microblogging service , which , among other things , is used for knowledge sharing among friends and peers .
irony is a particular type of figurative language in which the meaning is often the opposite of what is literally said and is not always evident without context or existing knowledge .
xu et al and yu and dredze exploited semantic knowledge to improve the semantic representation of word embeddings .
we used srilm to build a 4-gram language model with interpolated kneser-ney discounting .
we use sri language model toolkit to train a 5-gram model with modified kneser-ney smoothing on the target-side training corpus .
we use the srilm toolkit to compute our language models .
for data preparation and processing we use scikit-learn .
we estimate a 5-gram language model using interpolated kneser-ney discounting with srilm .
to learn the topics we use latent dirichlet allocation .
for the word-embedding based classifier , we use the glove pre-trained word embeddings .
word alignment is a fundamental problem in statistical machine translation .
a context-free grammar ( cfg ) is a tuple math-w-2-5-5-22 , where vn and vt are finite , disjoint sets of nonterminal and terminal symbols , respectively , and s e vn is the start symbol .
named entity ( ne ) recognition is a task in which proper nouns and numerical information in a document are detected and classified into categories such as person , organization , location , and date .
we use the linearsvc classifier as implemented in scikit-learn package 17 with the default parameters .
the word embeddings can provide word vector representation that captures semantic and syntactic information of words .
the binary syntactic features were automatically extracted using the stanford parser .
the pipeline is based on the uima framework and contains many text analysis components .
for learning coreference decisions , we used a maximum entropy model .
relation extraction ( re ) is the task of extracting semantic relationships between entities in text .
we use word embeddings of dimension 100 pretrained using word2vec on the training dataset .
for the classifiers we use the scikit-learn machine learning toolkit .
part-of-speech ( pos ) tagging is a fundamental language analysis task .
twitter is a widely used microblogging platform , where users post and interact with messages , “ tweets ” .
we evaluated the system using bleu score on the test set .
lda is a widely used topic model , which views the underlying document distribution as having a dirichlet prior .
the parameters of the systems were tuned using mert to optimize bleu on the development set .
we use stanford corenlp for pos tagging and lemmatization .
the translation quality is evaluated by case-insensitive bleu-4 .
we used the target side of the parallel corpus and the srilm toolkit to train a 5-gram language model .
the language model was constructed using the srilm toolkit with interpolated kneser-ney discounting .
for the language model , we used sri language modeling toolkit to train a trigram model with modified kneser-ney smoothing on the 31 , 149 english sentences .
we employ support vector machines to perform the classification .
for word embeddings , we trained a skip-gram model over wikipedia , using word2vec .
coreference resolution is a field in which major progress has been made in the last decade .
takamura et al proposed using spin models for extracting semantic orientation of words .
for both languages , we used the srilm toolkit to train a 5-gram language model using all monolingual data provided .
we used the moses toolkit with its default settings to build three phrase-based translation systems .
we use the sri language model toolkit to train a 5-gram model with modified kneser-ney smoothing on the target-side training corpus .
word sense disambiguation ( wsd ) is the task of assigning sense tags to ambiguous lexical items ( lis ) in a text .
the translations were evaluated with the widely used bleu and nist scores .
we used 100 dimensional glove embeddings for this purpose .
sentence compression is a task of creating a short grammatical sentence by removing extraneous words or phrases from an original sentence while preserving its meaning .
the trigram models were created using the srilm toolkit on the standard training sections of the ccgbank , with sentence-initial words uncapitalized .
zeng et al use a convolutional deep neural network to extract lexical features learned from word embeddings and then fed into a softmax classifier to predict the relationship between words .
we pre-processed the data to add part-ofspeech tags and dependencies between words using the stanford parser .
we used the first-stage pcfg parser of charniak and johnson for english and bitpar for german .
sentiment analysis ( sa ) is the task of analysing opinions , sentiments or emotions expressed towards entities such as products , services , organisations , issues , and the various attributes of these entities ( cite-p-9-3-3 ) .
choi and cardie assert that the sentiment polarity of natural language can be better inferred by compositional semantics .
word sense disambiguation ( wsd ) is the task of determining the correct meaning for an ambiguous word from its context .
since their introduction at the beginning of the twenty-first century , phrase-based translation models have become the state-of-the-art for statistical machine translation .
the language models are 4-grams with modified kneser-ney smoothing which have been trained with the srilm toolkit .
semantic role labeling ( srl ) is defined as the task to recognize arguments for a given predicate and assign semantic role labels to them .
minimum error rate training is applied to tune the cn weights .
the learning rate was automatically adjusted using adam .
we use mini-batch update and adagrad to optimize the parameter learning .
previous work consistently reported that the word-based translation models yielded better performance than the traditional methods for question retrieval .
semantic parsing is the task of translating natural language utterances into a machine-interpretable meaning representation ( mr ) .
we evaluated the translation quality using the case-insensitive bleu-4 metric .
recursive neural network and convolutional neural network have proven powerful in relation classification .
conditional random fields are a class of graphical models which are undirected and conditionally trained .
for the semantic language model , we used the srilm package and trained a tri-gram language model with the default goodturing smoothing .
we utilize minimum error rate training to optimize feature weights of the paraphrasing model according to ndcg .
to implement svm algorithm , we have used the publicly available python based scikit-learn package .
our a ∗ algorithm is 5 times faster than cky parsing , with no loss in accuracy .
in our implementation , we train a tri-gram language model on each phone set using the srilm toolkit .
twitter is a huge microblogging service with more than 500 million tweets per day from different locations of the world and in different languages ( cite-p-8-1-9 ) .
we implemented the different aes models using scikit-learn .
we use the scikit-learn toolkit as our underlying implementation .
zeng et al use a convolutional deep neural network to extract lexical features learned from word embeddings and then fed into a softmax classifier to predict the relationship between words .
we use conditional random fields sequence labeling as described in .
we estimate a 5-gram language model using interpolated kneser-ney discounting with srilm .
text summarization is the process of generating a short version of a given text to indicate its main topics .
we use the linearsvc classifier as implemented in scikit-learn package 17 with the default parameters .
for the mix one , we also train word embeddings of dimension 50 using glove .
semantic role labeling ( srl ) is a task of analyzing predicate-argument structures in texts .
relation extraction is a traditional information extraction task which aims at detecting and classifying semantic relations between entities in text ( cite-p-10-1-18 ) .
the evaluation metric for the overall translation quality is caseinsensitive bleu4 .
both systems are phrase-based smt models , trained using the moses toolkit .
word sense disambiguation ( wsd ) is a key enabling-technology that automatically chooses the intended sense of a word in context .
the srilm toolkit was used for training the language models using kneser-ney smoothing .
our translation model is implemented as an n-gram model of operations using the srilm toolkit with kneser-ney smoothing .
twitter is a huge microbloging service with more than 500 million tweets per day 1 from different locations in the world and in different languages .
we initialize the embedding layer using embeddings from dedicated word embedding techniques word2vec and glove .
a sentiment lexicon is a list of words and phrases , such as excellent , awful and not bad , each is being assigned with a positive or negative score reflecting its sentiment polarity .
importantly , word embeddings have been effectively used for several nlp tasks .
distributional semantic models produce vector representations which capture latent meanings hidden in association of words in documents .
sentiment analysis is a natural language processing ( nlp ) task ( cite-p-10-3-0 ) which aims at classifying documents according to the opinion expressed about a given subject ( federici and dragoni , 2016a , b ) .
word segmentation is a fundamental task for chinese language processing .
we used moses , a phrase-based smt toolkit , for training the translation model .
the annotation is based on the google universal part-ofspeech tags and the stanford dependencies , adapted and harmonized across languages .
to tackle this problem , hochreiter and schmidhuber proposed long short term memory , which uses a cell with input , forget and output gates to prevent the vanishing gradient problem .
for the pos-tagger , we trained hunpos 10 with the wall street journal english corpus .
we use opennmt , which is an implementation of the popular nmt approach that uses an attentional encoder-decoder network .
we used glove word embeddings with 300 dimensions pre-trained using commoncrawl to get a vector representation of the evidence sentence .
parsers are reporting impressive numbers these days , but coordination remains an area with room for improvement .
finally , we apply several unsupervised and supervised techniques of sentiment composition to determine their efficacy on this dataset .
we used a 5-gram language model with modified kneser-ney smoothing , built with the srilm toolkit .
we build a 9-gram lm using srilm toolkit with modified kneser-ney smoothing .
blitzer et al used structural correspondence learning to train a classifier on source data with new features induced from target unlabeled data .
mimno et al proposed a closely-related method for evaluating semantic coherence , replacing pmi with log conditional probability .
ccgs are a linguistically-motivated formalism for modeling a wide range of language phenomena .
we used moses for pbsmt and hpbsmt systems in our experiments .
for language model , we used sri language modeling toolkit to train a 4-gram model with modified kneser-ney smoothing .
luong and manning use transfer learning to adapt a general model to indomain data .
sentiment analysis is a collection of methods and algorithms used to infer and measure affection expressed by a writer .
the word embedding is pre-trained using the skip-gram model in word2vec and fine-tuned during the learning process .
coreference resolution is the task of clustering referring expressions in a text so that each resulting cluster represents an entity .
coreference resolution is a fundamental component of natural language processing ( nlp ) and has been widely applied in other nlp tasks ( cite-p-15-3-9 ) .
in our experiments the mt system used is hierarchical phrase-based system .
stance detection is a difficult task since it often requires reasoning in order to determine whether an utterance is in favor of or against a specific issue .
word sense disambiguation ( wsd ) is the task of determining the correct meaning ( “ sense ” ) of a word in context , and several efforts have been made to develop automatic wsd systems .
we use 300-dimensional glove vectors trained on 6b common crawl corpus as word embeddings , setting the embeddings of outof-vocabulary words to zero .
we use distributed word vectors trained on the wikipedia corpus using the word2vec algorithm .
sentiment classification is a special task of text categorization that aims to classify documents according to their opinion of , or sentiment toward a given subject ( e.g. , if an opinion is supported or not ) ( cite-p-11-1-2 ) .
thus , we propose a new approach based on the expectation-maximization algorithm .
the latent dirichlet allocation is a topic model that is assumed to provide useful information for particular subtasks .
recently , the field has been influenced by the success of neural language models .
for the source side we use the pos tags from stanford corenlp mapped to universal pos tags .
the process of identifying the correct meaning , or sense of a word in context , is known as word sense disambiguation ( wsd ) .
word segmentation is the foremost obligatory task in almost all the nlp applications where the initial phase requires tokenization of input into words .
sentiment classification is the fundamental task of sentiment analysis ( cite-p-15-3-11 ) , where we are to classify the sentiment of a given text .
semantic role labeling ( srl ) is the task of automatically labeling predicates and arguments in a sentence with shallow semantic labels .
sentence compression is the task of compressing long sentences into short and concise ones by deleting words .
automatic evaluation shows that our system is both less repetitive and more diverse than baselines .
barzilay and mckeown identify multi-word paraphrases from a sentence-aligned corpus of monolingual parallel texts .
as our baseline , we apply a high-performing chinese-english mt system based on hierarchical phrase-based translation framework .
for this purpose , we used phrase tables learned by the standard statistical mt toolkit moses .
relation extraction is the task of finding semantic relations between two entities from text .
dependency parsing is a fundamental task for language processing which has been investigated for decades .
the experiments of the phrase-based smt systems are carried out using the open source moses toolkit .
for the word-embedding based classifier , we use the glove pre-trained word embeddings .
word alignment is a natural language processing task that aims to specify the correspondence between words in two languages ( cite-p-19-1-0 ) .
we use the 300-dimensional pre-trained word2vec 3 word embeddings and compare the performance with that of glove 4 embeddings .
sentiment analysis is a much-researched area that deals with identification of positive , negative and neutral opinions in text .
we used a 5-gram language model with modified kneser-ney smoothing , built with the srilm toolkit .
we use pre-trained 50-dimensional word embeddings vector from glove .
sentiment analysis is the task of identifying positive and negative opinions , sentiments , emotions and attitudes expressed in text .
for instance , zeng et al utilized a cnn-based model to extract sentence-level features for relation classification .
for language model , we used sri language modeling toolkit to train a 4-gram model with modified kneser-ney smoothing .
to implement svm algorithm , we have used the publicly available python based scikit-learn package .
we used 300 dimensional skip-gram word embeddings pre-trained on pubmed .
pseudo-word is a kind of basic multi-word expression that characterizes minimal sequence of consecutive words in sense of translation .
bannard and callison-burch used the bilingual pivoting method on parallel corpora for the same task .
we then evaluate the effect of word alignment on machine translation quality using the phrase-based translation system moses .
we also describe a perceptron-style algorithm for training the neural networks , as an alternative to maximum-likelihood method , to speed up the training process and make the learning algorithm easier to be implemented .
motivated by this limitation , the study aims to investigate the use of content features in speech scoring systems .
we obtained distributed word representations using word2vec 4 with skip-gram .
word sense disambiguation ( wsd ) is a key task in computational lexical semantics , inasmuch as it addresses the lexical ambiguity of text by making explicit the meaning of words occurring in a given context ( cite-p-18-3-10 ) .
for training the translation model and for decoding we used the moses toolkit .
framing is further related to works which analyze biased language and subjectivity .
morphological analysis is the basis for many nlp applications , including syntax parsing , machine translation and automatic indexing .
the parsing algorithm is extended to handle translation candidates and to incorporate language model scores via cube pruning .
we use a bidirectional long short-term memory rnn to encode a sentence .
we use the moses software package 5 to train a pbmt model .
bleu is widely used for automatic evaluation of machine translation systems .
table 1 shows the evaluation of all the systems in terms of bleu score with the best score highlighted .
we first trained a trigram bnlm as the baseline with interpolated kneser-ney smoothing , using srilm toolkit .
we optimise the feature weights of the model with minimum error rate training against the bleu evaluation metric .
semantic role labeling ( srl ) has been defined as a sentence-level natural-language processing task in which semantic roles are assigned to the syntactic arguments of a predicate ( cite-p-14-1-7 ) .
we utilize a maximum entropy model to design the basic classifier used in active learning for wsd .
in addition , mixed feature sets also show potential for scaling well when dealing with larger number of verbs and verb classes .
coreference resolution is the task of determining which mentions in a text refer to the same entity .
regarding word embeddings , we use the ones trained by baziotis et al using word2vec and 550 million tweets .
hochreiter and schmidhuber developed long short-term memory to overcome the long term dependency problem .
we use conditional random fields sequence labeling as described in .
this paper presents our approach for the subtask of message polarity classification of semeval 2013 .
we used moses to train an alignment model on the created paraphrase dataset .
goldwasser et al took an unsupervised approach for semantic parsing based on self-training driven by confidence estimation .
pcfg parsing features were generated on the output of the berkeley parser , trained over an english and a spanish treebank .
we use srilm to train a 5-gram language model on the xinhua portion of the english gigaword corpus 5th edition with modified kneser-ney discounting .
we used a 5-gram language model with modified kneser-ney smoothing , built with the srilm toolkit .
we define the left descriptor of word type math-w-3-3-3-87 as : math-p-3-4-0
transliteration is the task of converting a word from one alphabetic script to another .
the classic work on this task was by bagga and baldwin , who adapted the vector space model .
as a point of comparison , we will also present results from the word2vec model of mikolov et al trained on the same underlying corpus as our models .
we used the moses toolkit for performing statistical machine translation .
we evaluated the translation quality of the system using the bleu metric .
we trained a 3-gram language model on the spanish side using srilm .
the core machinery of our system is driven by a latent dirichlet allocation topic model .
costa-juss脿 and fonollosa , 2006 ) view the source reordering as a translation task that translate the source language into a reordered source language .
we use the word2vec vectors with 300 dimensions , pre-trained on 100 billion words of google news .
for msa , we use the penn arabic treebank .
recently , mikolov et al presented a shallow network architecture that is specifically for learning word embeddings , known as the word2vec model .
we used the implementation provided by without tuning any hyper-parameters .
syntactic parsing is a computationally intensive and slow task .
we used the svm implementation provided within scikit-learn .
for this supervised structure learning task , we choose the approach conditional random fields .
we use the k-best batch mira to tune mt systems .
for english , we used the pre-trained word2vec by on google news .
following li et al , we define our model in the well-known log-linear framework .
we generate dependency structures from the ptb constituency trees using the head rules of yamada and matsumoto .
the statistical significance test is performed by the re-sampling approach .
sentiment analysis is a collection of methods and algorithms used to infer and measure affection expressed by a writer .
we use the stanford pos tagger to obtain the lemmatized corpora for the parss task .
we perform minimum error rate training to tune various feature weights .
we use sri language model toolkit to train a 5-gram model with modified kneser-ney smoothing on the target-side training corpus .
ganchev et al , 2010 ) describes a method based on posterior regularization that incorporates additional constraints within the em algorithm for estimation of ibm models .
mihalcea et al combine pointwise mutual information , latent semantic analysis and wordnet-based measures of word semantic similarity into an arbitrary text-to-text similarity metric .
to solve this problem , hochreiter and schmidhuber introduced the long short-term memory rnn .
additionally , coreference resolution is a pervasive problem in nlp and many nlp applications could benefit from an effective coreference resolver that can be easily configured and customized .
feature weights were set with minimum error rate training on a development set using bleu as the objective function .
we adopted the case-insensitive bleu-4 as the evaluation metric .
word sense disambiguation ( wsd ) is a task to identify the intended sense of a word based on its context .
bilingual dictionaries are an essential resource in many multilingual natural language processing tasks such as machine translation and cross-language information retrieval .
word embeddings are low-dimensional vector representations of words such as word2vec that recently gained much attention in various semantic tasks .
we use the glove pre-trained word embeddings for the vectors of the content words .
the translation quality is evaluated by case-insensitive bleu and ter metrics using multeval .
a 3-gram language model is trained on the target side of the training data by the srilm toolkits with modified kneser-ney smoothing .
we also use a 4-gram language model trained using srilm with kneser-ney smoothing .
coreference resolution is a fundamental component of natural language processing ( nlp ) and has been widely applied in other nlp tasks ( cite-p-15-3-9 ) .
rhetorical structure theory posits a hierarchical structure of discourse relations between spans of text .
sentiment analysis ( sa ) is the task of determining the sentiment of a given piece of text .
we used the scikit-learn implementation of svrs and the skll toolkit .
we used the sri language modeling toolkit with kneser-kney smoothing .
our model is a first order linear chain conditional random field .
we use the moses toolkit to create a statistical phrase-based machine translation model built on the best pre-processed data , as described above .
information extraction ( ie ) is the task of generating structured information , often in the form of subject-predicate-object relation triples , from unstructured information such as natural language text .
the sentiment analysis is a field of study that investigates feelings present in texts .
we used the srilm toolkit to build unpruned 5-gram models using interpolated modified kneser-ney smoothing .
mikolov et al presents a neural network-based architecture which learns a word representation by learning to predict its context words .
and while discourse parsing is a document level task , discourse segmentation is done at the sentence level , assuming that sentence boundaries are known .
we apply sri language modeling toolkit to train a 4-gram language model with kneser-ney smoothing .
we use srilm for training a trigram language model on the english side of the training corpus .
as a classifier , we employ support vector machines as implemented in svm light .
the nnlm weights are optimized as the other feature weights using minimum error rate training .
we used the srilm toolkit to train a 4-gram language model on the english side of the training corpus .
we perform smt experiments in all language pairs of the wmt13 and obtain smt performance close to the baseline moses system using less resources for training .
the n-gram language models are trained using the srilm toolkit or similar software developed at hut .
we use the wsj portion of the penn treebank 4 , augmented with head-dependant information using the rules of yamada and matsumoto .
as embedding vectors , we used the publicly available representations obtained from the word2vec cbow model .
for improving the word alignment , we use the word-classes that are trained from a monolingual corpus using the srilm toolkit .
word alignment is the task of identifying translational relations between words in parallel corpora , in which a word at one language is usually translated into several words at the other language ( fertility model ) ( cite-p-18-1-0 ) .
the target language model is trained by the sri language modeling toolkit on the news monolingual corpus .
we used minimum error rate training to tune the feature weights for maximum bleu on the development set .
we implement an in-domain language model using the sri language modeling toolkit .
for this supervised structure learning task , we choose the approach conditional random fields .
we used the svd implementation provided in the scikit-learn toolkit .
we utilize the nematus implementation to build encoder-decoder nmt systems with attention and gated recurrent units .
our translation system is an in-house phrasebased system analogous to moses .
twitter is a famous social media platform capable of spreading breaking news , thus most of rumour related research uses twitter feed as a basis for research .
we briefly review the path ranking algorithm , described in more detail by lao and cohen .
the resulting model is an instance of a conditional random field .
we use glove word embeddings , which are 50-dimension word vectors trained with a crawled large corpus with 840 billion tokens .
we used the 300-dimensional fasttext embedding model pretrained on wikipedia with skip-gram to initialize the word embeddings in the embedding layer .
to prevent overfitting , we apply dropout operators to non-recurrent connections between lstm layers .
among them , lexicalized reordering models have been widely used in practical phrase-based systems .
therefore , for both chinese and english srl systems , we use the 3-best parse trees of berkeley parser and 1-best parse trees of bikel parser and stanford parser as inputs .
marcu and echihabi proposed a method for cheap acquisition of training data for discourse relation sense prediction .
zeng et al proposed an approach for relation classification where sentence-level features are learned through a cnn , which has word embedding and position features as its input .
for the semantic language model , we used the srilm package and trained a tri-gram language model with the default goodturing smoothing .
the metrics that were used to evaluate the model were bleu , ne dist and nist .
we implemented linear models with the scikit learn package .
reisinger and mooney and huang et al also presented methods that learn multiple embeddings per word by clustering the contexts .
we used the srilm toolkit to train a 4-gram language model on the english side of the training corpus .
recently , neural networks , and in particular recurrent neural networks have shown excellent performance in language modeling .
we use the sri language model toolkit to train a 5-gram model with modified kneser-ney smoothing on the target-side training corpus .
numerous studies suggest that translated texts are different from original ones .
galley and manning propose a shift-reduce algorithm to integrate a hierarchical reordering model into phrase-based systems .
word sense disambiguation ( wsd ) is a widely studied task in natural language processing : given a word and its context , assign the correct sense of the word based on a predefined sense inventory ( cite-p-15-3-4 ) .
the anaphor is a definite noun phrase and the referent is in focus , that is .
for the language model , we used sri language modeling toolkit to train a trigram model with modified kneser-ney smoothing on the 31 , 149 english sentences .
visual question answering ( vqa ) is a well-known and challenging task that requires systems to jointly reason about natural language and vision .
we use 300-dimensional word embeddings from glove to initialize the model .
discourse parsing is a challenging task and plays a critical role in discourse analysis .
we use sri language model toolkit to train a 5-gram model with modified kneser-ney smoothing on the target-side training corpus .
information extraction ( ie ) is the nlp field of research that is concerned with obtaining structured information from unstructured text .
we further used a 5-gram language model trained using the srilm toolkit with modified kneser-ney smoothing .
li et al and dhingra et al also proposed end-to-end task-oriented dialog models that can be trained with hybrid supervised learning and rl .
uedin has used the srilm toolkit to train the language model and relies on kenlm for language model scoring during decoding .
table 2 gives the results measured by caseinsensitive bleu-4 .
we used moses with the default configuration for phrase-based translation .
we automatically parsed the french side of the corpus with the berkeley parser , while we used the fast vanilla pcfg model of the stanford parser for the english side .
previous work showed that word clusters derived from an unlabelled dataset can improve the performance of many nlp applications .
named entity recognition ( ner ) is a frequently needed technology in nlp applications .
we use latent dirichlet allocation to obtain the topic words for each lexical pos .
a bunsetsu is a japanese grammatical and phonological unit that consists of one or more content words such as a noun , verb , or adverb followed by a sequence of zero or more function words such as auxiliary verbs , postpositional particles , or sentence-final particles .
lda is a representative probabilistic topic model of document collections .
the models are built using the sri language modeling toolkit .
sentiment analysis is a growing research field , especially on web social networks .
the decoding weights were optimized with minimum error rate training .
moreover , arabic is a morphologically complex language .
relation extraction ( re ) is the task of extracting instances of semantic relations between entities in unstructured data such as natural language text .
in our experiments , we use 300-dimension word vectors pre-trained by glove .
collobert et al used word embeddings as the input of various nlp tasks , including part-of-speech tagging , chunking , ner , and semantic role labeling .
huang et al proposed a learning model based on chinese phonemic alphabet to detect chinese spelling errors .
we implemented linear models with the scikit learn package .
dropout is performed at the input of each lstm layer , including the first layer .
we regularize our network using dropout with the dropout rate tuned using the development set .
we use the glove vectors of 300 dimension to represent the input words .
luong et al , 2013 ) utilized recursive neural networks in which inputs are morphemes of words .
we use the pre-trained glove 50-dimensional word embeddings to represent words found in the glove dataset .
relation extraction is a subtask of information extraction that finds various predefined semantic relations , such as location , affiliation , rival , etc. , between pairs of entities in text .
to measure the translation quality , we use the bleu score and the nist score .
we use 300-dimensional word embeddings from glove to initialize the model .
we used a 5-gram language model with modified kneser-ney smoothing , built with the srilm toolkit .
in this paper we applied several probabilistic topic models to discourse within political blogs .
system tuning was carried out using minimum error rate training optimised with k-best mira on a held out development set .
we implement classification models using keras and scikit-learn .
we trained a linear log-loss model using stochastic gradient descent learning as implemented in the scikit learn library .
we use glove vectors with 100 dimensions trained on wikipedia and gigaword as word embeddings , which we do not optimize during training .
for both languages , we used the srilm toolkit to train a 5-gram language model using all monolingual data provided .
we also run our systems on the ontonotes dataset , which was used for evaluation in conll 2011 shared task .
the model was built using the srilm toolkit with backoff and kneser-ney smoothing .
for our implementation we use 300-dimensional part-of-speech-specific word embeddings v i generated using the gensim word2vec package .
we use word embeddings of dimension 100 pretrained using word2vec on the training dataset .
coreference resolution is the task of partitioning a set of mentions ( i.e . person , organization and location ) into entities .
we trained kneser-ney discounted 5-gram language models on each available corpus using the srilm toolkit .
the sentiment analysis is a field of study that investigates feelings present in texts .
we trained a 5-gram language model on the xinhua portion of gigaword corpus using the srilm toolkit .
twitter is a widely used microblogging platform , where users post and interact with messages , “ tweets ” .
plda is an extension of lda which is an unsupervised machine learning method that models topics of a document collection .
we use stanford log-linear partof-speech tagger to produce pos tags for the english side .
sentiment classification is a special task of text categorization that aims to classify documents according to their opinion of , or sentiment toward a given subject ( e.g. , if an opinion is supported or not ) ( cite-p-11-1-2 ) .
in a second baseline model , we also incorporate 300-dimensional glove word embeddings trained on wikipedia and the gigaword corpus .
part-of-speech ( pos ) tagging is a crucial task for natural language processing ( nlp ) tasks , providing basic information about syntax .
twitter is a popular microblogging service , which , among other things , is used for knowledge sharing among friends and peers .
table 4 shows end-to-end translation bleu score results .
we used the target side of the parallel corpus and the srilm toolkit to train a 5-gram language model .
the combination of multi-task learning and neural networks has shown its advantages in many tasks , ranging from computer vision to natural language processing .
for the task of event trigger prediction , we train a multi-class logistic regression classifier using liblinear .
the 5-gram target language model was trained using kenlm .
coreference resolution is the task of partitioning the set of mentions of discourse referents in a text into classes ( or ‘ chains ’ ) corresponding to those referents ( cite-p-12-3-14 ) .
sentiment analysis is a research area in the field of natural language processing .
to evaluate our approach , we classically adopted the rouge 2 framework , which estimates a summary score by its n-gram overlap with several reference summaries .
our method involved using the machine translation software moses .
coreference resolution is the process of linking together multiple referring expressions of a given entity in the world .
translation scores are reported using caseinsensitive bleu with a single reference translation .
we use a count-based distributional semantics model and the continuous bag-of-words model to learn word vectors .
the language models were interpolated kneser-ney discounted trigram models , all constructed using the srilm toolkit .
we use the chunker yamcha , which is based on svms .
for the textual sources , we populate word embeddings from the google word2vec embeddings trained on roughly 100 billion words from google news .
we used nwjc2vec 10 , which is a 200 dimensional word2vec model .
part-of-speech tagging is the process of assigning to a word the category that is most probable given the sentential context ( cite-p-4-1-2 ) .
relation extraction is the task of detecting and characterizing semantic relations between entities from free text .
the targetside 4-gram language model was estimated using the srilm toolkit and modified kneser-ney discounting with interpolation .
semantic parsing is the task of mapping natural language to a formal meaning representation .
in this work , we present a framework for information recommendation in such social media as internet forums and blogs .
sentence compression is the task of generating a grammatical and shorter summary for a long sentence while preserving its most important information .
we used the moses decoder , with default settings , to obtain the translations .
we used nltk wordnet synsets for obtaining the ambiguity of the word .
as a classifier , we employ support vector machines as implemented in svm light .
we trained a 4-gram language model on the xinhua portion of gigaword corpus using the sri language modeling toolkit with modified kneser-ney smoothing .
we initialize the word embeddings for our deep learning architecture with the 100-dimensional glove vectors .
zhu et al propose to use a tree-based translation model which covers splitting , dropping , reordering and substitution .
named entity recognition ( ner ) is a key technique for ie and other natural language processing tasks .
word representations , especially brown clustering , have been shown to improve the performance of ner system when added as a feature .
for representing words , we used 100 dimensional pre-trained glove embeddings .
the translation quality is evaluated by case-insensitive bleu-4 .
a 5-gram language model with kneser-ney smoothing is trained using s-rilm on the target language .
our system learns word and sentence embeddings jointly by training a multilingual skip-gram model together with a cross-lingual sentence similarity model .
srilm toolkit was used to create up to 5-gram language models using the mentioned resources .
pichotta and mooney applied a lstm recurrent neural network , coupled with beam search , to model event sequences and their representations .
we trained a specific language model using srilm from each of these corpora in order to estimate n-gram log-probabilities .
we achieve this by following goldberg and nivre in using a dynamic oracle to create partially labelled training data .
our model is a first order linear chain conditional random field .
the state of the art suggests that the use of heterogeneous measures can improve the evaluation reliability .
the penn discourse treebank is the largest available discourseannotated resource in english .
relation extraction is the task of predicting semantic relations over entities expressed in structured or semi-structured text .
the nnlm weights are optimized as the other feature weights using minimum error rate training .
the srilm toolkit is used to train 5-gram language model .
sarcasm is a sophisticated speech act which commonly manifests on social communities such as twitter and reddit .
according to lakoff and johnson , metaphors are cognitive mappings of concepts from a source to a target domain .
twitter is a huge microbloging service with more than 500 million tweets per day 1 from different locations in the world and in different languages .
transliteration is the task of converting a word from one writing script to another , usually based on the phonetics of the original word .
metaphor is a natural consequence of our ability to reason by analogy ( cite-p-16-1-12 ) .
furthermore , we train a 5-gram language model using the sri language toolkit .
morphological analysis is a staple of natural language processing for broad languages .
we initialize the embedding weights by the pre-trained word embeddings with 200 dimensional vectors .
we used the 200-dimensional word vectors for twitter produced by glove .
the bleu , rouge and ter scores by comparing the abstracts before and after human editing are presented in table 5 .
the challenge is to enforce the one-to-one topic correspondence .
sometimes a noun can refer to the entity denoted by a noun that has a different modifier .
in this paper , we propose a variant of annotation scheme for uncertainty identification and construct the first uncertainty corpus based on tweets .
we used the sri language modeling toolkit to train a fivegram model with modified kneser-ney smoothing .
our experimental results show that this approach can accurately predict missing topic preferences of users accurately ( 80¨c94 % ) .
we use the wn similarity jcn score since this gave reasonable results for and it is efficient at run time given precompilation of frequency information .
text categorization is a classical text information processing task which has been studied adequately ( cite-p-18-1-9 ) .
coreference resolution is the task of automatically grouping references to the same real-world entity in a document into a set .
wiebe et al analyze linguistic annotator agreement statistics to find bias , and use a similar model to correct labels .
text classification is a crucial and well-proven method for organizing the collection of large scale documents .
conditional random fields are a type of discriminative probabilistic model proposed for labeling sequential data .
we evaluate our proposed technique on a benchmark dataset of semeval-2017 shared task on financial sentiment analysis .
semantic parsing is the task of converting natural language utterances into their complete formal meaning representations which are executable for some application .
as a strong baseline , we trained the skip-gram model of mikolov et al using the publicly available word2vec 5 software .
to do this we examine the dataset created for the english lexical substitution task in semeval .
following , we minimize the objective by the diagonal variant of adagrad with minibatchs .
our system is based on the phrase-based part of the statistical machine translation system moses .
we applied a supervised machine-learning approach , based on conditional random fields .
we use the mallet implementation of conditional random fields .
rambow et al proposed a sentence extraction summarization approach for email threads .
culotta and sorensen extended this work to estimate similarity between augmented dependency trees .
twitter is a popular microblogging service which provides real-time information on events happening across the world .
domain adaptation is a challenge for ner and other nlp applications .
we implemented the different aes models using scikit-learn .
part-of-speech tagging is the act of assigning each word in a sentence a tag that describes how that word is used in the sentence .
we use the collapsed tree formalism of the stanford dependency parser .
combinatory categorial grammar ccg is a categorial formalism that provides a transparent interface between syntax and semantics , steedman , 1996 , steedman , 2000 .
we use different pretrained word embeddings such as glove 1 and fasttext 2 as the initial word embeddings .
named entity recognition ( ner ) is the task of detecting named entity mentions in text and assigning them to their corresponding type .
word sense disambiguation ( wsd ) is a widely studied task in natural language processing : given a word and its context , assign the correct sense of the word based on a predefined sense inventory ( cite-p-15-3-4 ) .
to exploit these kind of labeling constraints , we resort to conditional random fields .
word segmentation is a fundamental task for chinese language processing .
we used the sri language modeling toolkit with kneser-kney smoothing .
we use the pre-trained glove vectors to initialize word embeddings .
we used the srilm toolkit to train a 4-gram language model on the xinhua portion of the gigaword corpus , which contains 238m english words .
named entity recognition ( ner ) is the task of identifying named entities in free text—typically personal names , organizations , gene-protein entities , and so on .
our cdsm feature is based on word vectors derived using a skip-gram model .
to generate dependency links , we use the stanford pos tagger 18 and the malt parser .
the scaling factors are tuned with mert with bleu as optimization criterion on the development sets .
relation extraction ( re ) has been defined as the task of identifying a given set of semantic binary relations in text .
the treebank consists of approximately 30,000 sentences annotated with syntactic roles in addition to morphosyntactic features .
in our implementation , we use the binary svm light developed by joachims .
the probability of a word is governed by its latent topic , which is modeled as a categorical distribution in lda .
we implement the pbsmt system with the moses toolkit .
to optimize model parameters , we use the adagrad algorithm of duchi et al with l2 regularization .
we use word embedding pre-trained on newswire with 300 dimensions from word2vec .
lda is a representative probabilistic topic model of document collections .
we use the adagrad algorithm to optimize the conditional , marginal log-likelihood of the data .
we adopt berkeley parser 1 to train our sub-models .
conditional random fields are discriminative structured classification models for sequential tagging and segmentation .
we obtained distributed word representations using word2vec 4 with skip-gram .
for the actioneffect embedding model , we use pre-trained glove word embeddings as input to the lstm .
the special difficulty of this task is the length disparity between the two semantic comparison texts .
this is effectively what bilmes and kirchhoff did in generalizing n-gram language models to factored language models .
coreference resolution is the task of clustering a sequence of textual entity mentions into a set of maximal non-overlapping clusters , such that mentions in a cluster refer to the same discourse entity .
to compensate this shortcoming , we performed smoothing of the phrase table using the good-turing smoothing technique .
for this reason , previous work often included qualitative analyses and carefully defined heuristics to address these problems .
we use a conditional random field sequence model , which allows for globally optimal training and decoding .
we use binary cross-entropy as the objective function and the adam optimization algorithm with the parameters suggested by kingma and ba for training the network .
coreference resolution is the process of determining whether two expressions in natural language refer to the same entity in the world .
we train the model using the adam optimizer with the default hyper parameters .
sentiment classification is the task of classifying an opinion document as expressing a positive or negative sentiment .
we used the srilm toolkit to create 5-gram language models with interpolated modified kneser-ney discounting .
stemming is a popular way to reduce the size of a vocabulary in natural language tasks by conflating words with related meanings .
it has been empirically shown that word embeddings can capture semantic and syntactic similarities between words .
socher et al introduce a matrix-vector recursive neural network model that learns compositional vector representations for phrases and sentences .
semantic parsing is the task of mapping natural language to machine interpretable meaning representations .
metonymy is a figure of speech , in which one expression is used to refer to the standard referent of a related one ( cite-p-18-1-13 ) .
link grammar is a context-free lexicalized grammar without explicit constituents .
a particular generative model , which is well suited for the modeling of text , is called latent dirichlet allocation .
we tuned parameters of the smt system using minimum error-rate training .
we used the google news pretrained word2vec word embeddings for our model .
some of the very effective ml approaches used in ner are hmm , me , crfs and svm .
we use the pre-trained 300-dimensional word2vec embeddings trained on google news 1 as input features .
in our implementation , we train a tri-gram language model on each phone set using the srilm toolkit .
we used the opennmt-tf framework 4 to train a bidirectional encoder-decoder model with attention .
we used a 5-gram language model with modified kneser-ney smoothing implemented using the srilm toolkit .
we trained a 4-gram language model on the xinhua portion of gigaword corpus using the sri language modeling toolkit with modified kneser-ney smoothing .
semeval is a yearly event in which teams compete in natural language processing tasks .
relation extraction ( re ) is a task of identifying typed relations between known entity mentions in a sentence .
we initialize the embedding layer using embeddings from dedicated word embedding techniques word2vec and glove .
we use a fourgram language model with modified kneser-ney smoothing as implemented in the srilm toolkit .
relation extraction ( re ) is the task of extracting instances of semantic relations between entities in unstructured data such as natural language text .
in addition , we use an english corpus of roughly 227 million words to build a target-side 5-gram language model with srilm in combination with kenlm .
for the pos-tagger , we trained hunpos 10 with the wall street journal english corpus .
we investigate linguistic features that correlate with the readability of texts for adults with intellectual disabilities ( id ) .
we then lowercase all data and use all unique headlines in the training data to train a language model with the srilm toolkit .
relation extraction ( re ) is the task of identifying instances of relations , such as nationality ( person , country ) or place of birth ( person , location ) , in passages of natural text .
information extraction ( ie ) is a technology that can be applied to identifying both sources and targets of new hyperlinks .
a 5-gram language model with kneser-ney smoothing was trained with srilm on monolingual english data .
minimum error rate training is used for tuning to optimize bleu .
we use liblinear 9 to solve the lr and svm classification problems .
the embedding layer in the model is initialized with 300-dimensional glove word vectors obtained from common crawl .
we use the sri language model toolkit to train a 5-gram model with modified kneser-ney smoothing on the target-side training corpus .
each sentence in the dataset is parsed using stanford dependency parser .
coreference resolution is the task of partitioning the set of mentions of discourse referents in a text into classes ( or ‘ chains ’ ) corresponding to those referents ( cite-p-12-3-14 ) .
we adapted the moses phrase-based decoder to translate word lattices .
we use word embedding pre-trained on newswire with 300 dimensions from word2vec .
fader et al recently presented a scalable approach to learning an open domain qa system , where ontological mismatches are resolved with learned paraphrases .
the english side of the parallel corpus is trained into a language model using srilm .
classifier we use the l2-regularized logistic regression from the liblinear package , which we accessed through weka .
in addition , a 5-gram lm with kneser-ney smoothing and interpolation was built using the srilm toolkit .
the parameters of the log-linear model are tuned by optimizing bleu on the development data using mert .
in this and our other n-gram models , we used kneser-ney smoothing .
a knsmoothed 5-gram language model is trained on the target side of the parallel data with srilm .
our smt system is a phrase-based system based on the moses smt toolkit .
ding and palmer introduced a version of probabilistic extension of synchronous dependency insertion grammars to deal with the pervasive structure divergence .
continuous representation of words and phrases are proven effective in many nlp tasks .
sentence compression is a paraphrasing task where the goal is to generate sentences shorter than given while preserving the essential content .
we trained a 4-gram language model with kneser-ney smoothing and unigram caching using the sri-lm toolkit .
word sense disambiguation ( wsd ) is a widely studied task in natural language processing : given a word and its context , assign the correct sense of the word based on a predefined sense inventory ( cite-p-15-3-4 ) .
however , polysemy is a fundamental problem for distributional models .
relation extraction is a traditional information extraction task which aims at detecting and classifying semantic relations between entities in text ( cite-p-10-1-18 ) .
we trained a phrase-based smt engine to translate known words and phrases using the training tools available with moses .
relation extraction ( re ) is the task of extracting instances of semantic relations between entities in unstructured data such as natural language text .
we perform pre-training using the skipgram nn architecture available in the word2vec tool .
we use the moses software package 5 to train a pbmt model .
coreference resolution is the process of finding discourse entities ( markables ) referring to the same real-world entity or concept .
for word-level embeddings , we pre-train the word vectors using word2vec on the gigaword corpus mentioned in section 4 , and the text of the training dataset .
the sri language modeling toolkit was used to build 4-gram word-and character-based language models .
stance detection is the task of classifying the attitude previous work has assumed that either the target is mentioned in the text or that training data for every target is given .
coreference resolution is a task aimed at identifying phrases ( mentions ) referring to the same entity .
table 2 presents the results from the automatic evaluation , in terms of bleu and nist scores , of 4 system setups .
recently , neural networks , and in particular recurrent neural networks have shown excellent performance in language modeling .
relation extraction is the task of finding relationships between two entities from text .
however , we use a large 4-gram lm with modified kneser-ney smoothing , trained with the srilm toolkit , stolcke , 2002 and ldc english gigaword corpora .
relation extraction is a fundamental task in information extraction .
sentiment analysis is a natural language processing ( nlp ) task ( cite-p-10-3-0 ) which aims at classifying documents according to the opinion expressed about a given subject ( federici and dragoni , 2016a , b ) .
word embeddings have proven to be effective models of semantic representation of words in various nlp tasks .
distributional semantic models represent the meanings of words by relying on their statistical distribution in text .
to measure the importance of the generated questions , we use lda to identify the important subtopics 9 from the given body of texts .
shallow semantic representations can prevent the weakness of cosine similarity based models .
text segmentation is the task of determining the positions at which topics change in a stream of text .
luong et al created a hierarchical language model that uses rnn to combine morphemes of a word to obtain a word representation .
we used latent dirichlet allocation to perform the classification .
we train a 4-gram language model on the xinhua portion of the english gigaword corpus using the srilm toolkits with modified kneser-ney smoothing .
word alignment is the task of identifying translational relations between words in parallel corpora , in which a word at one language is usually translated into several words at the other language ( fertility model ) ( cite-p-18-1-0 ) .
we present a novel approach to fsd that operates in math-w-2-1-0-91 per tweet .
twitter is a widely used microblogging environment which serves as a medium to share opinions on various events and products .
we use srilm for training a trigram language model on the english side of the training data .
relation extraction is the task of recognizing and extracting relations between entities or concepts in texts .
a back-off 2-gram model with good-turing discounting and no lexical classes was also created from the training set , using the srilm toolkit , .
for the fst representation , we used the the opengrm-ngram language modeling toolkit and used an n-gram order of 4 , with kneser-ney smoothing .
twitter consists of a massive number of posts on a wide range of subjects , making it very interesting to extract information and sentiments from them .
semantic role labeling ( srl ) has been defined as a sentence-level natural-language processing task in which semantic roles are assigned to the syntactic arguments of a predicate ( cite-p-14-1-7 ) .
minimum error rate training is applied to tune the cn weights .
semantic parsing is the task of mapping natural language to a formal meaning representation .
word alignment is a key component of most endto-end statistical machine translation systems .
we learn our word embeddings by using word2vec 3 on unlabeled review data .
for estimating monolingual word vector models , we use the cbow algorithm as implemented in the word2vec package using a 5-token window .
we use the word2vec skip-gram model to learn initial word representations on wikipedia .
we initialize the embedding layer using embeddings from dedicated word embedding techniques word2vec and glove .
we use sri language modeling toolkit to train a 5-gram language model on the english sentences of fbis corpus .
relation extraction is a fundamental task in information extraction .
for pos tagging , we used the stanford pos tagger .
gru and lstm have been shown to yield comparable performance .
we used the stanford parser to extract dependency features for each quote and response .
we use moses , a statistical machine translation system that allows training of translation models .
we use distributed word vectors trained on the wikipedia corpus using the word2vec algorithm .
for input representation , we used glove word embeddings .
coreference resolution is the task of determining when two textual mentions name the same individual .
relation extraction ( re ) has been defined as the task of identifying a given set of semantic binary relations in text .
we used minimum error rate training to optimize the feature weights .
word sense disambiguation ( wsd ) is the task of determining the meaning of a word in a given context .
importantly , word embeddings have been effectively used for several nlp tasks , such as named entity recognition , machine translation and part-of-speech tagging .
for both languages , we used the srilm toolkit to train a 5-gram language model using all monolingual data provided .
pitler et al demonstrated that features developed to capture word polarity , verb classes and orientation , as well as some lexical features are strong indicator of the type of discourse relation .
relation extraction is the task of recognizing and extracting relations between entities or concepts in texts .
after standard preprocessing of the data , we train a 3-gram language model using kenlm .
we used moses with the default configuration for phrase-based translation .
for all experiments , we used a 4-gram language model with modified kneser-ney smoothing which was trained with the srilm toolkit .
we used the moses mt toolkit with default settings and features for both phrase-based and hierarchical systems .
the well-known phrase-based statistical translation model extends the basic translation units from single words to continuous phrases to capture local phenomena .
we use the linearsvc classifier as implemented in scikit-learn package 17 with the default parameters .
for english , we use the stanford parser for both pos tagging and cfg parsing .
the language models are trained on the corresponding target parts of this corpus using the sri language model tool .
we use the 300-dimensional skip-gram word embeddings built on the google-news corpus .
we use the sri language model toolkit to train a 5-gram model with modified kneser-ney smoothing on the target-side training corpus .
firstly , we built a forward 5-gram language model using the srilm toolkit with modified kneser-ney smoothing .
sentiment analysis is the task in natural language processing ( nlp ) that deals with classifying opinions according to the polarity of the sentiment they express .
we used the target side of the parallel corpus and the srilm toolkit to train a 5-gram language model .
morphological disambiguation is a well studied problem in the literature , but lstm-based contributions are still relatively scarce .
the bleu is a classical automatic evaluation method for the translation quality of an mt system .
sequence labeling is a structured prediction task where systems need to assign the correct label to every token in the input sequence .
word alignment is a key component in most statistical machine translation systems .
the syntax tree features were calculated using the stanford parser trained using the english caseless model .
the penn discourse tree bank is the largest resource to date that provides a discourse annotated corpus in english .
twitter is a widely used microblogging platform , where users post and interact with messages , “ tweets ” .
we used standard classifiers available in scikit-learn package .
named entity disambiguation ( ned ) is the task of linking mentions of entities in text to a given knowledge base , such as freebase or wikipedia .
for the semantic language model , we used the srilm package and trained a tri-gram language model with the default goodturing smoothing .
we trained a 4-gram language model on this data with kneser-ney discounting using srilm .
table 2 presents the results from the automatic evaluation , in terms of bleu and nist scores , of 4 system setups .
as shown , these distributions are efficiently estimable from positive data .
we train a 4-gram language model on the xinhua portion of english gigaword corpus by srilm toolkit .
li et al proposed a hybrid method based on wordnet and the brown corpus to incorporate semantic similarity between words , semantic similarity between sentences , and word order similarity to measure overall sentence similarity .
we use the term-sentence matrix to train a simple generative topic model based on lda .
we evaluated the translation quality using the case-insensitive bleu-4 metric .
case-insensitive bleu-4 is our evaluation metric .
the language model pis implemented as an n-gram model using the srilm-toolkit with kneser-ney smoothing .
semantic role labeling ( srl ) is the task of identifying the predicate-argument structure of a sentence .
to test this hypothesis , we extended our model to incorporate bigram dependencies using a hierarchical dirichlet process .
we use word2vec tool which efficiently captures the semantic properties of words in the corpus .
pang and lee propose a graph-based method which finds minimum cuts in a document graph to classify the sentences into subjective or objective .
we train a word2vec cbow model on raw 517 , 400 emails from the en-ron email dataset to obtain the word embeddings .
we used small portions of the penn wsj treebank for the experiments .
we set the feature weights by optimizing the bleu score directly using minimum error rate training on the development set .
we trained word vectors with the two architectures included in the word2vec software .
shen et al propose the well-formed dependency structure to filter the hierarchical rule table .
semeval 2014 is a semantic evaluation of natural language processing ( nlp ) that comprises several tasks .
to train the models we use the default stochastic gradient descent classifier provided by scikit-learn .
user affect parameters can increase the usefulness of these models .
structured syntactic knowledge is important for phrase reordering .
we used the pre-trained word embeddings that were learned using the word2vec toolkit on google news dataset .
wikipedia is a free multilingual online encyclopedia and a rapidly growing resource .
word sense disambiguation ( wsd ) is the task of determining the correct meaning for an ambiguous word from its context .
a bunsetsu consists of one independent word and zero or more ancillary words .
a semantic parser is learned given a set of sentences and their correct logical forms using smt methods .
we preprocessed the training corpora with scripts included in the moses toolkit .
relation extraction is the task of finding semantic relations between entities from text .
we measured translation performance with bleu .
we follow the neural machine translation architecture by bahdanau et al , which we will briefly summarize here .
the parameters of the log-linear model are tuned by optimizing bleu on the development data using mert .
for language modeling , we used the trigram model of stolcke .
jiang et al , 2007 ) put forward a ptc framework based on support vector machine .
sentiment analysis is a natural language processing ( nlp ) task ( cite-p-10-3-0 ) which aims at classifying documents according to the opinion expressed about a given subject ( federici and dragoni , 2016a , b ) .
we report bleu scores computed using sacrebleu .
sentiment analysis is the study of the subjectivity and polarity ( positive vs. negative ) of a text ( cite-p-7-1-10 ) .
our word embeddings is initialized with 100-dimensional glove word embeddings .
hence , we use the cmu twitter pos-tagger to obtain the part-of-speech tags .
a 4-grams language model is trained by the srilm toolkit .
word sense disambiguation ( wsd ) is the task of determining the correct meaning or sense of a word in context .
we use the well-known word embedding model that is a robust framework to incorporate word representation features .
relation extraction is the task of finding relationships between two entities from text .
relation extraction is the task of detecting and characterizing semantic relations between entities from free text .
discourse parsing is a challenging task and is crucial for discourse analysis .
a 3-gram language model is trained on the target side of the training data by the srilm toolkits with modified kneser-ney smoothing .
turney and littman use pointwise mutual information and latent semantic analysis to determine the similarity of the word of unknown polarity with the words in both positive and negative seed sets .
our translation model is implemented as an n-gram model of operations using the srilm toolkit with kneser-ney smoothing .
marcu and wong , 2002 ) presents a joint probability model for phrase-based translation .
for the word-embedding based classifier , we use the glove pre-trained word embeddings .
we used the moses machine translation decoder , using the default features and decoding settings .
coreference resolution is a challenging task , that involves identification and clustering of noun phrases mentions that refer to the same real-world entity .
for the mix one , we also train word embeddings of dimension 50 using glove .
the word embeddings are initialized with 100-dimensions vectors pre-trained by the cbow model .
we use bleu to evaluate translation quality .
for both languages , we used the srilm toolkit to train a 5-gram language model using all monolingual data provided .
part-of-speech ( pos ) tagging is the task of assigning each of the words in a given piece of text a contextually suitable grammatical category .
we use the sri language model toolkit to train a 5-gram model with modified kneser-ney smoothing on the target-side training corpus .
for the semantic language model , we used the srilm package and trained a tri-gram language model with the default goodturing smoothing .
the training of the classifiers has been performed with scikit-learn .
to learn the topics we use latent dirichlet allocation .
we report case-sensitive bleu and ter as the mt evaluation metrics .
these features are the output from the srilm toolkit .
sentiment analysis ( sa ) is a field of knowledge which deals with the analysis of people ’ s opinions , sentiments , evaluations , appraisals , attitudes and emotions towards particular entities ( cite-p-17-1-0 ) .
early work in frame-semantic analysis was pioneered by gildea and jurafsky .
we used trigram language models with interpolated kneser-kney discounting trained using the sri language modeling toolkit .
semantic role labeling ( srl ) is the task of automatic recognition of individual predicates together with their major roles ( e.g . frame elements ) as they are grammatically realized in input sentences .
ccg is a lexicalized , mildly context-sensitive parsing formalism that models a wide range of linguistic phenomena .
we used the wapiti toolkit , based on the linear-chain crfs framework .
arabic is a morphologically complex language .
we train a trigram language model with modified kneser-ney smoothing from the training dataset using the srilm toolkit , and use the same language model for all three systems .
we use the glove pre-trained word embeddings for the vectors of the content words .
we trained a 5-gram language model on the xinhua portion of gigaword corpus using the srilm toolkit .
the translation quality is evaluated by case-insensitive bleu-4 .
the probabilistic language model is constructed on google web 1t 5-gram corpus by using the srilm toolkit .
we chose the skip-gram model provided by word2vec tool developed by for training word embeddings .
inversion transduction grammar is a well studied synchronous grammar formalism .
our smt system is a phrase-based system based on the moses smt toolkit .
the optimisation of the feature weights of the model is done with minimum error rate training against the bleu evaluation metric .
semantic parsing is the task of translating text to a formal meaning representation such as logical forms or structured queries .
coreference resolution is a multi-faceted task : humans resolve references by exploiting contextual and grammatical clues , as well as semantic information and world knowledge , so capturing each of these will be necessary for an automatic system to fully solve the problem .
we use srilm for training a trigram language model on the english side of the training data .
we apply online training , where model parameters are optimized by using adagrad .
for language model , we use a trigram language model trained with the srilm toolkit on the english side of the training corpus .
we employed the machine learning tool of scikit-learn 3 , for training the classifier .
we also use a 4-gram language model trained using srilm with kneser-ney smoothing .
we implement an in-domain language model using the sri language modeling toolkit .
for the semantic language model , we used the srilm package and trained a tri-gram language model with the default goodturing smoothing .
it is a standard phrasebased smt system built using the moses toolkit .
we also use a 4-gram language model trained using srilm with kneser-ney smoothing .
zelenko et al and culotta and sorensen proposed kernels for dependency trees inspired by string kernels .
this paper presents a novel framework called error case frames for correcting preposition errors .
to this end , we propose a new annotation scheme to study how preferences are linguistically expressed in dialogues .
we employ a neural method , specifically the continuous bag-of-words model to learn high-quality vector representations for words .
semantic parsing is the task of converting a sentence into a representation of its meaning , usually in a logical form grounded in the symbols of some fixed ontology or relational database ( cite-p-21-3-3 , cite-p-21-3-4 , cite-p-21-1-11 ) .
we build an open-vocabulary language model with kneser-ney smoothing using the srilm toolkit .
kalchbrenner et al propose a dynamic cnn model using a dynamic k-max pooling mechanism which is able to generate a feature graph which captures a variety of word relations .
we use the stanford named entity recognizer for this purpose .
we trained word vectors with the two architectures included in the word2vec software .
we build a 9-gram lm using srilm toolkit with modified kneser-ney smoothing .
we use liblinear 9 to solve the lr and svm classification problems .
we used the target side of the parallel corpus and the srilm toolkit to train a 5-gram language model .
coreference resolution is a key task in natural language processing ( cite-p-13-1-8 ) aiming to detect the referential expressions ( mentions ) in a text that point to the same entity .
we use a standard phrasebased translation system .
twitter is a huge microblogging service with more than 500 million tweets per day from different locations of the world and in different languages ( cite-p-8-1-9 ) .
we use the glove word vector representations of dimension 300 .
we feed our features to a multinomial naive bayes classifier in scikit-learn .
relation extraction ( re ) is the process of generating structured relation knowledge from unstructured natural language texts .
our cdsm feature is based on word vectors derived using a skip-gram model .
ambiguity is a central issue in natural language processing .
similarly , turian et al collectively used brown clusters , cw and hlbl embeddings , to improve the performance of named entity recognition and chucking tasks .
the improved decision list can raise the f-measure of error detection .
regarding svm we used linear kernels implemented in svm-light .
we employ the glove and node2vec to generate the pre-trained word embedding , obtaining two distinct embedding for each word .
word sense disambiguation ( wsd ) is a task to identify the intended sense of a word based on its context .
we use pre-trained 100 dimensional glove word embeddings .
in order to measure translation quality , we use bleu 7 and ter scores .
our cdsm feature is based on word vectors derived using a skip-gram model .
we used a 5-gram language model with modified kneser-ney smoothing , built with the srilm toolkit .
as with our original refined language model , we estimate each coarse language model using the srilm toolkit .
recurrent neural network architectures have proven to be well suited for many natural language generation tasks .
specifically , we automatically reconstruct phylogenetic language trees from monolingual texts ( translated from several source languages ) .
translation results are evaluated using the word-based bleu score .
for our purpose we use word2vec embeddings trained on a google news dataset and find the pairwise cosine distances for all words .
in this task , we use the 300-dimensional 840b glove word embeddings .
many words have multiple meanings , and the process of identifying the correct meaning , or sense of a word in context , is known as word sense disambiguation ( wsd ) .
word sense disambiguation is the task of assigning a sense to a word based on the context in which it occurs .
since text categorization is a task based on predefined categories , we know the categories for classifying documents .
we learn our word embeddings by using word2vec 3 on unlabeled review data .
such approaches , for example , transition-based and graph-based models have attracted the most attention in dependency parsing in recent works .
discourse parsing is a natural language processing ( nlp ) task with the potential utility for many other natural language processing tasks ( webber et al. , 2011 ) .
in addition , we use an english corpus of roughly 227 million words to build a target-side 5-gram language model with srilm in combination with kenlm .
the present paper is the first to use a reranking parser and the first to address the adaptation scenario for this problem .
these language models were built up to an order of 5 with kneser-ney smoothing using the srilm toolkit .
the parameter weights are optimized with minimum error rate training .
event coreference resolution is the task of determining which event mentions in a text refer to the same real-world event .
model parameters 位 i are estimated using numer-ical optimization methods so as to maximize the log-likelihood of the training data .
relation extraction ( re ) is the task of recognizing the assertion of a particular relationship between two or more entities in text .
we implement an in-domain language model using the sri language modeling toolkit .
dependency parsing is a simpler task than constituent parsing , since dependency trees do not have extra non-terminal nodes and there is no need for a grammar to generate them .
coreference resolution is a well known clustering task in natural language processing .
for our purpose we use word2vec embeddings trained on a google news dataset and find the pairwise cosine distances for all words .
popular topic modeling techniques include latent dirichlet allocation and probabilistic latent semantic analysis .
coreference resolution is the task of clustering referring expressions in a text so that each resulting cluster represents an entity .
for this , we utilize the publicly available glove 1 word embeddings , specifically ones trained on the common crawl dataset .
