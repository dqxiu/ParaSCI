the phraseextraction heuristics of were used to build the phrase-based smt systems .
we measure the translation quality with automatic metrics including bleu and ter .
we relied on the multinomial naive bayes classifier by mccallum and nigam .
we used the pre-trained word embeddings that are learned using the word2vec toolkit on google news dataset .
liu et al developed a dependency-based neural network , in which a convolutional neural network has been used to capture features on the shortest path and a recursive neural network is designed to model subtrees .
the minimum error rate training was used to tune the feature weights .
taxonomies that are backbone of structured ontology knowledge have been found to be useful for many areas such as question answering , document clustering and textual entailment .
relation extraction ( re ) is the task of extracting semantic relationships between entities in text .
we chose to use support vector machines for our classifier .
we used 300 dimensional skip-gram word embeddings pre-trained on pubmed .
in this paper , we investigate the use of form-function mappings derived from human-human dialogues .
we use a fourgram language model with modified kneser-ney smoothing as implemented in the srilm toolkit .
the targetside 4-gram language model was estimated using the srilm toolkit and modified kneser-ney discounting with interpolation .
zeng et al exploit a convolutional neural network to extract lexical and sentence level features for relation classification .
we use the logistic regression classifier in the skll package , which is based on scikit-learn , optimizing for f 1 score .
negation is a grammatical category which comprises various kinds of devices to reverse the truth value of a proposition ( cite-p-18-3-8 ) .
lei et al proposed to learn features by representing the cross-products of some primitive units with low-rank tensors for dependency parsing .
word sense disambiguation ( wsd ) is the task of determining the meaning of a word in a given context .
we use the pre-trained glove 50-dimensional word embeddings to represent words found in the glove dataset .
we trained a 4-gram language model with kneser-ney smoothing and unigram caching using the sri-lm toolkit .
the model uses non-negative matrix factorization in order to find latent dimensions .
in the translation tasks , we used the moses phrase-based smt systems .
for the embeddings trained on stack overflow corpus , we use the word2vec implementation of gensim 8 toolkit .
traditional supervised learning methods heavily rely on large scale annotated data which is time and labor consuming .
in this paper , we propose a new automatic evaluation method for machine translation using noun-phrase chunking .
the pre-processed monolingual sentences will be used by srilm or berkeleylm to train a n-gram language model .
on all datasets and models , we use 300-dimensional word vectors pre-trained on google news .
bilingual lexicons serve as an indispensable source of knowledge for various cross-lingual tasks such as cross-lingual information retrieval or statistical machine translation .
we evaluated the reordering approach within the moses phrase-based smt system .
our baseline is a phrase-based mt system trained using the moses toolkit .
grammar induction is the task of learning grammatical structure from plain text without human supervision .
for our baseline we use the moses software to train a phrase based machine translation model .
we use a pbsmt model built with the moses smt toolkit .
our system is built using the open-source moses toolkit with default settings .
we use srilm toolkit to build a 5-gram language model with modified kneser-ney smoothing .
the decoder and encoder word embeddings are of size 500 , the encoder uses a bidirectional lstm layer with 1k units to encode the source side .
conditional random fields are probabilistic models for labelling sequential data .
table 1 shows the translation performance by bleu .
faruqui et al employ semantic relations of ppdb , wordnet , framenet to retrofit word embeddings for various prediction tasks .
negation is a grammatical category which comprises various kinds of devices to reverse the truth value of a proposition ( cite-p-18-3-8 ) .
in addition , we can use pre-trained neural word embeddings on large scale corpus for neural network initialization .
for emd we used the stanford named entity recognizer .
for instance , bahdanau et al advocate the attention mechanism to dynamically generate a context vector of the whole source sentence for improving the performance of the nmt .
for the sick and msrvid experiments , we used 300-dimension glove word embeddings .
the language models were created using the srilm toolkit on the standard training sections of the ccgbank , with sentenceinitial words uncapitalized .
table 5 shows the bleu and per scores obtained by each system .
thus , we propose a new approach based on the expectation-maximization algorithm .
discourse segmentation is the first step in building a discourse parser .
semantic parsing is the task of converting natural language utterances into their complete formal meaning representations which are executable for some application .
word representations , especially brown clustering , have been shown to improve the performance of ner system when added as a feature .
hatzivassiloglou and mckeown proposed the first method for determining adjective polarities or orientations .
latent dirichlet allocation is a representative of topic models .
the language models used were 7-gram srilm with kneser-ney smoothing and linear interpolation .
we used the google news pretrained word2vec word embeddings for our model .
we experiment with word2vec and glove for estimating similarity of words .
we automatically parse sentences with minipar , a broad-coverage dependency parser .
recently , gong and zhou also applied topic modeling into domain adaptation in smt .
the word embeddings are initialized by pre-trained glove embeddings 2 .
a similar idea called ibm bleu score has proved successful in automatic machine translation evaluation .
in the nlp field , nn-based multi-task learning has been proven to be effective .
we used the sri language modeling toolkit to train a fivegram model with modified kneser-ney smoothing .
we train a trigram language model with modified kneser-ney smoothing from the training dataset using the srilm toolkit , and use the same language model for all three systems .
the 5-gram target language model was trained using kenlm .
with the consideration of user and product information , our model can significantly improve the performance of sentiment classification .
since segmentation is the first stage of discourse parsing , quality discourse segments are critical to building quality discourse representations ( cite-p-12-1-10 ) .
for the language model , we used srilm with modified kneser-ney smoothing .
meanwhile , we adopt glove pre-trained word embeddings 5 to initialize the representation of input tokens .
dependency parsing is the task to assign dependency structures to a given sentence math-w-4-1-0-14 .
empirical studies show that our model can significantly outperform the state-of-the-art response generation models .
language modeling is trained using kenlm using 5-grams , with modified kneser-ney smoothing .
we apply the rules to each sentence with its dependency tree structure acquired from the stanford parser .
for example , wu et al identified aspects based on the features explored by dependency parser .
the language model is a 3-gram language model trained using the srilm toolkit on the english side of the training data .
we use sri language modeling toolkit to train a 5-gram language model on the english sentences of fbis corpus .
mikolov et al proposed a computationally efficient method for learning distributed word representation such that words with similar meanings will map to similar vectors .
we use word embeddings 3 as a cheap low-maintenance alternative for knowledge base construction .
we use srilm for training the 5-gram language model with interpolated modified kneser-ney discounting .
word embeddings have proven to be effective models of semantic representation of words in various nlp tasks .
we used the statistical japanese dependency parser cabocha for parsing .
we primarily compared our model with conditional random fields .
roth and yih use ilp to deal with the joint inference problem of named entity and relation identification .
we adopt adam for optimization , train for 20 epochs and pick the best epoch based on development set loss .
in this study , we focus on improving the corpus-based method for cross-lingual sentiment classification of chinese product reviews .
semantic parsing is then reduced to query graph generation , formulated as a search problem with staged .
when a pun is a spoken utterance , two types of puns are commonly distinguished : homophonic puns , which exploit different meanings of the same word , and heterophonic puns , in which one or more words have similar but not identical pronunciations to some other word or phrase that is alluded to in the pun .
we use minimum error rate training with nbest list size 100 to optimize the feature weights for maximum development bleu .
we use pre-trained glove embeddings to represent the words .
the log-linear model is then tuned as usual with minimum error rate training on a separate development set coming from the same domain .
tai et al introduced tree-lstm , a generalisation of lstms to tree-structured network topologies , eg , recursive neural networks .
we trained a trigram language model on the chinese side , with the srilm toolkit , using the modified kneser-ney smoothing option .
the decoding weights were optimized with minimum error rate training .
we extract our paraphrase grammar from the french-english portion of the europarl corpus .
for this purpose , we turn to the expectation maximization algorithm .
we then follow published procedures to extract hierarchical phrases from the union of the directional word alignments .
collobert et al , 2011 ) used word embeddings for pos tagging , named entity recognition and semantic role labeling .
we solve this sequence tagging problem using the mallet implementation of conditional random fields .
with the two alternative role annotations , we show that the propbank role set is more robust to the lack of verb ¨c specific semantic information .
we use case-insensitive bleu as evaluation metric .
we used the implementation of random forest in scikitlearn as the classifier .
in our experiments , we used the srilm toolkit to build 5-gram language model using the ldc arabic gigaword corpus .
a spelling-based model that directly maps english letter sequences into arabic letters was developed by al-onaizan and knight .
a trigram language model with modified kneser-ney discounting and interpolation was used as produced by the srilm toolkit .
we used a caseless parsing model of the stanford parser for a dependency representation of the messages .
in this paper , we focus on semantic tagging based on a domain-specific ontology , a dictionary-thesaurus and the overlapping coefficient .
we used 200 dimensional glove word representations , which were pre-trained on 6 billion tweets .
semeval is the international workshop on semantic evaluation that has evolved from senseval .
distributional semantic models induce large-scale vector-based lexical semantic representations from statistical patterns of word usage .
the language models are 4-grams with modified kneser-ney smoothing which have been trained with the srilm toolkit .
we use svm-light-tk 5 , which enables the use of structural kernels .
for word representation , we train the skip-gram word embedding on each dataset separately to initialize the word vectors .
we conducted baseline experiments for phrasebased machine translation using the moses toolkit .
active learning is a general framework and does not depend on tasks or domains .
an annotation effort demonstrates implicit relations reveal as much as 30 % of meaning .
our trigram word language model was trained on the target side of the training corpus using the srilm toolkit with modified kneser-ney smoothing .
the weights for the loglinear model are learned using the mert system .
we built a 5-gram language model on the english side of europarl and used the kneser-ney smoothing method and srilm as the language model toolkit .
we use the linear svm classifier from scikit-learn .
for this language model , we built a trigram language model with kneser-ney smoothing using srilm from the same automatically segmented corpus .
we used max-f 1 training to train the feature weights .
relation extraction ( re ) is the task of recognizing relationships between entities mentioned in text .
we obtained these scores by training a word2vec model on the wiki corpus .
djuric et al use a paragraph2vec approach to classify language on user comments as abusive or clean .
text categorization is a crucial and well-proven method for organizing the collection of large scale documents .
we use 5-grams for all language models implemented using the srilm toolkit .
the evaluation metric is the case-insensitive bleu4 .
one of the first challenges in sentiment analysis is the vast lexical diversity of subjective language .
the minimum error rate training was used to tune the feature weights .
on the input sentence , we propose two kinds of probabilistic parsing action models that can compute the entire dependency tree ’ s probability .
we have measured the performance of the segmenters with the windowdiff metric .
for the fluency and grammaticality features , we train 4-gram lms using the development dataset with the sri toolkit .
recently , mikolov et al introduced an efficient way for inferring word embeddings that are effective in capturing syntactic and semantic relationships in natural language .
translation performances are measured with case-insensitive bleu4 score .
we apply the 3-phase learning procedure proposed by where we first create word embeddings based on the skip-gram model .
the smt systems were built using the moses toolkit .
for this step we used regular expressions and nltk to tokenize the text .
for the sick and msrvid experiments , we used 300-dimension glove word embeddings .
we pre-train the word embeddings using word2vec .
we used the scikit-learn library the svm model .
the corpus is automatically tagged and lemmatised by treetagger .
the language models were 5-gram models with kneser-ney smoothing built using kenlm .
stance detection is the task of classifying the attitude previous work has assumed that either the target is mentioned in the text or that training data for every target is given .
part-of-speech tagging is the process of assigning to a word the category that is most probable given the sentential context ( cite-p-4-1-2 ) .
for language modeling we used the kenlm toolkit for standard n-gram modeling with an n-gram length of 5 .
in our approach is to reduce the tasks of content selection ( “ what to say ” ) and surface realization ( “ how to say ” ) into a common parsing problem .
we preinitialize the word embeddings by running the word2vec tool on the english wikipedia dump .
for all models , we use the 300-dimensional glove word embeddings .
we trained a 4-gram language model with kneser-ney smoothing and unigram caching using the sri-lm toolkit .
the negated event is the event or the entity that the negation indicates its absence or denies its occurrence .
for feature building , we use word2vec pre-trained word embeddings .
bleu is the most commonly used metric for mt evaluation .
these attributes were computed using stanford core nlp .
hochreiter and schmidhuber proposed long short-term memories as the specific version of rnn designed to overcome vanishing and exploding gradient problem .
we use a minibatch stochastic gradient descent algorithm together with the adam optimizer .
for language model , we used sri language modeling toolkit to train a 4-gram model with modified kneser-ney smoothing .
word sense disambiguation ( wsd ) is a task to identify the intended sense of a word based on its context .
for estimating the monolingual we , we use the cbow algorithm as implemented in the word2vec package using a 5-token window .
in this baseline , we applied the word embedding trained by skipgram on wiki2014 .
we use the moses toolkit to create a statistical phrase-based machine translation model built on the best pre-processed data , as described above .
the use of various synchronous grammar based formalisms has been a trend for statistical machine translation .
the translation quality is evaluated by caseinsensitive bleu-4 metric .
we ran mt experiments using the moses phrase-based translation system .
information extraction ( ie ) is a technology that can be applied to identifying both sources and targets of new hyperlinks .
we use the attention-based nmt model introduced by bahdanau et al as our text-only nmt baseline .
recently , mikolov et al introduced an efficient way for inferring word embeddings that are effective in capturing syntactic and semantic relationships in natural language .
the srilm toolkit was used for training the language models using kneser-ney smoothing .
mikolov et al , 2013a , builds a translation matrix using linear regression that transforms the source language word vectors to the target language space .
for language model , we used sri language modeling toolkit to train a 4-gram model with modified kneser-ney smoothing .
in this paper , we propose to use the von mises-fisher distribution .
we used the srilm toolkit to build unpruned 5-gram models using interpolated modified kneser-ney smoothing .
we train an english language model on the whole training set using the srilm toolkit and train mt models mainly on a 10k sentence pair subset of the acl training set .
we use the moses smt toolkit to test the augmented datasets .
sentiment analysis is a nlp task that deals with extraction of opinion from a piece of text on a topic .
text segmentation is the task of splitting text into segments by placing boundaries within it .
models were built and interpolated using srilm with modified kneser-ney smoothing and the default pruning settings .
parameter optimization is performed with the diagonal variant of adagrad with minibatchs .
semantic role labeling ( srl ) is the task of automatically annotating the predicate-argument structure in a sentence with semantic roles .
extensive experiments have leveraged word embeddings to find general semantic relations .
sentiment analysis ( sa ) is a hot-topic in the academic world , and also in the industry .
to start with , we replace word types with corresponding neural language model representations estimated using the skip-gram model .
further , we apply a 4-gram language model trained with the srilm toolkit on the target side of the training corpus .
we use our reordering model for n-best re-ranking and optimize bleu using minimum error rate training .
specifically , we tested the methods word2vec using the gensim word2vec package and pretrained glove word embeddings .
we measure the translation quality using a single reference bleu .
the source of bilingual data used in the experiments is the europarl collection .
we use a fourgram language model with modified kneser-ney smoothing as implemented in the srilm toolkit .
for sentences , we tokenize each sentence by stanford corenlp and use the 300-d word embeddings from glove to initialize the models .
also , we initialized all of the word embeddings using the 300 dimensional pre-trained vectors from glove .
coreference resolution is the task of determining which mentions in a text refer to the same entity .
the translations are evaluated in terms of bleu score .
we tag the source language with the stanford pos tagger .
we use the moses translation system , and we evaluate the quality of the automatically produced translations by using the bleu evaluation tool .
in natural language , a word often assumes different meanings , and the task of determining the correct meaning , or sense , of a word in different contexts is known as word sense disambiguation ( wsd ) .
the weights of the different feature functions were optimised by means of minimum error rate training .
we implement the weight tuning component according to the minimum error rate training method .
we parsed the corpus with rasp and with the stanford pcfg parser .
we use a variant on the the publicly available madamira tool for the arabic msa-egy pair .
shen et al proposed a target dependency language model for smt to employ target-side structured information .
we measure the translation quality with automatic metrics including bleu and ter .
for evaluation , we used the case-insensitive bleu metric with a single reference .
semantic parsing is the task of translating natural language utterances to a formal meaning representation language ( cite-p-16-1-6 , cite-p-16-3-6 , cite-p-16-1-8 , cite-p-16-3-7 , cite-p-16-1-0 ) .
this means in practice that the language model was trained using the srilm toolkit .
we used yamcha as a text chunker , which is based on support vector machine .
we used the google news pretrained word2vec word embeddings for our model .
we use pre-trained 50 dimensional glove vectors 4 for word embeddings initialization .
nenkova et al found that high frequency word entrainment in dialogue is correlated with engagement and task success .
the language models used were 7-gram srilm with kneser-ney smoothing and linear interpolation .
we used 5-gram models , estimated using the sri language modeling toolkit with modified kneser-ney smoothing .
these language models were built up to an order of 5 with kneser-ney smoothing using the srilm toolkit .
word alignment is a fundamental problem in statistical machine translation .
we trained a 4-gram language model with kneser-ney smoothing and unigram caching using the sri-lm toolkit .
similarity is the intrinsic ability of humans and some animals to balance commonalities and differences when comparing objects that are not identical .
our baseline is a phrase-based mt system trained using the moses toolkit .
for this reason , we used glove vectors to extract the vector representation of words .
we utilize a maximum entropy model to design the basic classifier used in active learning for wsd .
we set the feature weights by optimizing the bleu score directly using minimum error rate training on the development set .
shutova defined metaphor interpretation as a paraphrasing task , where literal paraphrases for metaphorical expressions are derived from corpus data using a set of statistical measures .
we adopt two standard metrics rouge and bleu for evaluation .
we use the treebanks from the conll shared tasks on dependency parsing for evaluation .
an affective lexicon , wordnet-affect was used to identify words with emotional content in the text .
coreference resolution is a key problem in natural language understanding that still escapes reliable solutions .
although coreference resolution is a subproblem of natural language understanding , coreference resolution evaluation metrics have predominately been discussed in terms of abstract entities and hypothetical system errors .
all the feature weights were trained using our implementation of minimum error rate training .
from this , we extract an old domain sense dictionary , using the moses mt framework .
this approach relies on word embeddings for the computation of semantic relatedness with word2vec .
the character embeddings are computed using a method similar to word2vec .
the bleu metric was used for translation evaluation .
twitter is a social platform which contains rich textual content .
our phrase-based mt system is trained by moses with standard parameters settings .
the evaluation metric for the overall translation quality was case-insensitive bleu4 .
we used the sri language modeling toolkit to train a fivegram model with modified kneser-ney smoothing .
we use srilm with its default parameters for this purpose .
we use minimum error rate training with nbest list size 100 to optimize the feature weights for maximum development bleu .
unpruned language models were trained using lmplz which employs modified kneser-ney smoothing .
the models were implemented using scikit-learn module .
note that we use the naive bayes multinomial classifier in weka for classification .
relation extraction is the key component for building relation knowledge graphs , and it is of crucial significance to natural language processing applications such as structured search , sentiment analysis , question answering , and summarization .
a language model is a probability distribution over strings p ( s ) that attempts to reflect the frequency with which each string s occurs as a sentence in natural text .
we develop a semantic parser for this corpus .
for hindi , dependency annotation is done using paninian framework .
word embeddings have been trained using word2vec 4 tool .
part-of-speech ( pos ) tagging is a critical task for natural language processing ( nlp ) applications , providing lexical syntactic information .
we trained a 4-gram language model on this data with kneser-ney discounting using srilm .
motivated by the idea of addressing word confidence estimation problem as a sequence labeling process , we employ the conditional random fields for our model training , with wapiti toolkit .
we then follow standard heuristics and filtering strategies to extract hierarchical phrases from the union of the directional word alignments .
semantic role labeling ( srl ) is the process of assigning semantic roles to strings of words in a sentence according to their relationship to the semantic predicates expressed in the sentence .
to be the only parse , the reduction in ppl ¡ª relative to a 3-gram baseline .
we train a kn-smoothed 5-gram language model on the target side of the parallel training data with srilm .
sentence compression is the task of producing a shorter form of a grammatical source sentence , so that the new form will still be grammatical and it will retain the most important information of the source .
we trained the statistical phrase-based systems using the moses toolkit with mert tuning .
in order to present a comprehensive evaluation , we evaluated the accuracy of each model output using both bleu and chrf3 metrics .
the translation quality is evaluated by case-insensitive bleu-4 metric .
in this work , we employ the toolkit word2vec to pre-train the word embedding for the source and target languages .
our hierarchical phrase-based system is similar to the one described in .
we use glove vectors with 100 dimensions trained on wikipedia and gigaword as word embeddings , which we do not optimize during training .
framenet is a lexicalsemantic resource manually built by fn experts .
translation performance was measured by case-insensitive bleu .
we also use editor score as an outcome variable for a linear regression classifier , which we evaluate using 10-fold cross-validation in scikit-learn .
neural networks , working on top of conventional n-gram back-off language models , have been introduced in as a potential means to improve discrete language models .
for our experiments , we use a phrase-based translation system similar to moses .
this paper presents an unsupervised learning approach to building a non-english .
dependency parsing is a very important nlp task and has wide usage in different tasks such as question answering , semantic parsing , information extraction and machine translation .
we then implemented our model using moses toolkit with kenlm as the language model in 5-gram setting .
in our experiments , we choose to use the published glove pre-trained word embeddings .
in this task , we used conditional random fields .
sentiment analysis is a research area where does a computational analysis of people ’ s feelings or beliefs expressed in texts such as emotions , opinions , attitudes , appraisals , etc . ( cite-p-12-1-3 ) .
we train attentional sequence-to-sequence models implemented in nematus .
we use a fourgram language model with modified kneser-ney smoothing as implemented in the srilm toolkit .
this section describes the classic hidden markov model based alignment model .
the language model was generated from the europarl corpus using the sri language modeling toolkit .
word embedding provides an unique property to capture semantics and syntactic information of different words .
we measure the translation quality with automatic metrics including bleu and ter .
coreference resolution is the task of partitioning a set of entity mentions in a text , where each partition corresponds to some entity in an underlying discourse model .
coreference resolution is the process of linking multiple mentions that refer to the same entity .
we trained a 5-gram language model on the xinhua portion of gigaword corpus using the srilm toolkit .
in recent years , error mining techniques have been developed to help identify the most likely sources of parsing failure .
the matrix was then normalized with pointwise mutual information .
cohesion can be defined as a set of resources linking within a text that organize the text together ( cite-p-16-1-12 ) .
we use the svm implementation from scikit-learn , which in turn is based on libsvm .
we used moses , a phrase-based smt toolkit , for training the translation model .
word sense disambiguation ( wsd ) is the task of determining the correct meaning ( “ sense ” ) of a word in context , and several efforts have been made to develop automatic wsd systems .
coreference resolution is a set partitioning problem in which each resulting partition refers to an entity .
a 4-gram language model was trained on the target side of the parallel data using the srilm toolkit .
the pre-processed monolingual sentences will be used by srilm or berkeleylm to train a n-gram language model .
for the classification task , we use pre-trained glove embedding vectors as lexical features .
the translation models are included within a log-linear model which allows a weighted combination of features functions .
the annotation scheme is derived from the universal stanford dependencies , the google universal part-of-speech tags and the interset interlingua for morphological tagsets .
for the classifiers we use the scikit-learn machine learning toolkit .
we report the mt performance using the original bleu metric .
the language models in this experiment were trigram models with good-turing smoothing built using srilm .
relation extraction is the key component for building relation knowledge graphs , and it is of crucial significance to natural language processing applications such as structured search , sentiment analysis , question answering , and summarization .
we used the sri language modeling toolkit to train a fivegram model with modified kneser-ney smoothing .
we use the 100-dimensional pre-trained word embeddings trained by word2vec 2 and the 100-dimensional randomly initialized pos tag embeddings .
kalchbrenner et al , 2014 ) proposes a cnn framework with multiple convolution layers , with latent , dense and low-dimensional word embeddings as inputs .
the penn discourse treebank is the largest available corpus of annotations for discourse relations , covering one million words of the wall street journal .
another corpus has been annotated for discourse phenomena in english , the penn discourse treebank .
zens and ney showed that itg constraints allow a higher flexibility in word-ordering for longer sentences than the conventional ibm model .
semantic role labeling ( srl ) is the task of automatic recognition of individual predicates together with their major roles ( e.g . frame elements ) as they are grammatically realized in input sentences .
we use srilm toolkit to build a 5-gram language model with modified kneser-ney smoothing .
results are reported using case-insensitive bleu with a single reference .
we use 300-dimensional word embeddings from glove to initialize the model .
sentiment classification is a useful technique for analyzing subjective information in a large number of texts , and many studies have been conducted ( cite-p-15-3-1 ) .
feature weights were trained with minimum error-rate training on the news-test2008 development set using the dp beam search decoder and the mert implementation of the moses toolkit .
using word2vec , we compute word embeddings for our text corpus .
in natural language , a word often assumes different meanings , and the task of determining the correct meaning , or sense , of a word in different contexts is known as word sense disambiguation ( wsd ) .
we used moses , a state-of-the-art phrase-based smt model , in decoding .
we use sri language model toolkit to train a 5-gram model with modified kneser-ney smoothing on the target-side training corpus .
we implement logistic regression with scikit-learn and use the lbfgs solver .
twitter is a microblogging site where people express themselves and react to content in real-time .
we follow ji and eisenstein , exploiting a transition-based framework for rst discourse parsing .
unsupervised word embeddings trained from large amounts of unlabeled data have been shown to improve many nlp tasks .
sentence compression is a text-to-text generation task in which an input sentence must be transformed into a shorter output sentence which accurately reflects the meaning in the input and also remains grammatically well-formed .
system tuning was carried out using minimum error rate training optimised with k-best mira on a held out development set .
we use a support vector machine -based chunker yamcha for the chunking process .
we train a kn-smoothed 5-gram language model on the target side of the parallel training data with srilm .
the statistical significance test is performed by the re-sampling approach .
discourse segmentation is the task of identifying coherent clusters of sentences and the points of transition between those groupings .
in particular , collobert et al and turian et al learn word embeddings to improve the performance of in-domain pos tagging , named entity recognition , chunking and semantic role labelling .
relation extraction is the task of detecting and classifying relationships between two entities from text .
top accuracy on the entire data set and on the semantic subset was reached by mikolov et al using a skip-gram predict model .
our system is built using the open-source moses toolkit with default settings .
the phrase-based translation model uses the con- the baseline lm was a regular n-gram lm with kneser-ney smoothing and interpolation by means of the srilm toolkit .
note that we use the naive bayes multinomial classifier in weka for classification .
we then learn reranking weights using minimum error rate training on the development set for this combined list , using only these two features .
we trained word vectors with the two architectures included in the word2vec software .
coreference resolution is a key task in natural language processing ( cite-p-13-1-8 ) aiming to detect the referential expressions ( mentions ) in a text that point to the same entity .
named entity recognition ( ner ) is a fundamental task in text mining and natural language understanding .
otero et al used wikipedia categories as the restriction to detect the equivalents within small-scale reliable candidates .
we also measure overall performance with uncased bleu .
as embedding vectors , we used the publicly available representations obtained from the word2vec cbow model .
socher et al , 2012 , uses a recursive neural network in relation extraction , and further use lstm .
here , for textual representation of captions , we use fisher-encoded word2vec features .
coreference resolution is the task of clustering a set of mentions in the text such that all mentions in the same cluster refer to the same entity .
we use corpus-level bleu score to quantitatively evaluate the generated paragraphs .
moreover , throughout this paper we use the hierarchical phrase-based translation system , which is based on a synchronous contextfree grammar model .
the pun is defined as “ a joke exploiting the different possible meanings of a word or the fact that there are words which sound alike but have different meanings ” ( cite-p-7-1-6 ) .
all of our parsing models are based on the transition-based dependency parsing paradigm .
we used the sri language modeling toolkit to train a fivegram model with modified kneser-ney smoothing .
beam search decoding is effective with a small beam size .
name tagging is a key task for language understanding , and provides input to several other tasks such as question answering , summarization , searching and recommendation .
we use srilm toolkits to train two 4-gram language models on the filtered english blog authorship corpus and the xinhua portion of gigaword corpus , respectively .
the advent of the supervised method proposed by gildea and jurafsky has led to the creation of annotated corpora for semantic role labeling .
we use three standard human judgements datasets -mc , rg and wordsim353 , composed of 30 , 65 , and 353 pairs of terms respectively .
we extract the named entities from the web pages using the stanford named entity recognizer .
we use the popular word2vec 1 tool proposed by mikolov et al to extract the vector representations of words .
we used 300-dimensional pre-trained glove word embeddings .
we also measure overall performance with uncased bleu .
we evaluate the performance of different translation models using both bleu and ter metrics .
language models were built using the sri language modeling toolkit with modified kneser-ney smoothing .
we utilized pre-trained global vectors trained on tweets .
a pun is a form of wordplay in which one signifier ( e.g. , a word or phrase ) suggests two or more meanings by exploiting polysemy , or phonological similarity to another signifier , for an intended humorous or rhetorical effect .
these word vectors can be randomly initialized , or be pre-trained from text corpus with learning algorithms .
our approach to relation embedding is based on a variant of the glove word embedding model .
this task can be formulated as a topic modeling problem for which we chose to employ latent dirichlet allocation .
all the weights of those features are tuned by using minimal error rate training .
we show that ltag-based features improve on the best known set of features used in current srl .
we evaluated translation quality using uncased bleu and ter .
a language model is a probability distribution over strings p ( s ) that attempts to reflect the frequency with which each string s occurs as a sentence in natural text .
we use several classifiers including logistic regression , random forest and adaboost implemented in scikit-learn .
we used the pb smt system in moses 12 for je and kj translation tasks .
the srilm toolkit was used to build the trigram mkn smoothed language model .
in collobert et al , the authors proposed a unified cnn architecture to tackle various nlp problems traditionally handled with statistical approaches .
we use the moses package to train a phrase-based machine translation model .
similar to goldwater and griffiths and johnson , toutanova and johnson also use bayesian inference for pos tagging .
we obtained both phrase structures and dependency relations for every sentence using the stanford parser .
the language model used was a 5-gram with modified kneserney smoothing , built with srilm toolkit .
named entity recognition ( ner ) is the task of detecting named entity mentions in text and assigning them to their corresponding type .
the most widely used topic modeling approach is the latent dirichlet allocation which is based on latent semantic analysis and probabilistic latent semantic analysis .
target language models were trained on the english side of the training corpus using the srilm toolkit .
we use the pre-trained glove 50-dimensional word embeddings to represent words found in the glove dataset .
semantic role labeling ( srl ) is the task of automatically annotating the predicate-argument structure in a sentence with semantic roles .
we train a 5-gram language model with the xinhua portion of english gigaword corpus and the english side of the training set using the srilm toolkit .
we initialize these word embeddings with glove vectors .
relation extraction ( re ) has been defined as the task of identifying a given set of semantic binary relations in text .
lexical simplification is the task to find and substitute a complex word or phrase in a sentence with its simpler synonymous expression .
thus , zesch and gurevych used a semi-automatic process to create word pairs from domain-specific corpora .
crowdsourcing is a promising new mechanism for collecting large volumes of annotated data at low cost .
for data preparation and processing we use scikit-learn .
distributed representations for words and sentences have been shown to significantly boost the performance of a nlp system .
the word embeddings are initialized with the publicly available word vectors trained through glove 5 and updated through back propagation .
for the language model , we used srilm with modified kneser-ney smoothing .
soricut and marcu use a standard bottomup chart parsing algorithm to determine the discourse structure of sentences .
we use word2vec tool for learning distributed word embeddings .
we use the skll and scikit-learn toolkits .
hierarchical phrase-based translation has emerged as one of the dominant current approaches to statistical machine translation .
we use five datasets from the conll-x shared task .
kulkarni et al used a synthetic task to evaluate how well diachronic distributional models can detect semantic shift .
the character embeddings are computed using a method similar to word2vec .
we implement classification models using keras and scikit-learn .
however , ccg is a binary branching grammar , and as such , can not leave np structure underspecified .
visargue system offers the first web-based , interactive visual analytics approach of multi-party discourse data .
seki et al proposed a probabilistic model for zero pronoun detection and resolution that uses hand-crafted case frames .
discourse parsing is a challenging natural language processing ( nlp ) task that has utility for many other nlp tasks such as summarization , opinion mining , etc . ( cite-p-17-3-3 ) .
we use the stanford pos tagger to obtain the lemmatized corpora for the parss task .
for language model , we used sri language modeling toolkit to train a 4-gram model with modified kneser-ney smoothing .
relation extraction is the task of finding relational facts in unstructured text and putting them into a structured ( tabularized ) knowledge base .
coreference resolution is the task of partitioning the set of mentions of discourse referents in a text into classes ( or ‘ chains ’ ) corresponding to those referents ( cite-p-12-3-14 ) .
the dominant approach for domain adaptation is training on large-scale out-of-domain data and then fine-tuning on the in-domain data .
of such an extension , we present a complete , correct , terminating extension of earley ' s algorithm that uses restriction .
for nb and svm , we used their implementation available in scikit-learn .
the similarity-based model showed error rates down to 0 . 16 , far lower than both em-based clustering and resnik ¡¯ s wordnet model .
distributional models use statistics of word cooccurrences to predict semantic similarity of words and phrases , based on the observation that semantically similar words occur in similar contexts .
a 4-gram language model is trained on the monolingual data by srilm toolkit .
semantic parsing is a domain-dependent process by nature , as its output is defined over a set of domain symbols .
we use the stanford corenlp for obtaining pos tags and parse trees from our data .
part-of-speech tagging is a key process for various tasks such as ` information extraction , text-to-speech synthesis , word sense disambiguation and machine translation .
we use the mallet implementation of conditional random fields .
in particular , rush et al proposed an approach for the abstractive summarization of sentences combining a neural language model with a contextual encoder .
most previous approaches that address bilingual lexicon extraction from comparable corpora are based on the standard approach .
performance is measured in terms of bleu and ter computed using the multeval script .
sentiment analysis is a natural language processing task whose aim is to classify documents according to the opinion ( polarity ) they express on a given subject ( cite-p-13-8-14 ) .
in movies that fail the test , women are in fact portrayed as less-central and less-important characters .
we used a 5-gram language model with modified kneser-ney smoothing implemented using the srilm toolkit .
we will show translation quality measured with the bleu score as a function of the phrase table size .
if the anaphor is a pronoun but no referent is found in the cache , it is then necessary to operatingsearch memory .
in this work , we use a nmt system featuring long short-term memory units -in both the encoder and decoder-and equipped with an attention mechanism .
sentiment classification is a special task of text categorization that aims to classify documents according to their opinion of , or sentiment toward a given subject ( e.g. , if an opinion is supported or not ) ( cite-p-11-1-2 ) .
text segmentation is the task of splitting text into segments by placing boundaries within it .
bunescu and mooney show that using dependency trees to generate the input sequence to a model performs well in relation extraction tasks .
for the language model we use the corpus of 60,000 simple english wikipedia articles 3 and build a 3-gram language model with kneser-ney smoothing trained with srilm .
we use conditional random fields for sequence labelling .
relation extraction is a subtask of information extraction that finds various predefined semantic relations , such as location , affiliation , rival , etc. , between pairs of entities in text .
sentence compression is a paraphrasing task where the goal is to generate sentences shorter than given while preserving the essential content .
jiang and zhai recently proposed an instance re-weighting framework to take domain shift into account .
we apply sri language modeling toolkit to train a 4-gram language model with kneser-ney smoothing .
coreference resolution is the process of finding discourse entities ( markables ) referring to the same real-world entity or concept .
semantic role labeling ( srl ) is a major nlp task , providing a shallow sentence-level semantic analysis .
table 1 presents the results from the automatic evaluation , in terms of bleu and nist test .
xing et al presented topic aware response generation by incorporating topic words obtained from a pre-trained lda model .
we used svm classifier that implements linearsvc from the scikit-learn library .
we trained a subword model using bpe with 29,500 merge operations .
besides , we propose a hierarchical neural attention mechanism to capture the sentiment attention .
for all the systems we train , we build n-gram language model with modified kneserney smoothing using kenlm .
sentiment analysis is a fundamental problem aiming to give a machine the ability to understand the emotions and opinions expressed in a written text .
dependency parsing is a topic that has engendered increasing interest in recent years .
sentiment analysis ( sa ) is a fundamental problem aiming to allow machines to automatically extract subjectivity information from text ( cite-p-16-5-8 ) , whether at the sentence or the document level ( cite-p-16-3-3 ) .
conditional random fields are undirected graphical models trained to maximize a conditional probability .
the translation quality is evaluated by caseinsensitive bleu-4 metric .
all input and output must conform to the format of the conll-2012 shared task on coreference resolution .
for language model , we use a trigram language model trained with the srilm toolkit on the english side of the training corpus .
the evaluation metric is the case-insensitive bleu4 .
sentiment analysis is a natural language processing task whose aim is to classify documents according to the opinion ( polarity ) they express on a given subject ( cite-p-13-8-14 ) .
the language models were trained with kneser-ney backoff smoothing using the sri language modeling toolkit , .
this result is opposed to yamashita stating that scrambling is unrelated to information structure .
we used the sri language modeling toolkit to train a fivegram model with modified kneser-ney smoothing .
shen et al propose the well-formed dependency structure to filter the hierarchical rule table .
we use the multi-class logistic regression classifier from the liblinear package 2 for the prediction of edit scripts .
then we perform minimum error rate training on validation set to give different features corresponding reasonable weights .
supertagging is the tagging process of assigning the correct elementary tree of ltag , or the correct supertag , to each word of an input sentence 1 .
for our experiments , we use 40,000 sentences from europarl for each language pair following the basic setup of tiedemann .
relation extraction is a fundamental step in many natural language processing applications such as learning ontologies from texts ( cite-p-12-1-0 ) and question answering ( cite-p-12-3-6 ) .
we use srilm to train a 5-gram language model on the xinhua portion of the english gigaword corpus 5th edition with modified kneser-ney discounting .
the target-side language models were estimated using the srilm toolkit .
ambiguity is a common feature of weps and wsd .
the scaling factors are tuned with mert with bleu as optimization criterion on the development sets .
relation extraction is the task of finding relationships between two entities from text .
we use the simplified factual statement extractor model 3 of heilman and smith .
in the reranking stage , we propose an exact 1-best search algorithm .
we use the scikit-learn toolkit as our underlying implementation .
we used the sri language modeling toolkit to train lms on our training data for each ilr level .
for evaluation , we measured the end translation quality with case-sensitive bleu .
this architecture is similar to the cbow model of , where the center word is replaced by a label .
we use sri language model toolkit to train a 5-gram model with modified kneser-ney smoothing on the target-side training corpus .
a 4-gram language model was trained on the monolingual data by the srilm toolkit .
blitzer et al , 2007 ) use structural correspondence learning to adapt the vocabulary of the various domains .
named entity recognition ( ner ) is a well-known problem in nlp which feeds into many other related tasks such as information retrieval ( ir ) and machine translation ( mt ) and more recently social network discovery and opinion mining .
we preprocessed the corpus with tokenization and true-casing tools from the moses toolkit .
we perform pre-training using the skipgram nn architecture available in the word2vec tool .
we apply bi-directional long shortterm memory networks to encode an input utterance into a vector .
for our experiments reported here , we obtained word vectors using the word2vec tool and the text8 corpus .
we use the moses mt framework to build a standard statistical phrase-based mt model using our old-domain training data .
here too , we used the weka implementation of the na茂ve bayes model and the svmlight implementation of the svm .
we used the penn treebank wall street journal corpus .
many words have multiple meanings , and the process of identifying the correct meaning , or sense of a word in context , is known as word sense disambiguation ( wsd ) .
the srilm toolkit was used to build this language model .
word segmentation is the first step prior to word alignment for building statistical machine translations ( smt ) on language pairs without explicit word boundaries such as chinese-english .
we use the pre-trained glove 50-dimensional word embeddings to represent words found in the glove dataset .
we use 300d glove vectors trained on 840b tokens as the word embedding input to the lstm .
in this paper , the proposed model improves the acquirement ability for oov translation through web mining and solves the translation pair .
as a classifier , we chose support vector machines .
for the neural models , we use 100-dimensional glove embeddings , pre-trained on wikipedia and gigaword .
twitter is a well-known social network service that allows users to post short 140 character status update which is called “ tweet ” .
to extract part-of-speech tags , phrase structure trees , and typed dependencies , we use the stanford parser on both train and test sets .
the targetside 4-gram language model was estimated using the srilm toolkit and modified kneser-ney discounting with interpolation .
we develop translation models using the phrase-based moses smt system .
we train a linear support vector machine classifier using the efficient liblinear package .
in this paper , we introduce a discriminatively trained , globally normalized log-linear model of lexical translation .
this baseline uses pre-trained word embeddings using word2vec cbow and fasttext .
we use srilm for training a trigram language model on the english side of the training corpus .
we follow cite-p-31-3-9 , use freebase as source of distant supervision , and employ wikipedia as source of unlabelled text ¡ª .
coreference resolution is the problem of identifying which noun phrases ( nps , or mentions ) refer to the same real-world entity in a text or dialogue .
we use the stanford part-of-speech tagger and chunker to identify noun and verb phrases in the sentences .
we use the stanford dependency parser with the collapsed representation so that preposition nodes become edges .
during the last few years , smt systems have evolved from the original word-based approach to phrase-based translation systems .
word sense disambiguation ( wsd ) is a natural language processing ( nlp ) task in which the correct meaning ( sense ) of a word in a given context is to be determined .
our system is based on the phrase-based part of the statistical machine translation system moses .
since the english treebanks are in constituency format , we used the stanfordconverter to convert the parse trees to dependencies and ignored the arc labels .
translation performances are measured with case-insensitive bleu4 score .
as for ej translation , we use the stanford parser to obtain english abstraction trees .
the composite kernel consists of an entity kernel and a convolution parse tree kernel .
we experimented using the standard phrase-based statistical machine translation system as implemented in the moses toolkit .
morpa is a morphological parser developed for use in the text-to-speech conversion system .
semantic role labeling ( srl ) is the process of producing such a markup .
gram language models are trained over the target-side of the training data , using srilm with modified kneser-ney discounting .
for phrase-based smt translation , we used the moses decoder and its support training scripts .
in this paper , we integrate the context and glosses of the target word into a unified framework .
we use distributed word vectors trained on the wikipedia corpus using the word2vec algorithm .
phrase-based approaches to statistical machine translation have recently achieved impressive results , leading to significant improvements in accuracy over the original ibm models .
wikipedia , as it is a popular choice due to its large and ever expanding coverage and its ability to keep up with world events on a timely basis .
the weights for the language model and the grammar , are tuned towards bleu using mert .
our phrase-based mt system is trained by moses with standard parameters settings .
following lample et al , the character-based representation is computed with a bi-lstm whose parameters are defined by users .
these models were implemented using the package scikit-learn .
discourse parsing is the process of discovering the latent relational structure of a long form piece of text and remains a significant open challenge .
we use liblinear logistic regression module to classify document-level embeddings .
the models are built using the sri language modeling toolkit .
we used 300 dimensional skip-gram word embeddings pre-trained on pubmed .
transliteration is the task of converting a word from one writing script to another , usually based on the phonetics of the original word .
relation extraction ( re ) is a task of identifying typed relations between known entity mentions in a sentence .
lda is a probabilistic generative model that can be used to uncover the underlying semantic structure of a document collection .
pang and lee attempted to improve the performance of an svm classifier by identifying and removing objective sentences from the texts .
unfortunately , the non-projective parsing problem is known to be np-hard for all but the simplest models .
previous work has focused on congressional debates , company-internal discussions , and debates in online forums .
the feature weights 位 m are tuned with minimum error rate training .
we used the srilm toolkit to create 5-gram language models with interpolated modified kneser-ney discounting .
we use the moses phrase-based mt system with standard features .
we use the mallet implementation of a maximum entropy classifier to construct our models .
language models were trained with the kenlm toolkit .
coreference resolution is the task of partitioning the set of mentions of discourse referents in a text into classes ( or ‘ chains ’ ) corresponding to those referents ( cite-p-12-3-14 ) .
for word embeddings , we consider word2vec and glove .
for word embeddings , we used popular pre-trained word vectors from glove .
we trained linear-chain conditional random fields as the baseline .
we use case-sensitive bleu-4 to measure the quality of translation result .
we used the implementation of random forest in scikitlearn as the classifier .
in this paper , we present an approach that leverages structured knowledge contained in fdts .
for sampling nodes , non-interactive active learning algorithms exclude expert annotators ’ human labels .
for wordnet , we employ the basictokenizer built in bert to tokenize text , and look up synsets for each word using nltk .
for this language model , we built a trigram language model with kneser-ney smoothing using srilm from the same automatically segmented corpus .
semantic parsing is the task of mapping a natural language ( nl ) sentence into a completely formal meaning representation ( mr ) or logical form .
the language models are 4-grams with modified kneser-ney smoothing which have been trained with the srilm toolkit .
barzilay and mckeown extracted both single-and multiple-word paraphrases from a sentence-aligned corpus for use in multi-document summarization .
lei et al also use low-rank tensor learning in the context of dependency parsing , where like in our case dependencies are represented by conjunctive feature spaces .
in addition to these two key indicators , we evaluated the translation quality using an automatic measure , namely bleu score .
ccg is a linguistically motivated categorial formalism for modeling a wide range of language phenomena .
sentence compression is the task of compressing long , verbose sentences into short , concise ones .
to optimize the system towards a maximal bleu or nist score , we use minimum error rate training as described in .
for the hierarchical phrase-based model we used the default moses rule extraction settings , which are taken from chiang .
the log-linear feature weights are tuned with minimum error rate training on bleu .
wikipedia is a massively multilingual resource that currently hosts 295 languages and contains naturally annotated markups 2 and rich informational structures through crowdsourcing for 35 million articles in 3 billion words .
we used the srilm toolkit to build unpruned 5-gram models using interpolated modified kneser-ney smoothing .
in the work of mikolov et al , they introduced two new architectures for estimating continuous representations of words using log-linear models , called continuous bag-of-word and continuous skip-gram .
the log-linear feature weights are tuned with minimum error rate training on bleu .
in this run , we use a sentence vector derived from word embeddings obtained from word2vec .
to measure the importance of the generated questions , we use lda to identify the important sub-topics from the given body of texts .
lda is a topic model that generates topics based on word frequency from a set of documents .
for the fluency and grammaticality features , we train 4-gram lms using the development dataset with the sri toolkit .
we use the glove vectors of 300 dimension to represent the input words .
davidov et al utilize hashtags and smileys to build a largescale annotated tweet dataset automatically .
negation is a linguistic phenomenon present in all languages ( cite-p-12-3-6 , cite-p-12-1-5 ) .
the embeddings were trained over the english wikipedia using word2vec .
the feature weights are tuned to optimize bleu using the minimum error rate training algorithm .
shallow semantic representations can prevent the sparseness of deep structural approaches and the weakness of cosine similarity based models .
we used the annotation and features available for the training set , to train the attribute detectors using a linear svm classifier .
the srilm language modelling toolkit was used with interpolated kneser-ney discounting .
parameter tuning was carried out using both k-best mira and minimum error rate training on a held-out development set .
translation quality is measured in truecase with bleu on the mt08 test sets .
we have shown that there are two distinct ways of representing the parses of a tag .
coreference resolution is the problem of identifying which noun phrases ( nps , or mentions ) refer to the same real-world entity in a text or dialogue .
we use word vectors produced by the cbow approach-continuous bagof-words .
our 5-gram language model was trained by srilm toolkit .
convolutional neural networks are useful in many nlp tasks , such as language modeling , semantic role labeling and semantic parsing .
next we consider the context-predicting vectors available as part of the word2vec 6 project .
a pun is a form of wordplay , which is often profiled by exploiting polysemy of a word or by replacing a phonetically similar sounding word for an intended humorous effect .
semantic parsing is the task of mapping natural language utterances to machine interpretable meaning representations .
we use the scikit-learn toolkit as our underlying implementation .
automatic alignment can be performed using different algorithms such as em or hmm-based alignment .
faruqui et al proposed a related approach that performs a post-processing of word embeddings on the basis of lexical relations from the same resources .
we are using word embeddings trained on google news corpus for our experiments .
for training the model , we use the linear kernel svm implemented in the scikit-learn toolkit .
similarity is the intrinsic ability of humans and some animals to balance commonalities and differences when comparing objects that are not identical .
arabic is a morphologically rich language , in which a word carries not only inflections but also clitics , such as pronouns , conjunctions , and prepositions .
sentiment analysis is the task of identifying the polarity ( positive , negative or neutral ) of review .
sentiment classification is a hot research topic in natural language processing field , and has many applications in both academic and industrial areas ( cite-p-17-1-16 , cite-p-17-1-12 , cite-p-17-3-4 , cite-p-17-3-3 ) .
text segmentation is the task of splitting text into segments by placing boundaries within it .
gao et al described a transformationbased converter to transfer a certain annotationstyle word segmentation result to another style .
these word representations are used in various natural language processing tasks such as part-of-speech tagging , chunking , named entity recognition , and semantic role labeling .
the word vectors are learned using a skip-gram model with negative sampling , implemented in the word2vec toolkit .
the hierarchical phrase-based model is capable of capturing rich translation knowledge with the synchronous context-free grammar .
we use the moses smt toolkit to test the augmented datasets .
we use srilm to train a 5-gram language model on the xinhua portion of the english gigaword corpus 5th edition with modified kneser-ney discounting .
relation extraction is a key step towards question answering systems by which vital structured data is acquired from underlying free text resources .
we translated each german sentence using the moses statistical machine translation toolkit .
heilman et al extended this approach and worked towards retrieving relevant reading materials for language learners in the reap 3 project .
word embeddings are low-dimensional vector representations of words such as word2vec that recently gained much attention in various semantic tasks .
for our baseline we use the moses software to train a phrase based machine translation model .
for the language model , we used sri language modeling toolkit to train a trigram model with modified kneser-ney smoothing on the 31 , 149 english sentences .
we use binary crossentropy loss and the adam optimizer for training the nil-detection models .
a spelling-based model that directly maps english letter sequences into arabic letters was developed by al-onaizan and knight .
we use the seq2seq attention architecture with 2 lstm layers for both encoder and decoder , and 512 hidden nodes in each layer .
twitter is a rich resource for information about everyday events – people post their tweets to twitter publicly in real-time as they conduct their activities throughout the day , resulting in a significant amount of mundane information about common events .
event coreference resolution is the task of determining which event mentions in a text refer to the same real-world event .
we use sri language modeling toolkit to train a 5-gram language model on the english sentences of fbis corpus .
srilm toolkit is used to build these language models .
we test the statistical significance of differences between various mt systems using the bootstrap resampling method .
the word2vec is among the most widely used word embedding models today .
we use the sentiment pipeline of stanford corenlp to obtain this feature .
an interesting implementation to get the word embeddings is the word2vec model which is used here .
the target language model is built on the target side of the parallel data with kneser-ney smoothing using the irstlm tool .
we use a fourgram language model with modified kneser-ney smoothing as implemented in the srilm toolkit .
for the classification task , we use pre-trained glove embedding vectors as lexical features .
our baseline system was a vanilla phrase-based system built with moses using default settings .
central to our approach is the intuition that word meaning is represented as a probability distribution over a set of latent senses and is modulated by context .
word sense disambiguation ( wsd ) is formally defined as the task of computationally identifying senses of a word in a context .
recent years have witnessed the success of various statistical machine translation models using different levels of linguistic knowledgephrase , hiero , and syntax-based .
we use an nmt-small model from the opennmt framework for the neural translation .
we implement classification models using keras and scikit-learn .
kalchbrenner et al proposed a dynamic convolution neural network with multiple layers of convolution and k-max pooling to model a sentence .
for each gradient step , the step size is calculated using adagrad .
semantic role labeling ( srl ) is the task of labeling the predicate-argument structures of sentences with semantic frames and their roles ( cite-p-18-1-2 , cite-p-18-1-19 ) .
the smt weighting parameters were tuned by mert using the development data .
named entity recognition ( ner ) is the task of identifying and typing phrases that contain the names of persons , organizations , locations , and so on .
the inversion transduction grammar of wu is a type of context-free grammar for generating two languages synchronously .
we use 300d glove vectors trained on 840b tokens as the word embedding input to the lstm .
conditional random fields are undirected graphical models that are conditionally trained .
relation classification is a crucial ingredient in numerous information extraction systems seeking to mine structured facts from text .
we use glove word embeddings , an unsupervised learning algorithm for obtaining vector representations of words .
this study is called morphological analysis .
we train our neural model with stochastic gradient descent and use adagrad to update the parameters .
transliteration is the task of converting a word from one writing script to another , usually based on the phonetics of the original word .
in our experiments we use a publicly available implementation of conditional random fields .
socher et al later introduced the recursive neural network architecture for supervised learning tasks such as syntactic parsing and sentiment analysis .
one of the first challenges in sentiment analysis is the vast lexical diversity of subjective language .
we used the moses toolkit for performing statistical machine translation .
twitter is a popular microblogging service which provides real-time information on events happening across the world .
we further investigated the usefulness of using lexicons using a recurrent neural network with bidirectional long short-term memory .
the language models are 4-grams with modified kneser-ney smoothing which have been trained with the srilm toolkit .
collobert and weston and collobert et al employed a deep learning framework for multi-task learning including part-of-speech tagging , chunking , namedentity recognition , language modelling and semantic role-labeling .
we use srilm to train a 5-gram language model on the target side of our training corpus with modified kneser-ney discounting .
negation is a linguistic phenomenon present in all languages ( cite-p-12-3-6 , cite-p-12-1-5 ) .
moreover , xing et al incorporated topic words into seq2seq frameworks , where topic words are obtained from a pre-trained l-da model .
it is a sequence-tosequence neural system with attention .
table 4 shows the bleu scores of the output descriptions .
luong and manning proposed a hybrid scheme that consults character-level information whenever the model encounters an oov word .
our baseline system is an standard phrase-based smt system built with moses .
language modeling is trained using kenlm using 5-grams , with modified kneser-ney smoothing .
neural machine translation has recently become the dominant approach to machine translation .
we use a fourgram language model with modified kneser-ney smoothing as implemented in the srilm toolkit .
relation extraction is the task of recognizing and extracting relations between entities or concepts in texts .
unfortunately , wordnet is a fine-grained resource , which encodes possibly subtle sense distictions .
we trained kneser-ney discounted 5-gram language models on each available corpus using the srilm toolkit .
the model parameters of word embedding are initialized using word2vec .
translation model has been extensively employed in question search and has been shown to outperform the traditional ir methods significantly .
tanev and magnini proposed a weakly supervised method that requires as training data a list of terms without context for each class under consideration .
we trained a trigram language model on the chinese side , with the srilm toolkit , using the modified kneser-ney smoothing option .
it has previously been shown that word embeddings represent the contextualised lexical semantics of words .
we use word embeddings of dimension 100 pretrained using word2vec on the training dataset .
taking the sequence of the word representation as input , our flat ner layer enables capturing context representation by a long short-term memory layer .
lexical simplification is a subtask of the more general text simplification task which attempts at reducing the cognitive complexity of a text so that it can be ( better ) understood by a larger audience .
we used the svm light package with a linear kernel .
this problem can be alleviated by long-short term memory units .
we use srilm for training the 5-gram language model with interpolated modified kneser-ney discounting .
however , dependency parsing , which is a popular choice for japanese , can incorporate only shallow syntactic information , i.e. , pos tags , compared with the richer syntactic phrasal categories in constituency parsing .
here we use stanford corenlp toolkit to deal with the co-reference problem .
we perform the mert training to tune the optimal feature weights on the development set .
we trained a 4-gram language model with kneser-ney smoothing and unigram caching using the sri-lm toolkit .
we adopt pretrained embeddings for word forms with the provided training data by word2vec .
in our experiments we use a publicly available implementation of conditional random fields .
sun and xu enhanced a cws model by interpolating statistical features of unlabeled data into the crfs model .
we use a minibatch stochastic gradient descent algorithm together with the adam optimizer .
coreference resolution is the problem of identifying which noun phrases ( nps , or mentions ) refer to the same real-world entity in a text or dialogue .
we used the logistic regression implemented in the scikit-learn library with the default settings .
in this paper , we propose a sentiment-aligned topic model ( satm ) for product aspect rating prediction .
results were evaluated with both bleu and nist metrics .
coreference resolution is the task of clustering a sequence of textual entity mentions into a set of maximal non-overlapping clusters , such that mentions in a cluster refer to the same discourse entity .
we used the implementation of the scikit-learn 2 module .
for language model , we used sri language modeling toolkit to train a 4-gram model with modified kneser-ney smoothing .
for this purpose , we use the moses toolkit for training translation models and decoding , as well as srilm 2 to build the language models .
word sense disambiguation ( wsd ) is a problem of finding the relevant clues in a surrounding context .
word alignment is a critical first step for building statistical machine translation systems .
stemming is a heuristic approach to reducing form-related sparsity issues .
relation extraction is the task of predicting attributes and relations for entities in a sentence ( zelenko et al. , 2003 ; bunescu and mooney , 2005 ; guodong et al. , 2005 ) .
we apply the adam algorithm for optimization , where the parameters of adam are set as in .
we used the srilm toolkit to create 5-gram language models with interpolated modified kneser-ney discounting .
luong and manning have proposed a hybrid nmt model flexibly switching from the word-based to the character-based model .
table 1 shows the performance for the test data measured by case sensitive bleu .
we used the penn treebank wsj corpus to perform the empirical evaluation of the considered approaches .
for training the translation model and for decoding we used the moses toolkit .
our model appears to be able to do well also on recognizing non-overlapping mentions .
bilingual lexicons serve as an indispensable source of knowledge for various cross-lingual tasks such as cross-lingual information retrieval or statistical machine translation .
we use a popular word2vec neural language model to learn the word embeddings on an unsupervised tweet corpus .
hassan and menezes proposed an approach for normalizing social media text which used random walk framework on a contextual similarity bipartite graph constructed from n-grams sequences , which they interpolated with edit distance .
the weights of the different feature functions were tuned by means of minimum error-rate training executed on the europarl development corpus .
we use glove word embeddings , an unsupervised learning algorithm for obtaining vector representations of words .
the translation quality is evaluated by case-insensitive bleu-4 .
bengio et al propose a feedforward neural network to train a word-level language model with a limited n-gram history .
the evaluation metric is casesensitive bleu-4 .
li and yarowsky proposed an unsupervised method extracting the relation between a full-form phrase and its abbreviation from monolingual corpora .
the weights are optimized over the bleu metric .
dependency parsing is a topic that has engendered increasing interest in recent years .
we use a fourgram language model with modified kneser-ney smoothing as implemented in the srilm toolkit .
transition-based and graph-based models have attracted the most attention of dependency parsing in recent years .
there are hand-crafted semantic frames in the lexicons of framenet and propbank .
for the language model we use the corpus of 60,000 simple english wikipedia articles 3 and build a 3-gram language model with kneser-ney smoothing trained with srilm .
richman and schone used english linguis-tic tools and cross language links in wikipedia to automatically annotate text in different languages .
we employ moses , an open-source toolkit for our experiment .
the language models used were 7-gram srilm with kneser-ney smoothing and linear interpolation .
on all datasets and models , we use 300-dimensional word vectors pre-trained on google news .
part-of-speech ( pos ) tagging is a fundamental task in natural language processing .
we build an open-vocabulary language model with kneser-ney smoothing using the srilm toolkit .
language models were built using the srilm toolkit 16 .
the cbow model introduced in mikolov et al learns vector representations using a neural network architecture by trying to predict a target word given the words surrounding it .
word embeddings have proven to be effective models of semantic representation of words in various nlp tasks .
we implement classification models using keras and scikit-learn .
the language model used was a 5-gram with modified kneserney smoothing , built with srilm toolkit .
to calculate language model features , we train traditional n-gram language models with ngram lengths of four and five using the srilm toolkit .
neural machine translation has recently gained popularity in solving the machine translation problem .
for training our system classifier , we have used scikit-learn .
we used the phrasebased translation system in moses 5 as a baseline smt system .
thus , event extraction is a difficult task and requires substantial training data .
riedel et al , 2010 ) made the at-least-once assumption that led the distant supervision for relation extraction to multi-instance learning .
aspect extraction is a key task of opinion mining ( cite-p-15-1-14 ) .
word alignment , which can be defined as an object for indicating the corresponding words in a parallel text , was first introduced as an intermediate result of statistical translation models ( cite-p-13-1-2 ) .
for the neural models , we use 100-dimensional glove embeddings , pre-trained on wikipedia and gigaword .
we use the pre-trained glove 50-dimensional word embeddings to represent words found in the glove dataset .
word sense disambiguation ( wsd ) is the task of determining the meaning of an ambiguous word in its context .
coreference resolution is the task of grouping mentions to entities .
in all cases , we used the implementations from the scikitlearn machine learning library .
we use srilm for training a trigram language model on the english side of the training data .
the srilm toolkit was used to build the trigram mkn smoothed language model .
we use word2vec as the vector representation of the words in tweets .
cussens and pulman used a symbolic approach employing inductive logic programming , while erbach , barg and walther and fouvry followed a unificationbased approach .
our baseline is a phrase-based mt system trained using the moses toolkit .
relation extraction ( re ) is a task of identifying typed relations between known entity mentions in a sentence .
coreference resolution is the task of determining which mentions in a text refer to the same entity .
in recent years , phrase-based systems for statistical machine translation have delivered state-of-the-art performance on standard translation tasks .
the srilm language modelling toolkit was used with interpolated kneser-ney discounting .
coreference resolution is the task of determining which mentions in a text are used to refer to the same real-world entity .
the approach relies on the assumption that the term and its translation appear in similar contexts .
we utilize the google news dataset created by mikolov et al , which consists of 300-dimensional vectors for 3 million words and phrases .
dependency parsing consists of finding the structure of a sentence as expressed by a set of directed links ( dependencies ) between words .
we use logistic regression with l2 regularization , implemented using the scikit-learn toolkit .
we use kaldi speech recognition toolkit to train our acoustic models .
coreference resolution is the process of determining whether two expressions in natural language refer to the same entity in the world .
zeng et al exploit a convolutional neural network to extract lexical and sentence level features for relation classification .
bilmes and kirchhoff proposed a more general framework for n-gram language modelling .
word alignment is the task of identifying word correspondences between parallel sentence pairs .
for feature building , we use word2vec pre-trained word embeddings .
we built a linear svm classifier using svm light package .
we used implementations from scikitlearn , and the parameters of both classifiers were tuned on the development set using grid search .
coreference resolution is a challenging task , that involves identification and clustering of noun phrases mentions that refer to the same real-world entity .
the translation quality is evaluated by case-insensitive bleu-4 .
in this paper , we propose a novel branch and bound ( b & b ) algorithm for efficient parsing .
in this paper , we present a novel method that enhances authorship attribution effectiveness .
we used the sri language modeling toolkit for this purpose .
more recently , mikolov et al showed that word vectors could be added or subtracted to isolate certain semantic and syntactic features .
we use the glove algorithm to obtain 300-dimensional word embeddings from a union of these corpora .
much current work in discourse parsing focuses on the labelling of discourse relations , using data from the penn discourse treebank .
we use pre-trained 100 dimensional glove word embeddings .
ngram features have been generated with the srilm toolkit .
barman et al , 2014 ) addressed the problem of language identification on bengali-hindi-english facebook comments .
if the anaphor is a definite noun phrase and the referent is in focus ( i.e . in the cache ) , anaphora resolution will be hindered .
to do this we examine the dataset created for the english lexical substitution task in semeval .
the language models in this experiment were trigram models with good-turing smoothing built using srilm .
we use srilm train a 5-gram language model on the xinhua portion of the english gigaword corpus 5th edition with modified kneser-ney discounting .
for support vector learning , we use svm-light and svm-multiclass .
for all baselines we used the phrase-based statistical machine translation system moses , with the default model features , weighted in a log-linear framework .
semantic parsing is the task of mapping natural language sentences to a formal representation of meaning .
the smt systems were built using the moses toolkit .
recently , the field has been influenced by the success of neural language models .
pun is a way of using the characteristics of the language to cause a word , a sentence or a discourse to involve two or more different meanings .
a 4-gram language model is trained on the monolingual data by srilm toolkit .
we use the adam optimizer with its default parameters and a mini-batch size of 32 .
word segmentation is the first step prior to word alignment for building statistical machine translations ( smt ) on language pairs without explicit word boundaries such as chinese-english .
we measure machine translation performance using the bleu metric .
we use pre-trained 50 dimensional glove vectors 4 for word embeddings initialization .
we also report the results using bleu and ter metrics .
we employ the glove and node2vec to generate the pre-trained word embedding , obtaining two distinct embedding for each word .
the english data representation was done using tokenizer 6 and glove pretrained word vectors .
to tackle this issue , we leverage pretrained word embeddings , specifically the 300 dimension glove embeddings trained on 42b tokens of external text corpora .
coreference resolution is the task of grouping mentions to entities .
to train the parsing models , while we use subtree-based features .
the english side of the parallel corpus is trained into a language model using srilm .
jtt is an lda-style model that is trained jointly on source and target documents linked by browsing transitions .
we use the earley algorithm with cube-pruning for the string-to-amr parsing .
we have implemented a hierarchical phrase-based smt model similar to chiang .
the model parameters of word embedding are initialized using word2vec .
our results also show that incorporating and exploiting more information from the target domain is much more useful for improving performance than excluding misleading training .
these models were implemented using the package scikit-learn .
the language models used were 7-gram srilm with kneser-ney smoothing and linear interpolation .
we used 300-dimensional pre-trained glove word embeddings .
the system used a tri-gram language model built from sri toolkit with modified kneser-ney interpolation smoothing technique .
word alignment is a critical component in training statistical machine translation systems and has received a significant amount of research , for example , ( cite-p-17-1-0 , cite-p-17-1-8 , cite-p-17-1-4 ) , including work leveraging syntactic parse trees , e.g. , ( cite-p-17-1-1 , cite-p-17-1-2 , cite-p-17-1-3 ) .
in particular , we created standard trigram language models from the written training data without making use of concurrent perceptual context information using srilm .
the language model used was a 5-gram with modified kneserney smoothing , built with srilm toolkit .
for each target language , we used the srilm toolkit to estimate separate 4-gram lms with kneser-ney smoothing , for each of the corpora listed in tables 3 , 4 and 5 .
turney and littman determined the semantic orientation of a target word t by comparing its association with two seed sets of manually crafted target words .
table 5 shows the bleu and per scores obtained by each system .
the language models used were 7-gram srilm with kneser-ney smoothing and linear interpolation .
the translation quality is evaluated by case-insensitive bleu-4 metric .
notable discriminative approaches are conditional random fields and structural svm .
we used the sri language modeling toolkit to train a fivegram model with modified kneser-ney smoothing .
translation performances are measured with case-insensitive bleu4 score .
we evaluate our models using the standard bleu metric 2 on the detokenized translations of the test set .
the bleu score measures the agreement between a hypothesis e i 1 generated by the mt system and a reference translation锚脦 1 .
we extract the named entities from the web pages using the stanford named entity recognizer .
for the classification task , we use pre-trained glove embedding vectors as lexical features .
sentiment analysis is the task of identifying the polarity ( positive , negative or neutral ) of review .
in this paper , we propose another phrase-level combination approach – a paraphrasing model .
we train a 4-gram language model on the xinhua portion of the gigaword corpus using the sri language toolkit with modified kneser-ney smoothing .
the word embeddings were obtained using word2vec 2 tool .
we built a 5-gram language model from it with the sri language modeling toolkit .
stance detection is the task of assigning stance labels to a piece of text with respect to a topic , i.e . whether a piece of text is in favour of “ abortion ” , neutral , or against .
semantic parsing is the task of translating natural language utterances into a machine-interpretable meaning representation ( mr ) .
we used the sri language modeling toolkit to train a fivegram model with modified kneser-ney smoothing .
language modeling is trained using kenlm using 5-grams , with modified kneser-ney smoothing .
we use an attention-augmented architecture with a bi-directional lstm as encoder .
language models are built using the sri-lm toolkit .
language models were estimated using the sri language modeling toolkit with modified kneser-ney smoothing .
we run skip-gram model on training dataset , and use the obtained word vector to initialize the word embedding part of model input .
sentiment analysis is a research area where does a computational analysis of people ’ s feelings or beliefs expressed in texts such as emotions , opinions , attitudes , appraisals , etc . ( cite-p-12-1-3 ) .
the language model pis implemented as an n-gram model using the srilm-toolkit with kneser-ney smoothing .
a pseudoword is a composite comprised of two or more words chosen at random ; the individual occurrences of the original words within a text are replaced by their conflation .
in all cases , we use a support vector machine approach to training the model , using the smo implementation found in weka , using a linear polynomial kernel and default settings .
lei et al introduce a syntactic dependency parser using a low-rank tensor component for scoring dependency edges .
passing additional information to a neural network via word-attached features was first introduced by collobert et al as a way to add linguistic annotation for various nlp tasks using feed-forward and convolutional networks .
a 4-gram language model was trained on the monolingual data by the srilm toolkit .
we used the srilm toolkit and kneser-ney discounting for estimating 5-grams lms .
coreference resolution is a fundamental component of natural language processing ( nlp ) and has been widely applied in other nlp tasks ( cite-p-15-3-9 ) .
we used srilm to build a 4-gram language model with kneser-ney discounting .
later , ji and grishman employed a rule-based approach to propagate consistent triggers and arguments across topic-related documents .
relation extraction is the task of detecting and classifying relationships between two entities from text .
such models have been very frequently used in question-answering tasks and lee et al , machine translation , and many other nlp applications .
sentiment analysis is a recent attempt to deal with evaluative aspects of text .
coreference resolution is the process of linking multiple mentions that refer to the same entity .
twitter is a social platform which contains rich textual content .
coreference resolution is the process of linking together multiple referring expressions of a given entity in the world .
sentence compression is a paraphrasing task aimed at generating sentences shorter than the given ones , while preserving the essential content .
and we pretrain the chinese word embeddings on a huge unlabeled data , the chinese wikipedia corpus , with word2vec toolkit .
despite being relatively simple , this baseline has been previously used as a point of comparison by other unsupervised semantic role labeling systems and shown difficult to outperform .
systems that jointly annotate syntactic and semantic dependencies were introduced in the past conll-2008 shared task .
unfortunately , wordnet is a fine-grained resource , encoding sense distinctions that are difficult to recognize even for human annotators ( cite-p-13-1-2 ) .
neural models , with various neural architectures , have recently achieved great success .
we applied liblinear via its scikitlearn python interface to train the logistic regression model with l2 regularization .
parameter optimization is performed with the diagonal variant of adagrad with minibatchs .
word sense disambiguation ( wsd ) is a natural language processing ( nlp ) task in which the correct meaning ( sense ) of a word in a given context is to be determined .
the model weights are automatically tuned using minimum error rate training .
convolutional neural networks are useful in many nlp tasks , such as language modeling , semantic role labeling and semantic parsing .
faruqui et al demonstrated that embeddings learned without supervision can be retro-fitted to better conform to some semantic lexicon .
we use an in-house implementation of a pbsmt system similar to moses .
in this and our other n-gram models , we used kneser-ney smoothing .
in this paper , we propose a novel hl-sot approach to labeling a product ’ s attributes and their associated sentiments in product reviews .
this is motivated by the fact that multi-task learning has shown to be beneficial in several nlp tasks .
in this work we use the open-source toolkit moses .
the pre-processed monolingual sentences will be used by srilm or berkeleylm to train a n-gram language model .
coreference resolution is the process of linking together multiple referring expressions of a given entity in the world .
n-gram features were based on language models of order 5 , built with the srilm toolkit on monolingual training material from the europarl and the news corpora .
at the document level , we find satirical news generally contain paragraphs which are more complex than true news .
we report bleu and ter on tokenized output , as computed by multeval .
table 4 shows the bleu scores of the output descriptions .
li et al recently proposed a joint detection method to detect both triggers and arguments using a structured perceptron model .
korhonen et al performed a clustering experiment with highly polysemous verbs .
we use scikit learn python machine learning library for implementing these models .
we use word vectors produced by the cbow approach-continuous bagof-words .
the probabilistic language model is constructed on google web 1t 5-gram corpus by using the srilm toolkit .
word segmentation is the first step prior to word alignment for building statistical machine translations ( smt ) on language pairs without explicit word boundaries such as chinese-english .
coreference resolution is the task of automatically grouping references to the same real-world entity in a document into a set .
we use the logistic regression implementation of liblinear wrapped by the scikit-learn library .
lda is a probabilistic model that can be used to model and discover underlying topic structures of documents .
we compute the syntactic features only for pairs of event mentions from the same sentence , using the stanford dependency parser .
we use srilm for training the 5-gram language model with interpolated modified kneser-ney discounting .
performance of a simple baseline model can be improved significantly if long-range dependencies are also captured .
in this paper , we propose a broad-coverage normalization system by integrating three human perspectives , including the enhanced letter .
an hierarchical phrase-based model is a powerful method to cover any format of translation pairs by using synchronous context free grammar .
we represent input words using pre-trained glove wikipedia 6b word embeddings .
we trained a support vector machine with rbf kernel per temporal span using scikit-learn and tuned svm parameters using 5-fold crossvalidation with the training set .
semantic parsing is a domain-dependent process by nature , as its output is defined over a set of domain symbols .
dependency parsing consists of finding the structure of a sentence as expressed by a set of directed links ( dependencies ) between words .
coreference resolution is the task of grouping mentions to entities .
as a further test , we ran the stanford parser on the queries to generate syntactic parse trees .
we use a shared subword vocabulary by applying byte-pair encoding to the data for all variants concatenated .
we use the adaptive gradients method for weight updates and averaging of the weight vector .
we use bleu 2 , ter 3 and meteor 4 , which are the most-widely used mt evaluation metrics .
the word representations are generated based on the co-occurrence count modeling using stanford glove tool .
the model weights are automatically tuned using minimum error rate training .
we used the srilm toolkit to create 5-gram language models with interpolated modified kneser-ney discounting .
twitter is a communication platform which combines sms , instant messages and social networks .
then , we trained word embeddings using word2vec .
although the itg constraint allows more flexible reordering during decoding , zens and ney showed that the ibm constraint results in higher bleu scores .
coreference resolution is the problem of partitioning a sequence of noun phrases ( or mentions ) , as they occur in a natural language text , into a set of referential entities .
snow et al demonstrated that annotations by crowdworkers have almost identical quality with those by experts in various nlp tasks .
semantic parsing is the mapping of text to a meaning representation .
we employ the glove and node2vec to generate the pre-trained word embedding , obtaining two distinct embedding for each word .
he et al proposed a method to find bursts , periods of elevated occurrence of events as a dynamic phenomenon instead of focusing on arrival rates .
sentiment analysis is a research area where does a computational analysis of people ’ s feelings or beliefs expressed in texts such as emotions , opinions , attitudes , appraisals , etc . ( cite-p-12-1-3 ) .
the state-of-the-art techniques of statistical machine translation demonstrate good performance on translation of languages with relatively similar word orders .
minimum error rate training under bleu criterion is used to estimate 20 feature function weights over the larger development set .
we train randomly initialized word embeddings of size 500 for the dialog model and use 300 dimentional glove embeddings for reranking classifiers .
the minimum error rate training was used to tune the feature weights .
we use word embeddings of dimension 100 pretrained using word2vec on the training dataset .
we use srilm for n-gram language model training and hmm decoding .
a modified kn model , termed p , was estimated on the training set count files and applied to the test set using srilm , the sri language modeling toolkit .
for all the experiments below , we utilize the pretrained word embeddings word2vec from mikolov et al to initialize the word embedding table .
we use the stanford named entity recognizer to identify named entities in s and t .
we used moses , a phrase-based smt toolkit , for training the translation model .
semantic role labeling ( srl ) is defined as the task to recognize arguments for a given predicate and assign semantic role labels to them .
coreference resolution is the task of determining whether two or more noun phrases refer to the same entity in a text .
we use pre-trained glove vector for initialization of word embeddings .
for decoding , we used moses with the default options .
taxonomies are widely used for knowledge standardization , knowledge sharing , and inferencing in natural language processing tasks .
mihalcea et al defines a measure of text semantic similarity and evaluates it in an unsupervised paraphrase detector on this data set .
hiero is a hierarchical system that expresses its translation model as a synchronous context-free grammar .
nenkova et al proposed a score to evaluate the lexical entrainment in highly frequent words , and found that the score has high correlation with task success and engagement .
for word embeddings , we used popular pre-trained word vectors from glove .
the rules are extracted from the trees generated by the stanford dependency parser for the candidate sentences of our corpora .
we implemented the different aes models using scikit-learn .
coreference resolution is a set partitioning problem in which each resulting partition refers to an entity .
word sense disambiguation ( wsd ) is the task of automatically determining the correct sense for a target word given the context in which it occurs .
bengio et al presented a neural network language model where word embeddings are simultaneously learned along with a language model .
the system used a tri-gram language model built from sri toolkit with modified kneser-ney interpolation smoothing technique .
for evaluating the effectiveness of our approach , we perform language modeling over penn treebak dataset .
we employ the glove and node2vec to generate the pre-trained word embedding , obtaining two distinct embedding for each word .
word sense disambiguation ( wsd ) is the task of assigning sense tags to ambiguous lexical items ( lis ) in a text .
a 4-gram language model was trained on the target side of the parallel data using the srilm toolkit .
the log-linear parameter weights are tuned with mert on a development set to produce the baseline system .
semantic role labeling ( srl ) is the task of identifying semantic arguments of predicates in text .
regarding svm we used linear kernels implemented in svm-light .
we use a count-based distributional semantics model and the continuous bag-of-words model to learn word vectors .
we extract the 4096-dimension full-connected layer of 19-layer vggnet as the vector representation of images .
pitler et al use several linguistically informed features , including polarity tags , levin verb classes and length of verb phrases .
twitter 1 is a microblogging service , which according to latest statistics , has 284 million active users , 77 % outside the us that generate 500 million tweets a day in 35 different languages .
phrases are extracted using standard phrase-based heuristics and used to build a translation table and lexicalized reordering model .
for example , turian et al used word embeddings as input features for several nlp systems , including a traditional chunking system based on conditional random fields .
we used the bleu score to evaluate the translation accuracy with and without the normalization .
we train a trigram language model with modified kneser-ney smoothing from the training dataset using the srilm toolkit , and use the same language model for all three systems .
the weights for these features are optimized using mert .
we used moses , a state-of-the-art phrase-based smt model , in decoding .
we use a popular word2vec neural language model to learn the word embeddings on an unsupervised tweet corpus .
semantic parsing is the task of translating text to a formal meaning representation such as logical forms or structured queries .
dependency relations have been extracted running the stanford parser .
we use minimum error rate training with nbest list size 100 to optimize the feature weights for maximum development bleu .
this paper described a simple pattern-matching algorithm for restoring empty nodes in parse trees that do not contain them , and appropriately .
the translation quality is evaluated by case-insensitive bleu and ter metric .
we use the k-best batch mira to tune mt systems .
the evaluation metric is casesensitive bleu-4 .
ji and grishman extended the scope from a single document to a cluster of topic-related documents and employed a rule-based approach to propagate consistent trigger classification and event arguments across sentences and documents .
previous work on the relation between dms and drs is mostly based on corpora annotated with drs , most notably the penn discourse treebank for english .
to keep consistent , we initialize the embedding weight with pre-trained word embeddings .
we use the wrapper of the scikit learn python library over the liblinear logistic regression implementation .
pdtb is drawn from wall street journal articles with overlapping annotations with the penn treebank .
semantic parsing is the task of converting natural language utterances into formal representations of their meaning .
with this induced model , we perform word alignment between languages l1 and l2 .
in clark and curran we investigate several log-linear parsing models for ccg .
this task specifically focuses on the identification of hypernym-hyponym relation among terms in four different languages .
we initialize the embedding layer using embeddings from dedicated word embedding techniques word2vec and glove .
word sense disambiguation ( wsd ) is the task of automatically determining the correct sense for a target word given the context in which it occurs .
as we know , document summarization is a very useful means for people to quickly read and browse news articles in the big data era .
we trained a 4-gram language model with kneser-ney smoothing and unigram caching using the sri-lm toolkit .
we used latent dirichlet allocation to construct our topics .
relation extraction is the key component for building relation knowledge graphs , and it is of crucial significance to natural language processing applications such as structured search , sentiment analysis , question answering , and summarization .
word sense disambiguation ( wsd ) is the problem of assigning a sense to an ambiguous word , using its context .
the experiment was set up and run using the scikit-learn machine learning library for python .
we built a 5-gram language model on the english side of europarl and used the kneser-ney smoothing method and srilm as the language model toolkit .
grammar induction is the task of learning grammatical structure from plain text without human supervision .
coreference resolution is the problem of identifying which noun phrases ( nps , or mentions ) refer to the same real-world entity in a text or dialogue .
for both languages , we used the srilm toolkit to train a 5-gram language model using all monolingual data provided .
in this research , we use the pre-trained google news dataset 2 by word2vec algorithms .
for all data sets , we trained a 5-gram language model using the sri language modeling toolkit .
we use a fourgram language model with modified kneser-ney smoothing as implemented in the srilm toolkit .
we report bleu scores computed using sacrebleu .
social media is a valuable source for studying health-related behaviors ( cite-p-11-1-8 ) .
phonetic translation across these pairs is called transliteration .
the feature weights are tuned to optimize bleu using the minimum error rate training algorithm .
therefore , dependency parsing is a potential “ sweet spot ” that deserves investigation .
high quality word embeddings have been proven helpful in many nlp tasks .
in this paper we presented a new corpus for context-dependent semantic parsing .
semantic parsing is the problem of mapping natural language strings into meaning representations .
for all machine learning results , we train a logistic regression classifier implemented in scikitlearn with l2 regularization and the liblinear solver .
word sense disambiguation ( wsd ) is the task of determining the correct meaning or sense of a word in context .
we measure machine translation performance using the bleu metric .
we trained the classifiers for relation extraction using l1-regularized logistic regression with default parameters using the liblinear package .
the smt weighting parameters were tuned by mert in the development data .
the first application of machine translation system combination used a consensus decoding strategy relying on a confusion network .
we used an l2-regularized l2-loss linear svm to learn the attribute predictions .
word sense disambiguation ( wsd ) is a key task in computational lexical semantics , inasmuch as it addresses the lexical ambiguity of text by making explicit the meaning of words occurring in a given context ( cite-p-18-3-10 ) .
we extend the perceptron training method of maaten et al to train a hucrf from partially labeled sequences .
coreference resolution is the task of determining which mentions in a text refer to the same entity .
semantic parsing is the task of mapping natural language to a formal meaning representation .
for probabilities , we trained 5-gram language models using srilm .
we have used the srilm with kneser-ney smoothing for training a language model of order five and mert for tuning the model with development data .
word sense disambiguation ( wsd ) is a difficult natural language processing task which requires that for every content word ( noun , adjective , verb or adverb ) the appropriate meaning is automatically selected from the available sense inventory 1 .
additionally , for en-de , compound splitting of the german side of the corpus was performed using a frequency based method described in .
in this paper , we propose a method based on importance sampling that allows us to use a very large target vocabulary .
we experiment with a machine learning strategy to model multilingual coreference for the conll-2012 shared task .
we evaluated the reordering approach within the moses phrase-based smt system .
word embeddings have proven to be effective models of semantic representation of words in various nlp tasks .
for probabilities , we trained 5-gram language models using srilm .
event coreference resolution is the task of identifying event mentions and clustering them such that each cluster represents a unique real world event .
in practical terms , we will use a paraphrase ranking task derived from the semeval 2007 lexical substitution task .
word embeddings have also been effectively employed in several tasks such as named entity recognition , adjectival scales and text classification .
for the automatic evaluation , we used the bleu metric from ibm .
we use the scikit-learn machine learning library to implement the entire pipeline .
this approach relies on word embeddings for the computation of semantic relatedness with word2vec .
the evaluation metric is casesensitive bleu-4 .
qiu et al proposed double propagation to collectively extract aspect terms and opinion words based on information propagation over a dependency graph .
we experimentally evaluate the effectiveness of multiple importance sampling distributions .
the basic building blocks of our models are recurrent neural networks with long short-term memory units .
for the machine learning component of our system we use the l2-regularised logistic regression implementation of the liblinear 3 software library .
we trained a 5-gram language model on the english side of each training corpus using the sri language modeling toolkit .
the srilm toolkit was used to build the trigram mkn smoothed language model .
relation extraction is the task of finding relations between entities in text , which is useful for several tasks such as information extraction , summarization , and question answering ( cite-p-14-3-7 ) .
translation performances are measured with case-insensitive bleu4 score .
in this paper , we study the impact of persuasive argumentation in political debates .
we then learn reranking weights using minimum error rate training on the development set for this combined list , using only these two features .
based on hypothesis 1 , we learn sense-based embeddings from a large data set , using the continuous skip-gram model .
named entity recognition ( ner ) is a challenging learning problem .
examples are yago , dbpedia , and freebase .
ner is a task to identify names in texts and to assign names with particular types ( cite-p-12-3-17 , cite-p-12-3-19 , cite-p-12-3-18 , cite-p-12-3-2 ) .
the target language model was a trigram language model with modified kneser-ney smoothing trained on the english side of the bitext using the srilm tookit .
we used the scikit-learn implementation of svrs and the skll toolkit .
we use the moses smt toolkit to test the augmented datasets .
semi-supervised learning is a type of machine learning where one has access to a small amount of labeled data and a large amount of unlabeled data .
semantic role labeling ( srl ) is the process of producing such a markup .
we use the svm implementation from scikit-learn , which in turn is based on libsvm .
we apply sri language modeling toolkit to train a 4-gram language model with kneser-ney smoothing .
the probabilistic language model is constructed on google web 1t 5-gram corpus by using the srilm toolkit .
barzilay and mckeown and callisonburch et al extracted paraphrases from monolingual parallel corpus where multiple translations were present for the same source .
training approach can improve the robustness of nmt models .
for this language model , we built a trigram language model with kneser-ney smoothing using srilm from the same automatically segmented corpus .
we train a linear support vector machine classifier using the efficient liblinear package .
we used the implementation of the scikit-learn 2 module .
in order to measure translation quality , we use bleu 7 and ter scores .
our holing system uses collapsed stanford parser dependencies as context features .
kondrak and dorr reported that a simple average of several orthographic similarity measures outperformed all the measures on the task of the identification of cognates for drug names .
we begin by computing the similarity between words using word embeddings .
wikipedia is a resource of choice exploited in many nlp applications , yet we are not aware of recent attempts to adapt coreference resolution to this resource .
we used the moses toolkit for performing statistical machine translation .
the n-gram models are created using the srilm toolkit with good-turning smoothing for both the chinese and english data .
sentiment classification is the task of classifying an opinion document as expressing a positive or negative sentiment .
relation extraction ( re ) is the process of generating structured relation knowledge from unstructured natural language texts .
as a classifier , we choose a first-order conditional random field model .
we implement logistic regression with scikit-learn and use the lbfgs solver .
semantic role labeling ( srl ) is defined as the task to recognize arguments for a given predicate and assign semantic role labels to them .
we use the glove pre-trained word embeddings for the vectors of the content words .
luong et al segment words using morfessor , and use recursive neural networks to build word embeddings from morph embeddings .
then , we trained word embeddings using word2vec .
word embeddings such as word2vec and glove have been widely recognized for their ability to capture linguistic regularities .
semantic role labeling ( srl ) is the process of producing such a markup .
we optimized each system separately using minimum error rate training .
sentence vectors were generated using doc2vec .
domain adaptation is a common concern when optimizing empirical nlp applications .
we use crf to learn the correlations between the current label and its neighbors .
additionally , a back-off 2-gram model with goodturing discounting and no lexical classes was built from the same training data , using the srilm toolkit .
in this research , we use the pre-trained google news dataset 2 by word2vec algorithms .
we apply linear regression with elastic net regularization and support vector regression with an rbf kernel for comparison .
we have used the srilm with kneser-ney smoothing for training a language model for the first stage of decoding .
we parse the corpus using the stanford dependency parser and extract the main verb of each segment .
we used the sri language modeling toolkit to train a fivegram model with modified kneser-ney smoothing .
we use distributed word vectors trained on the wikipedia corpus using the word2vec algorithm .
for all models , we use the 300-dimensional glove word embeddings .
in this paper , we explore an implicit content-introducing method for generative short-text conversation .
language modeling is a fundamental task , used for example to predict the next word or character in a text sequence given the context .
long short-term memory network was proposed by to specifically address this issue of learning longterm dependencies .
to set the weights , 位 m , we performed minimum error rate training on the development set using bleu as the objective function .
the target-side language models were estimated using the srilm toolkit .
to calculate language model features , we train traditional n-gram language models with ngram lengths of four and five using the srilm toolkit .
luong et al utilized the morpheme segments produced by morfessor and constructed morpheme trees for words to learn morphologically-aware word embeddings by the recursive neural network .
we trained the statistical phrase-based systems using the moses toolkit with mert tuning .
semantic role labeling ( srl ) is the task of labeling predicate-argument structure in sentences with shallow semantic information .
transliteration is the task of converting a word from one alphabetic script to another .
evaluation sets are translated using the cdec decoder and evaluated with the bleu metric .
although coreference resolution is a subproblem of natural language understanding , coreference resolution evaluation metrics have predominately been discussed in terms of abstract entities and hypothetical system errors .
keyphrase extraction is a fundamental task in natural language processing that facilitates mapping of documents to a set of representative phrases .
distributional semantic models induce large-scale vector-based lexical semantic representations from statistical patterns of word usage .
relation extraction ( re ) is a task of identifying typed relations between known entity mentions in a sentence .
part-of-speech ( pos ) tagging is a crucial task for natural language processing ( nlp ) tasks , providing basic information about syntax .
the srilm toolkit was used to build the 5-gram language model .
we estimated 5-gram language models using the sri toolkit with modified kneser-ney smoothing .
we use stanford part-of-speech tagger to automatically detect nouns from text .
feature weights are tuned using minimum error rate training on the 455 provided references .
semantic role labeling ( srl ) is a form of shallow semantic parsing whose goal is to discover the predicate-argument structure of each predicate in a given input sentence .
coreference resolution is the problem of identifying which mentions ( i.e. , noun phrases ) refer to which real-world entities .
we measure the overall translation quality using 4-gram bleu , which is computed on tokenized and lowercased data for all systems .
1 bunsetsu is a linguistic unit in japanese that roughly corresponds to a basic phrase in english .
we train a 4-gram language model on the xinhua portion of the gigaword corpus using the sri language toolkit with modified kneser-ney smoothing .
relation extraction is the task of finding semantic relations between entities from text .
we use a count-based distributional semantics model and the continuous bag-of-words model to learn word vectors .
relation extraction is a traditional information extraction task which aims at detecting and classifying semantic relations between entities in text ( cite-p-10-1-18 ) .
we pretrain word vectors with the word2vec tool on the news dataset released by ding et al , which are fine-tuned during training .
the srilm toolkit is used to build the character-level language model for generating the lm features in nsw detection system .
we will show translation quality measured with the bleu score as a function of the phrase table size .
one of the first challenges in sentiment analysis is the vast lexical diversity of subjective language .
coreference resolution is the process of finding discourse entities ( markables ) referring to the same real-world entity or concept .
sarcasm is a pervasive phenomenon in social media , permitting the concise communication of meaning , affect and attitude .
the database of typological features we used is the online edition 8 of the world atlas of language structures .
recently , to reduce labeling effort for relation extraction , distant supervision has been proposed .
as classifier we use a traditional model , a support vector machine with linear kernel implemented in scikit-learn .
information extraction ( ie ) is a main nlp aspects for analyzing scientific papers , which includes named entity recognition ( ner ) and relation extraction ( re ) .
we considered one layer and used the adam optimizer for parameter optimization .
coreference resolution is the task of automatically grouping references to the same real-world entity in a document into a set .
we use the stanford ner to identify named entities in our corpus , and then use these entities as bag-of-features .
the word embeddings are initialized with pre-trained word vectors using word2vec 2 and other parameters are randomly initialized including pos embeddings .
the weights are learned automatically using expectation maximization .
sentiment analysis is a nlp task that deals with extraction of opinion from a piece of text on a topic .
in arabic , there is a reasonable number of sentiment lexicons but with major deficiencies .
dependency parsing is a central nlp task .
for the first two features , we adopt a set of pre-trained word embedding , known as global vectors for word representation .
the case insensitive nist bleu-4 metric is adopted for evaluation .
based on word2vec , we obtained both representations using the skipgram architecture with negative sampling .
since coreference resolution is a pervasive discourse phenomenon causing performance impediments in current ie systems , we considered a corpus of aligned english and romanian texts to identify coreferring expressions .
sentiment analysis is the computational analysis of people ’ s feelings or beliefs expressed in texts such as emotions , opinions , attitudes , appraisals , etc . ( cite-p-11-3-3 ) .
sentiment analysis is the task of identifying the polarity ( positive , negative or neutral ) of review .
we use the pre-trained 300-dimensional word2vec embeddings trained on google news 1 as input features .
the translation quality is evaluated by caseinsensitive bleu-4 metric .
according to lakoff and johnson , metaphors are cognitive mappings of concepts from a source to a target domain .
for this , we used the combination of the entire swedish-english europarl corpus and the smultron data .
word sense disambiguation ( wsd ) is a key enabling-technology that automatically chooses the intended sense of a word in context .
barzilay and lapata propose an entity grid model which represents the distribution of referents in a discourse for sentence ordering .
our baseline is the smt toolkit moses run over letter strings rather than word strings .
for word embeddings , we used popular pre-trained word vectors from glove .
as an early work , li et al used maximum mutual information as the objective to penalize general responses .
we used the svm implementation of scikit learn .
amr is a semantic formalism , structured as a graph ( cite-p-13-1-1 ) .
for the english sts subtask used regression models that combined a wide array of features including semantic similarity scores obtained with various methods .
we use the mallet implementation of conditional random fields .
in this task , we use the 300-dimensional 840b glove word embeddings .
the results evaluated by bleu score is shown in table 2 .
for training our system classifier , we have used scikit-learn .
word sense disambiguation ( wsd ) is a key enabling technology that automatically chooses the intended sense of a word in context .
for the word-embedding based classifier , we use the glove pre-trained word embeddings .
for all languages in our dataset , we used treetagger with its built-in lemmatiser .
as classifier we use a traditional model , a support vector machine with linear kernel implemented in scikit-learn .
for language model , we used sri language modeling toolkit to train a 4-gram model with modified kneser-ney smoothing .
phrase-based statistical machine translation models have achieved significant improvements in translation accuracy over the original ibm word-based model .
we use srilm to train a 5-gram language model on the xinhua portion of the english gigaword corpus 5th edition with modified kneser-ney discounting .
further , we apply a 4-gram language model trained with the srilm toolkit on the target side of the training corpus .
we use sri language model toolkit to train a 5-gram model with modified kneser-ney smoothing on the target-side training corpus .
in recent years , various phrase translation approaches have been shown to outperform word-to-word translation models .
we use the skipgram model to learn word embeddings .
we used minimum error rate training mert for tuning the feature weights .
in renew , we exploit the stanford typed dependency representations that use triples to formalize dependency relations .
we trained a 3-gram language model on all the correct-side sentences using kenlm .
our baseline system is an standard phrase-based smt system built with moses .
we use three common evaluation metrics including bleu , me-teor , and ter .
one of the very few available discourse annotated corpora is the penn discourse treebank in english .
named entity ( ne ) transliteration is the process of transcribing a ne from a source language to some target language based on phonetic similarity between the entities .
we used latent dirichlet allocation to create these topics .
we use the moses mt framework to build a standard statistical phrase-based mt model using our old-domain training data .
we used the case-insensitive bleu-4 to evaluate translation quality and run mert three times .
language models of order 5 have been built and interpolated with srilm and kenlm .
we initialize our word representation using publicly available word2vec trained on google news dataset and keep them fixed during training .
we use a random forest classifier , as implemented in scikit-learn .
semantic parsing is the task of converting natural language utterances into formal representations of their meaning .
for nb and svm , we used their implementation available in scikit-learn .
sentiment analysis ( sa ) is the task of prediction of opinion in text .
coreference resolution is the task of clustering a sequence of textual entity mentions into a set of maximal non-overlapping clusters , such that mentions in a cluster refer to the same discourse entity .
relation extraction ( re ) is the task of recognizing relationships between entities mentioned in text .
we used a caseless parsing model of the stanford parser for a dependency representation of the messages .
the penn discourse treebank is another annotated discourse corpus .
entity linking ( el ) is a central task in information extraction — given a textual passage , identify entity mentions ( substrings corresponding to world entities ) and link them to the corresponding entry in a given knowledge base ( kb , e.g . wikipedia or freebase ) .
for word-level embedding e w , we utilize pre-trained , 300-dimensional embedding vectors from glove 6b .
text classification is a crucial and well-proven method for organizing the collection of large scale documents .
coreference resolution is the task of clustering a set of mentions in the text such that all mentions in the same cluster refer to the same entity .
we evaluate our results with case-sensitive bleu-4 metric .
llu铆s et al introduced a dual decomposition based joint model for joint syntactic and semantic parsing .
discourse parsing is a challenging task and plays a critical role in discourse analysis .
relation extraction is a subtask of information extraction that finds various predefined semantic relations , such as location , affiliation , rival , etc. , between pairs of entities in text .
svms have been shown to be robust in classification tasks involving text where the dimensionality is high .
to train our models , we utilized the standard machine learning package , scikit-learn for the models using a shallow feature representation .
mikolov et al observed a strong similarity of the geometric arrangements of corresponding concepts between the vector spaces of different languages , and suggested that a crosslingual mapping between the two vector spaces is technically plausible .
we extract lexical relations from the question using the stanford dependencies parser .
choi and cardie combine different kinds of negations with lexical polarity items through various compositional semantic models to improve phrasal sentiment analysis .
we use the moses smt toolkit to test the augmented datasets .
we apply a pretrained glove word embedding on .
and we will show that this framework captures many existing topic models ( ¡ì 4 ) .
sarcasm is defined as ‘ a cutting , often ironic remark intended to express contempt or ridicule ’ 1 .
semantic role labeling ( srl ) is the task of identifying semantic arguments of predicates in text .
coreference resolution is a fundamental component of natural language processing ( nlp ) and has been widely applied in other nlp tasks ( cite-p-15-3-9 ) .
for all data sets , we trained a 5-gram language model using the sri language modeling toolkit .
we implement some of these features using the stanford parser .
we measured translation performance with bleu .
sentiment classification is a very domain-specific problem ; training a classifier using the data from one domain may fail when testing against data from another .
the target-side language models were estimated using the srilm toolkit .
we use srilm toolkit to build a 5-gram language model with modified kneser-ney smoothing .
more recently , neural networks have become prominent in word representation learning .
word sense disambiguation ( wsd ) is the task of automatically determining the correct sense for a target word given the context in which it occurs .
we use srilm toolkit to build a 5-gram language model with modified kneser-ney smoothing .
we trained a trigram language model on the chinese side , with the srilm toolkit , using the modified kneser-ney smoothing option .
we used the moses toolkit with its default settings .
in this work , we introduce an extension to the continuous bag-of-words model .
the nnlm weights are optimized as the other feature weights using minimum error rate training .
we used the scikit-learn library the svm model .
we train a 4-gram language model on the xinhua portion of english gigaword corpus by srilm toolkit .
semantic parsing is the mapping of text to a meaning representation .
we conduct experiments on the benchmark twitter sentiment classification dataset from semeval 2013 .
discourse parsing is a challenging natural language processing ( nlp ) task that has utility for many other nlp tasks such as summarization , opinion mining , etc . ( cite-p-17-3-3 ) .
the model weights are automatically tuned using minimum error rate training .
coreference resolution is a key problem in natural language understanding that still escapes reliable solutions .
a 5-gram language model was created with the sri language modeling toolkit and trained using the gigaword corpus and english sentences from the parallel data .
brockett et al showed that phrase-based statistical mt can help to correct mistakes made on mass nouns .
we trained a support vector machine for regression with rbf kernel using scikitlearn , which in turn uses libsvm .
sentiment analysis is a natural language processing task whose aim is to classify documents according to the opinion ( polarity ) they express on a given subject ( cite-p-13-8-14 ) .
coreference resolution is the process of determining whether two expressions in natural language refer to the same entity in the world .
we adopt the brown cluster algorithm to find the word cluster .
our experimental results demonstrate both that our proposed approach is useful in predicting missing preferences of users .
the smt systems were built using the moses toolkit .
stance detection is the task of automatically determining from the text whether the author of the text is in favor of , against , or neutral towards a proposition or target .
we will show translation quality measured with the bleu score as a function of the phrase table size .
we train and evaluate a l2-regularized logistic regression classifier with the liblin-ear solver as implemented in scikit-learn .
for unsupervised baselines we use morfessor categories-map and undivide .
relation extraction ( re ) is the task of recognizing relationships between entities mentioned in text .
we used word2vec to preinitialize the word embeddings .
relation extraction is the task of predicting semantic relations over entities expressed in structured or semi-structured text .
in smt , we propose a coverage-based approach to nmt .
our ncpg system is an attention-based bidirectional rnn architecture that uses an encoder-decoder framework .
as a case study , we explore the task of learning to solve geometry problems .
luong and manning propose training a model on an out-of-domain corpus and do finetuning with small sized in-domain parallel data to mitigate the domain shift problem .
we set all feature weights by optimizing bleu directly using minimum error rate training on the tuning part of the development set .
the log-lineal combination weights were optimized using mert .
a multiword expression is any combination of words with lexical , syntactic or semantic idiosyncrasy , in that the properties of the mwe are not predictable from the component words .
in , the authors use a recursive neural network to explicitly model the morphological structures of words and learn morphologically-aware embeddings .
we pre-trained embeddings using word2vec with the skip-gram training objective and nec negative sampling .
marcu and wong present a joint probability model for phrase-based translation .
sequence labeling is the simplest subclass of structured prediction problems .
additionally , a back-off 2-gram model with goodturing discounting and no lexical classes was built from the same training data , using the srilm toolkit .
we use sri language modeling toolkit to train a 5-gram language model on the english sentences of fbis corpus .
in recent years , many researchers have employed statistical models or association measures to build alignment links .
for training our system classifier , we have used scikit-learn .
conditional random fields are discriminative structured classification models for sequential tagging and segmentation .
twitter is a microblogging service that has 313 million monthly active users 1 .
we assume that a morphological analysis consists of three processes : tokenization , dictionary lookup , and disambiguation .
we then perform mert which optimizes parameter settings using the bleu metric , while a 5-gram language model is derived with kneser-ney smoothing trained using srilm .
we use the multi-class logistic regression classifier from the liblinear package 2 for the prediction of edit scripts .
we trained a standard 5-gram language model with modified kneser-ney smoothing using the kenlm toolkit on 4 billion running words .
we use srilm toolkit to train a trigram language model with modified kneser-ney smoothing on the target side of training corpus .
we used the sri language modeling toolkit to train a fivegram model with modified kneser-ney smoothing .
automatic word alignment can be defined as the problem of determining a translational correspondence at word level given a parallel corpus of aligned sentences .
we report the mt performance using the original bleu metric .
we train a 4-gram language model on the xinhua portion of the gigaword corpus using the sri language toolkit with modified kneser-ney smoothing .
we evaluate the translation quality using the case-insensitive bleu-4 metric .
a trigram language model with modified kneser-ney discounting and interpolation was used as produced by the srilm toolkit .
we develop translation models using the phrase-based moses smt system .
semantic parsing is the problem of mapping natural language strings into meaning representations .
we adopt glove vectors as the initial setting of word embeddings v .
in our implementation , we use a kn-smoothed trigram model .
we used srilm to build a 4-gram language model with kneser-ney discounting .
we trained a 5-grams language model by the srilm toolkit .
the process of identifying the correct meaning , or sense of a word in context , is known as word sense disambiguation ( wsd ) .
coreference resolution is the process of finding discourse entities ( markables ) referring to the same real-world entity or concept .
we present an expressive entity-mention model that performs coreference resolution .
relation extraction ( re ) has been defined as the task of identifying a given set of semantic binary relations in text .
hatzivassiloglou and mckeown proposed a method to identify the polarity of adjectives based on conjunctions linking them .
word sense disambiguation ( wsd ) is a natural language processing ( nlp ) task in which the correct meaning ( sense ) of a word in a given context is to be determined .
mikolov et al have proposed to obtain cross-lingual word representations by learning a linear mapping between two monolingual word embedding spaces .
we measure the quality of the automatically created summaries using the rouge measure .
in previous work , hatzivassiloglou and mckeown proposed a method to identify the polarity of adjectives based on conjunctions linking them in a large corpus .
the skip-gram and continuous bag-of-words models of mikolov et al propose a simple single-layer architecture based on the inner product between two word vectors .
recently , neural networks , and in particular recurrent neural networks have shown excellent performance in language modeling .
the minimum error rate training was used to tune the feature weights .
extractive summarization is a task to create summaries by pulling out snippets of text form the original text and combining them to form a summary .
dreyer and eisner propose a dirichlet process mixture model to learn paradigms .
coreference resolution is the process of determining whether two expressions in natural language refer to the same entity in the world .
we use an attention-based bidirectional rnn architecture with an encoder-decoder framework to build our ncpg models .
we also use an in-house implementation of a japanese chunker to obtain chunks in japanese sentences .
wordnet is a general english thesaurus which additionally covers biological terms .
trigram language models were estimated using the sri language modeling toolkit with modified kneser-ney smoothing .
the lstm word embeddings are initialized with 100-dim embeddings from glove and fine-tuned during training .
minimum error training under bleu was used to optimise the feature weights of the decoder with respect to the dev2006 development set .
these weights are optimized using minimum error-rate training on a held-out 500 sentence-pair development set for each of the experiments .
sentiment analysis ( sa ) is the task of analysing opinions , sentiments or emotions expressed towards entities such as products , services , organisations , issues , and the various attributes of these entities ( cite-p-9-3-3 ) .
since chinese is the dominant language in our data set , a word-by-word statistical machine translation strategy ( cite-p-14-1-22 ) is adopted to translate english words into chinese .
we report the mt performance using the original bleu metric .
coreference resolution is the task of clustering a sequence of textual entity mentions into a set of maximal non-overlapping clusters , such that mentions in a cluster refer to the same discourse entity .
we evaluate the performance of different translation models using both bleu and ter metrics .
mihalcea et al learn multilingual subjectivity via cross-lingual projections .
a 4-gram language model was trained on the target side of the parallel data using the srilm toolkit .
we ¡¯ ve demonstrated that the benefits of unsupervised multilingual learning increase steadily with the number of available languages .
we first use the popular toolkit word2vec 1 provided by mikolov et al to train our word embeddings .
experimental results show that our algorithm can find important feature subset , estimate model order ( cluster number ) and achieve better performance .
for word embeddings , we used popular pre-trained word vectors from glove .
within this subpart of our ensemble model , we used a svm model from the scikit-learn library .
we trained a 3-gram language model on all the correct-side sentences using kenlm .
our baseline russian-english system is a hierarchical phrase-based translation model as implemented in cdec .
in this paper , we will improve upon collins ’ algorithm by introducing a bidirectional searching strategy , so as to effectively utilize more context information .
sentiment analysis is a growing research field , especially on web social networks .
we train a word embedding using word2vec over a large corpus of 55 , 463 product reviews .
social media is a natural place to discover new events missed by curation , but mentioned online by someone planning to attend .
for language modeling , we use the english gigaword corpus with 5-gram lm implemented with the kenlm toolkit .
we train a 4-gram language model on the xinhua portion of english gigaword corpus by srilm toolkit .
we use the mstparser implementation described in mcdonald et al for feature extraction .
previous work consistently reported that word-based translation models yielded better performance than traditional methods for question retrieval .
huang et al train their vectors with a neural network and additionally take global context into account .
we use glove pre-trained word embeddings , a 100 dimension embedding layer that is followed by a bilstm layer of size 32 .
with this data , we can investigate whether the relationship between personal traits and brand preferences .
semantic similarity is a central concept that extends across numerous fields such as artificial intelligence , natural language processing , cognitive science and psychology .
we used the logistic regression implementation in scikit-learn for the maximum entropy models in our experiments .
furthermore , the concept of word embedding introduced by mikolov et al allows for words to have vector representations , such that syntactic and semantic similarities are embodied in the vector space .
word segmentation is a fundamental task for processing most east asian languages , typically chinese .
we measure translation performance by the bleu and meteor scores with multiple translation references .
hearst proposed a lexico-syntactic pattern based method for automatic acquisition of hyponymy from unrestricted texts .
we use the adam optimizer for the gradient-based optimization .
in our experiments , we used the srilm toolkit to build 5-gram language model using the ldc arabic gigaword corpus .
therefore , we employ negative sampling and adam to optimize the overall objective function .
language models were built using the srilm toolkit 16 .
we used the srilm toolkit to create 5-gram language models with interpolated modified kneser-ney discounting .
semantic role labeling was first defined in gildea and jurafsky .
we used the stanford corenlp toolkit for word segmentation , part-of-speech tagging , and syntactic parsing .
our method of learning multilingual word vectors is most closely associated to zou et al who learn bilingual word embeddings and show their utility in machine translation .
recently , mikolov et al introduced an efficient way for inferring word embeddings that are effective in capturing syntactic and semantic relationships in natural language .
definition is most effective and is able to significantly and consistently improve retrieval performance .
we extend the model by adding continuous word representations , induced from the unlabeled data using the skip-gram algorithm , to the feature representations .
our smt-based query expansion techniques are based on a recent implementation of the phrasebased smt framework .
mikolov et al introduced the skip-gram architecture built on a single hidden layer neural network to learn efficiently a vector representation for each word w of a vocabulary v from a large corpora of size c .
framenet is a lexico-semantic resource focused on semantic frames .
dependency parsing is the task to assign dependency structures to a given sentence math-w-4-1-0-14 .
in our experiments , we used the implementation of l2-regularised logistic regression in fan et al as our local classifier .
for this , we used the combination of the entire swedish-english europarl corpus and the smultron data .
coreference resolution is the process of determining whether two expressions in natural language refer to the same entity in the world .
we evaluate the output of our generation system against the raw strings of section 23 using the simple string accuracy and bleu evaluation metrics .
word sense disambiguation is the task of assigning sense labels to occurrences of an ambiguous word .
pang and lee frame the problem of detecting subjective sentences as finding the minimum cut in a graph representation of the sentences .
for the newsgroups and sentiment datasets , we used stopwords from the nltk python package .
semantic parsing is the task of automatically translating natural language text to formal meaning representations ( e.g. , statements in a formal logic ) .
caseinsensitive nist bleu is used to measure translation performance .
each candidate property ’ s compatibility with the complementary simile component .
we have established a grouping-based ordering scheme to accommodate both local and global coherence .
we used moses as the implementation of the baseline smt systems .
we also obtain the embeddings of each word from word2vec .
we use the logistic regression classifier in the skll package , which is based on scikit-learn , optimizing for f 1 score .
we present the first approach for applying distant supervision to cross-sentence relation extraction .
we pre-trained word embeddings using word2vec over tweet text of the full training data .
we perform our translation experiments using an in-house state-of-the-art phrase-based smt system similar to moses .
we use the logistic regression classifier as implemented in the skll package , which is based on scikitlearn , with f1 optimization .
relation extraction ( re ) is the task of recognizing the assertion of a particular relationship between two or more entities in text .
in this paper , we propose an algorithmic approach for training a word problem solver based on both explicit and implicit supervision .
for support vector learning , we use svm-light and svm-multiclass .
for this task , we use the widely-used bleu metric .
the model weights were trained using the minimum error rate training algorithm .
in addition to these two key indicators , we evaluated the translation quality using an automatic measure , namely bleu score .
vaswani et al proposed the transformer as an alternative model to the rnn .
the comparison was done in terms of bleu and processing times .
we employ widely used and standard machine translation tool moses to train the phrasebased smt system .
gabrilovich and markovitch introduced the esa model in which wikipedia and open directory project 1 was used to obtain the explicit concepts .
because the parser is incremental , it should be well suited to unsegmented text .
semantic parsing is the mapping of text to a meaning representation .
we used svm classifier that implements linearsvc from the scikit-learn library .
the second decoding method is to use conditional random field .
the decoder uses a cky-style parsing algorithm to integrate the language model scores .
we use the wsj portion of the penn treebank 4 , augmented with head-dependant information using the rules of yamada and matsumoto .
the skip-gram model aims to find word representations that are useful for predicting the surrounding words in a sentence or document .
we evaluate the performance of different translation models using both bleu and ter metrics .
here , we extract unigram and bigram features and use them in a logistic regression classifier with elastic net regularization .
the language models are 4-grams with modified kneser-ney smoothing which have been trained with the srilm toolkit .
gu et al combined a copying mechanism with the seq2seq framework to improve the quality of the generated summaries .
we use the logistic regression implementation of liblinear wrapped by the scikit-learn library .
yarowsky has used a few seeds and untagged sentences in a bootstrapping algorithm based on decision lists .
we use the stanford dependency parser to extract nouns and their grammatical roles .
gamon et al and gamon use a combination of classification and language modeling .
we use glove vectors with 100 dimensions trained on wikipedia and gigaword as word embeddings .
dependency parsing consists of finding the structure of a sentence as expressed by a set of directed links ( dependencies ) between words .
we use srilm toolkit to build a 5-gram language model with modified kneser-ney smoothing .
recently socher et al introduced compositional vector grammar to address the above limitations .
semantic role labeling ( srl ) is the task of identifying the predicate-argument structure of a sentence .
we built a 5-gram language model on the english side of europarl and used the kneser-ney smoothing method and srilm as the language model toolkit .
as with our original refined language model , we estimate each coarse language model using the srilm toolkit .
koo et al used a clustering algorithm to produce word clusters on a large amount of unannotated data and represented new features based on the clusters for dependency parsing models .
all the feature weights and the weight for each probability factor are tuned on the development set with minimum-error-rate training .
coreference resolution is the next step on the way towards discourse understanding .
named entity recognition ( ner ) is the first step for many tasks in the fields of natural language processing and information retrieval .
berger and lafferty , 1999 , proposed a translation model that expands the document model .
the translation quality is evaluated by caseinsensitive bleu-4 metric .
coreference resolution is the task of grouping mentions to entities .
our nnape model is inspired by the mt work of bahdanau et al which is based on bidirectional recurrent neural networks .
a 4-gram language model is trained on the monolingual data by srilm toolkit .
named entity recognition ( ner ) is the task of detecting named entity mentions in text and assigning them to their corresponding type .
we update the model parameters by minimizing l c and l k with adam optimizer .
it has been shown in previous work that word pairs are effective for identifying implicit discourse relations .
since the bleu scores we obtained are close , we did a significance test on the scores .
we use the 100-dimensional pre-trained word embeddings trained by word2vec 2 and the 100-dimensional randomly initialized pos tag embeddings .
semantic parsing is the task of mapping natural language sentences to a formal representation of meaning .
we utilize minimum error rate training to optimize feature weights of the paraphrasing model according to ndcg .
srilm toolkit was used to create up to 5-gram language models using the mentioned resources .
a 5-gram language model was built using srilm on the target side of the corresponding training corpus .
we use 300-dimensional word embeddings from glove to initialize the model .
we trained a 4-gram language model with kneser-ney smoothing and unigram caching using the sri-lm toolkit .
our mt decoder is a proprietary engine similar to moses .
semantic parsing is the task of translating natural language utterances into a machine-interpretable meaning representation ( mr ) .
relation extraction is a fundamental task in information extraction .
to compute statistical significance , we use the approximate randomization test .
the universal dependencies project has produced a languageindependent but extensible standard for morphological and syntactic annotation using a formalism based on dependency grammar .
in this paper , we propose an approach to solve a significant problem : how to learn distinguishable representations from word sequences .
table 2 shows size of the inferred mdl-based pb models , and bleu score of their translations of the tune and test partitions .
zhou et al further extend it to context-sensitive shortest pathenclosed tree , which includes necessary predicate-linked path information .
stance detection is the task of automatically determining from text whether the author is in favor of the given target , against the given target , or whether neither inference is likely .
relation extraction ( re ) is the task of identifying instances of relations , such as nationality ( person , country ) or place of birth ( person , location ) , in passages of natural text .
the srilm toolkit was used for training the language models using kneser-ney smoothing .
collobert et al used word embeddings as the input of various nlp tasks , including part-of-speech tagging , chunking , ner , and semantic role labeling .
twitter is a widely used social networking service .
we used the disambig tool provided by the srilm toolkit .
the srilm toolkit was used to build the trigram mkn smoothed language model .
we use the wrapper of the scikit learn python library over the liblinear logistic regression implementation .
we used crfsuite and the glove word vector .
we used the open source moses decoder package for word alignment , phrase table extraction and decoding for sentence translation .
relation extraction is the task of finding semantic relations between entities from text .
part-of-speech ( pos ) tagging is a job to assign a proper pos tag to each linguistic unit such as word for a given sentence .
all the language models are built with the sri language modeling toolkit .
we trained a 4-gram language model on this data with kneser-ney discounting using srilm .
the decoder uses cky-style parsing with cube pruning to integrate the language model .
target language models were trained on the english side of the training corpus using the srilm toolkit .
we minimize cross-entropy loss over all 42 relations using adagrad .
the translation results are evaluated with case insensitive 4-gram bleu .
a 5-gram language model of the target language was trained using kenlm .
we use the webquestions dataset as our main dataset , which contains 5,810 question-answer pairs .
the framenet database provides an inventory of semantic frames together with a list of lexical units associated with these frames .
finally , we used kenlm to create a trigram language model with kneser-ney smoothing on that data .
to the best of our knowledge , this is the first time that the ¡° benefit of depths ¡± was shown for convolutional neural networks .
a 4-gram language model was trained on the monolingual data by the srilm toolkit .
the penn discourse treebank is the largest available discourse-annotated corpus in english .
we first use bleu score to perform automatic evaluation .
in this paper , we propose a novel unsupervised model , sentiment distribution consistency regularized .
we use the pre-trained word2vec embeddings provided by mikolov et al as model input .
miwa and bansal adopt a bidirectional dependency tree-lstm model by introducing a top-down lstm path .
we used the statistical japanese dependency parser cabocha for parsing .
we used the target side of the parallel corpus and the srilm toolkit to train a 5-gram language model .
the srilm toolkit was used to build this language model .
we use srilm to train a 5-gram language model on the xinhua portion of the english gigaword corpus 5th edition with modified kneser-ney discounting .
modified kneser-ney trigram models are trained using srilm upon the chinese portion of the training data .
sarcasm is defined as ‘ a cutting , often ironic remark intended to express contempt or ridicule ’ 1 .
we use the scikit-learn machine learning library to implement the entire pipeline .
the weights of the different feature functions were optimised by means of minimum error rate training on the 2008 test set .
case-insensitive bleu4 was used as the evaluation metric .
semantic role labeling ( srl ) is a kind of shallow semantic parsing task and its goal is to recognize some related phrases and assign a joint structure ( who did what to whom , when , where , why , how ) to each predicate of a sentence ( cite-p-24-3-4 ) .
we used moses , a phrase-based smt toolkit , for training the translation model .
the language model is a 5-gram lm with modified kneser-ney smoothing .
our source for syntactically annotated training data was the penn treebank .
in addition , we can use pre-trained neural word embeddings on large scale corpus for neural network initialization .
badjatiya et al used an lstm model with features extracted by character n-grams for hate speech detection .
like more data , performance improves log-linearly with the number of parameters ( unique n-grams ) .
the translation technology used in our system is based on the well-known phrase-based translation statistical approach .
the annotation scheme leans on the universal stanford dependencies complemented with the google universal pos tagset and the interset interlingua for morphological tagsets .
parsing { 2 } is the same as searching for an s node that dominates the entire string , ie .
we use the word2vec tool to train monolingual vectors , 6 and the cca-based tool for projecting word vectors .
we use the moses package to train a phrase-based machine translation model .
the parameters of the systems were tuned using mert to optimize bleu on the development set .
for the optimization process , we apply the diagonal variant of adagrad with mini-batches .
morphological analysis is the task of segmenting a word into morphemes , the smallest meaning-bearing elements of natural languages .
in this study , we propose to leverage both the information in the source language .
in our experiments we used 5-gram language models trained with modified kneser-ney smoothing using kenlm toolkit .
xing et al presented topic aware response generation by incorporating topic words obtained from a pre-trained lda model .
in our work , we build on lda , which is often used as a building block for topic models .
the most common word embeddings used in deep learning are word2vec , glove , and fasttext .
we report the mt performance using the original bleu metric .
the target language model was a trigram language model with modified kneser-ney smoothing trained on the english side of the bitext using the srilm tookit .
word sense disambiguation ( wsd ) is the problem of assigning a sense to an ambiguous word , using its context .
the development set is used to optimize feature weights using the minimum-error-rate algorithm .
twitter is the medium where people post real time messages to discuss on the different topics , and express their sentiments .
named entity disambiguation ( ned ) is the task of determining which concrete person , place , event , etc . is referred to by a mention .
for our purpose we use word2vec embeddings trained on a google news dataset and find the pairwise cosine distances for all words .
we trained a 4-gram language model on this data with kneser-ney discounting using srilm .
semantic role labeling ( srl ) is the process of producing such a markup .
srilm toolkit is used to build these language models .
word segmentation is the first step of natural language processing for japanese , chinese and thai because they do not delimit words by whitespace .
as word vectors the authors use word2vec embeddings trained with the skip-gram model .
we substitute our language model and use mert to optimize the bleu score .
socher et al used an rnn-based architecture to generate compositional vector representations of sentences .
standard vector space models of semantics are based in a term-document or word-context matrix .
for example , turian et al have improved the performance of chunking and named entity recognition by using word embedding also as one of the features in their crf model .
the process of identifying the correct meaning , or sense of a word in context , is known as word sense disambiguation ( wsd ) .
semantic parsing is the task of translating natural language utterances into a machine-interpretable meaning representation .
some researchers used similarity and association measures to build alignment links .
sentiment analysis is a natural language processing ( nlp ) task ( cite-p-10-1-14 ) which aims at classifying documents according to the opinion expressed about a given subject ( federici and dragoni , 2016a , b ) .
sentiment analysis is a recent attempt to deal with evaluative aspects of text .
for adjusting feature weights , the mert method was applied , optimizing the bleu-4 metric obtained on the development corpus .
the lms are build using the srilm language modelling toolkit with modified kneserney discounting and interpolation .
specifically , we tested the methods word2vec using the gensim word2vec package and pretrained glove word embeddings .
we rely on distributed representation based on the neural network skip-gram model of mikolov et al .
one of the first challenges in sentiment analysis is the vast lexical diversity of subjective language .
we initialize the vectors corresponding to words in our input layer with 100-dimensional vectors generated by a word2vec model trained on over one million words from the pubmed central article repository .
xiao et al introduce a topic similarity model to select the synchronous rules for hierarchical phrase-based translation .
as a baseline system for our experiments we use the syntax-based component of the moses toolkit .
all annotations were carried out with the brat rapid annotation tool .
the stanford parser was used to generate the dependency parse information for each sentence .
bengio et al proposed neural probabilistic language model by using a distributed representation of words .
the target language model is built on the target side of the parallel data with kneser-ney smoothing using the irstlm tool .
the model weights were trained using the minimum error rate training algorithm .
part-of-speech ( pos ) tagging is a job to assign a proper pos tag to each linguistic unit such as word for a given sentence .
we used the scikit-learn library the svm model .
we use pre-trained vectors from glove for word-level embeddings .
gram language model with modified kneser-ney smoothing is trained with the srilm toolkit on the epps , ted , newscommentary , and the gigaword corpora .
we use glove word embeddings , an unsupervised learning algorithm for obtaining vector representations of words .
we used the srilm toolkit to create 5-gram language models with interpolated modified kneser-ney discounting .
coreference resolution is the process of linking together multiple expressions of a given entity .
and we use sri language modeling toolkit to tune our feature weights .
word sense disambiguation ( wsd ) is a difficult natural language processing task which requires that for every content word ( noun , adjective , verb or adverb ) the appropriate meaning is automatically selected from the available sense inventory 1 .
the language model used was a 5-gram with modified kneserney smoothing , built with srilm toolkit .
coreference resolution is the process of linking together multiple expressions of a given entity .
metanet ’ s aims of increasing communication between citizens of different european countries .
as described in this paper , we propose a new automatic evaluation method for machine translation .
for our experiments , we used the latent variablebased berkeley parser .
a 4-gram language model was trained on the target side of the parallel data using the srilm toolkit .
in spite of the small-scale of training set , our approach outperforms the state-of-the-art systems in nlp & cc 2013 clsc .
curran and lin use syntactic features in the vector definition .
feature weight tuning was carried out using minimum error rate training , maximizing bleu scores on a held-out development set .
taxonomies which serve as backbone of structured knowledge are useful for many applications such as question answering and document clustering .
the lstm model is developed to solve the gradient vanishing or exploding problems in the rnn .
ccg is a lexicalized grammar formalism in which every constituent in a sentence is associated with a structured category that specifies its syntactic relationship to other constituents .
in our experiments , we used the srilm toolkit to build 5-gram language model using the ldc arabic gigaword corpus .
we set the feature weights by optimizing the bleu score directly using minimum error rate training on the development set .
negation is a linguistic phenomenon present in all languages ( cite-p-12-3-6 , cite-p-12-1-5 ) .
nlp researchers are especially well-positioned to contribute to the national discussion about gun violence .
we used trigram language models with interpolated kneser-kney discounting trained using the sri language modeling toolkit .
to evaluate our method , we use the webquestions dataset , which contains 5,810 questions crawled via google suggest api .
for the language model , we used sri language modeling toolkit to train a trigram model with modified kneser-ney smoothing on the 31 , 149 english sentences .
we preprocessed the corpus with tokenization and true-casing tools from the moses toolkit .
kalchbrenner et al propose a convolutional architecture for sentence representation that vertically stacks multiple convolution layers , each of which can learn independent convolution kernels .
coreference resolution is the process of linking multiple mentions that refer to the same entity .
to measure translation accuracy , we use the automatic evaluation measures of bleu and ribes measured over all sentences in the test corpus .
rosa et al and mare膷ek et al applied ape on english-to-czech mt outputs on morphological level .
we used srilm to build a 4-gram language model with kneser-ney discounting .
we use srilm to train a 5-gram language model on the target side of our training corpus with modified kneser-ney discounting .
germanet is a lexical semantic network that is modeled after the princeton wordnet for english .
the language model pis implemented as an n-gram model using the srilm-toolkit with kneser-ney smoothing .
a 3-gram language model was trained from the target side of the training data for chinese and arabic , using the srilm toolkit .
feature weights were trained with minimum error-rate training on the news-test2008 development set using the dp beam search decoder and the mert implementation of the moses toolkit .
we train 300 dimensional word embedding using word2vec on all the training data , and fine-turning during the training process .
coreference resolution is the task of grouping mentions to entities .
semantic role labeling ( srl ) is the task of identifying the arguments of lexical predicates in a sentence and labeling them with semantic roles ( cite-p-13-3-3 , cite-p-13-3-11 ) .
for estimating the monolingual we , we use the cbow algorithm as implemented in the word2vec package using a 5-token window .
we use the best performing model amongst those tested by baroni and colleagues , which has been constructed with word2vec 5 using the cbow approach proposed by mikolov et al .
mihalcea et al compared knowledgebased and corpus-based methods , using word similarity and word specificity to define one general measure of text semantic similarity .
sentiment analysis is a natural language processing ( nlp ) task ( cite-p-10-1-14 ) which aims at classifying documents according to the opinion expressed about a given subject ( federici and dragoni , 2016a , b ) .
we employ the pretrained word vector , glove , to obtain the fixed word embedding of each word .
the lexicalized reordering model was trained with the msd-bidirectionalfe option .
we use srilm for n-gram language model training and hmm decoding .
twitter is a huge microbloging service with more than 500 million tweets per day 1 from different locations in the world and in different languages .
for regularization , dropout is applied to the input and hidden layers .
active learning approach can effectively avoid this problem .
reading comprehension ( rc ) is a high-level task in natural language understanding that requires reading a document and answering questions about its content .
we use bleu and meteor for our automatic metric-based evaluation .
the word embeddings can provide word vector representation that captures semantic and syntactic information of words .
coreference resolution is the task of clustering a set of mentions in the text such that all mentions in the same cluster refer to the same entity .
as for ej translation , we use the stanford parser to obtain english abstraction trees .
coreference resolution is the task of partitioning a set of mentions ( i.e . person , organization and location ) into entities .
additionally , lexical substitution is a more natural task than similarity ratings , it makes it possible to evaluate meaning composition at the level of individual words , and provides a common ground to compare cdsms with dedicated lexical substitution models .
dredze et al showed the possibility that many parsing errors in the domain adaptation tasks came from inconsistencies between annotation manners of training resources .
we use five datasets from the conll-x shared task .
we also use glove vectors to initialize the word embedding matrix in the caption embedding module .
we use liblinear with l2 regularization and default parameters to learn a model .
we evaluate our system using bleu and ter .
we trained a trigram language model on the chinese side , with the srilm toolkit , using the modified kneser-ney smoothing option .
yu and chen proposed to use conditional random field to detect chinese word ordering errors .
it is a standard phrasebased smt system built using the moses toolkit .
for building our statistical ape system , we used maximum phrase length of 7 and a 5-gram language model trained using kenlm .
we train a recurrent neural network language model on a large collection of tweets .
a 4-gram language model was trained on the monolingual data by the srilm toolkit .
more recently , mikolov et al propose two log-linear models , namely the skip-gram and cbow model , to efficiently induce word embeddings .
word embeddings are critical for high-performance neural networks in nlp tasks .
we extract our paraphrase grammar from the french-english portion of the europarl corpus .
part-of-speech tagging is a crucial preliminary process in many natural language processing applications .
in this paper , we propose the use of autoencoders based on long short term memory neural networks for capturing long distance relationships between phonemes in a word .
the skip-gram model adopts a neural network structure to derive the distributed representation of words from textual corpus .
we evaluate our models using the standard bleu metric 2 on the detokenized translations of the test set .
we implement our approach in the framework of phrase-based statistical machine translation .
we used kenlm with srilm to train a 5-gram language model based on all available target language training data .
transliteration is a key building block for multilingual and cross-lingual nlp since it is essential for ( i ) handling of names in applications like machine translation ( mt ) and cross-lingual information retrieval ( clir ) , and ( ii ) user-friendly input methods .
le and mikolov extends the neural network of word embedding to learn the document embedding .
named entity recognition ( ner ) is the task of detecting named entity mentions in text and assigning them to their corresponding type .
we used the dataset from the conll shared task for cross-lingual dependency parsing .
we apply statistical significance tests using the paired bootstrapped resampling method .
we used maltparser to derive syntactic dependency relations in english .
conditional random fields are discriminative structured classification models for sequential tagging and segmentation .
we set the feature weights by optimizing the bleu score directly using minimum error rate training on the development set .
named entity recognition ( ner ) is the task of identifying and classifying phrases that denote certain types of named entities ( nes ) , such as persons , organizations and locations in news articles , and genes , proteins and chemicals in biomedical literature .
in order to deal with the evolutionary nature of the problem , nepveu et al propose an imt system with dynamic adaptation via cache-based model extensions for language and translation models .
we built a 5-gram language model on the english side of europarl and used the kneser-ney smoothing method and srilm as the language model toolkit .
luong et al created a hierarchical language model that uses rnn to combine morphemes of a word to obtain a word representation .
we cast the problem of event property extraction as a sequence labeling task , using conditional random fields for learning and inference .
word sense disambiguation ( wsd ) is the task to identify the intended sense of a word in a computational manner based on the context in which it appears ( cite-p-13-3-4 ) .
relation extraction is a traditional information extraction task which aims at detecting and classifying semantic relations between entities in text ( cite-p-10-1-18 ) .
we use pre-trained glove vector for initialization of word embeddings .
the process of identifying the correct meaning , or sense of a word in context , is known as word sense disambiguation ( wsd ) .
most existing works are based on variants and extensions of lda .
relation extraction is the task of finding relational facts in unstructured text and putting them into a structured ( tabularized ) knowledge base .
for the language model we use the corpus of 60,000 simple english wikipedia articles 3 and build a 3-gram language model with kneser-ney smoothing trained with srilm .
as described by joshi et al , recent approaches to irony can roughly be classified as either rule-based or machine learning-based .
meanwhile , we adopt glove pre-trained word embeddings 5 to initialize the representation of input tokens .
sentiment analysis is a natural language processing ( nlp ) task ( cite-p-10-1-14 ) which aims at classifying documents according to the opinion expressed about a given subject ( federici and dragoni , 2016a , b ) .
to quantify it , we train a word2vec model on a mid-2011 copy of english wikipedia .
we used trigram language models with interpolated kneser-kney discounting trained using the sri language modeling toolkit .
if the anaphor is a pronoun , the cache is searched for a plausible referent .
further , we apply a 4-gram language model trained with the srilm toolkit on the target side of the training corpus .
morfessor 2.0 is a new implementation of the morfessor baseline algorithm .
we used the sri language modeling toolkit to train a fivegram model with modified kneser-ney smoothing .
coreference resolution is the task of determining whether two or more noun phrases refer to the same entity in a text .
we use srilm to train a 5-gram language model on the target side of our training corpus with modified kneser-ney discounting .
dependency parsing is the task of building dependency links between words in a sentence , which has recently gained a wide interest in the natural language processing community and has been used for many problems ranging from machine translation ( cite-p-12-1-4 ) to question answering ( zhou et al. , 2011a ) .
huang et al , 2012 ) used the multi-prototype models to learn the vector for different senses of a word .
zeng et al exploit a convolutional neural network to extract lexical and sentence level features for relation classification .
mintz et al proposed a distant supervision approach for relation extraction using a richfeatured logistic regression model .
all models used interpolated modified kneser-ney smoothing .
since our dataset is not so large , we make use of pre-trained word embeddings , which are trained on a much larger corpus with word2vec toolkit .
unlike dong et al , we initialize our word embeddings using a concatenation of the glove and cove embeddings .
we use conditional random field sequence labeling as described in .
coreference resolution is the process of determining whether two expressions in natural language refer to the same entity in the world .
meanwhile , we adopt glove pre-trained word embeddings 5 to initialize the representation of input tokens .
to exploit these kind of labeling constraints , we resort to conditional random fields .
we use srilm for training the 5-gram language model with interpolated modified kneser-ney discounting .
relation extraction is the task of finding relations between entities in text , which is useful for several tasks such as information extraction , summarization , and question answering ( cite-p-14-3-7 ) .
we use the glove algorithm to obtain 300-dimensional word embeddings from a union of these corpora .
we further used a 5-gram language model trained using the srilm toolkit with modified kneser-ney smoothing .
for word-level embeddings , we pre-train the word vectors using word2vec on the gigaword corpus mentioned in section 4 , and the text of the training dataset .
coreference resolution is the task of determining which mentions in a text are used to refer to the same real-world entity .
cite-p-20-1-5 extended work by adding several more subsystems in this error model .
coreference resolution is a fundamental component of natural language processing ( nlp ) and has been widely applied in other nlp tasks ( cite-p-15-3-9 ) .
table 1 shows the translation performance by bleu .
for the sick and msrvid experiments , we used 300-dimension glove word embeddings .
tai et al and zhu et al extended sequential lstms to tree-structured lstms by adding branching factors .
a joint probability model for phrase translation was proposed by marcu and wong .
following the current practice in evaluating summarization , particularly duc 3 , we use the rouge evaluation package .
coreference resolution is the task of determining which mentions in a text refer to the same entity .
coreference resolution is the process of linking together multiple referring expressions of a given entity in the world .
we pretrain word vectors with the word2vec tool on the news dataset released by ding et al , which are fine-tuned during training .
to this end , we use first-and second-order conditional random fields .
a sentiment lexicon is a list of words and phrases , such as “ excellent ” , “ awful ” and “ not bad ” , each of them is assigned with a positive or negative score reflecting its sentiment polarity and strength ( cite-p-18-3-8 ) .
for this language model , we built a trigram language model with kneser-ney smoothing using srilm from the same automatically segmented corpus .
a 4-gram language model was trained on the target side of the parallel data using the srilm toolkit from stolcke .
word sense disambiguation ( wsd ) is a task to identify the intended sense of a word based on its context .
a 4-gram language model was trained on the target side of the parallel data using the srilm toolkit .
qiu et al propose a double propagation method to extract opinion word and opinion target simultaneously .
we preprocessed all aligned translations by means of the treetagger tool that outputs part-of-speech and 55 lemma information .
we use the open-source moses toolkit to build four arabic-english phrase-based statistical machine translation systems .
we begin by building two word alignment models using the berkeley aligner , a state-of-the-art word alignment package that relies on ibm models 1 and 2 and an hmm .
more recently , matsuo et al presented a method of word clustering based on web counts using a search engine .
pre-trained word embeddings were shown to boost the performance in various nlp tasks and specifically in ner .
to convert into a distributed representation here , a neural network for word embedding learns via the skip-gram model .
the evaluation metric for the overall translation quality was case-insensitive bleu4 .
we use conditional random fields , a popular approach to solve sequence labeling problems .
we parsed the corpus with rasp and with the stanford pcfg parser .
we pre-train the word embeddings using word2vec .
we use srilm toolkit to build a 5-gram language model with modified kneser-ney smoothing .
more recently , features drawn from word embeddings have been shown to be effective in various text classification tasks such as sentiment analysis and named entity recognition .
yarowsky used the one sense per collocation property as an essential ingredient for an unsupervised word-sense disambiguation algorithm .
word2vec is a prediction-based distributional model in which a word representation is obtained from a neural network trying to predict a word from its context or vice-versa .
this approach has already been used with great success in the domain of language models .
we model the task of finding commonalities from student answers in a manner similar to the sequential pattern mining problem .
further , the word embeddings are initialized with glove , and not tied with the softmax weights .
relation extraction is the key component for building relation knowledge graphs , and it is of crucial significance to natural language processing applications such as structured search , sentiment analysis , question answering , and summarization .
the subtask of aspect category detection obtains the best result when applying the boosting method .
dependency parsing is a central nlp task .
barzilay and mckeown used a corpus-based method to identify paraphrases from a corpus of multiple english translations of the same source text .
bansal et al show the benefits of such modified-context embeddings in dependency parsing task .
we use the weka toolkit and the derived features to train a naive-bayes classifier .
relation extraction is the task of predicting semantic relations over entities expressed in structured or semi-structured text .
in previous approaches , the features are collected from corpora , those we make use of are retrieved from the lexicon entries .
we use srilm toolkit to train a trigram language model with modified kneser-ney smoothing on the target side of training corpus .
sentiment analysis ( sa ) is a hot-topic in the academic world , and also in the industry .
xu et al and min et al improve the quality of distant supervision training data by reducing false negative examples .
the probabilistic language model is constructed on google web 1t 5-gram corpus by using the srilm toolkit .
as a classifier , we employ support vector machines as implemented in svm light .
we used adam optimizer with its standard parameters .
the translation results are evaluated with case insensitive 4-gram bleu .
named entity recognition ( ner ) is the task of identifying and typing phrases that contain the names of persons , organizations , locations , and so on .
for our experiments reported here , we obtained word vectors using the word2vec tool and the text8 corpus .
more recently wang et al proposed to train a conditional random field using an entropy-based regularizer .
long-short term memory networks have been proposed to solve this issue , and so we employ them .
the annotation scheme is based on an evolution of stanford dependencies and google universal part-of-speech tags .
we trained a 4-gram language model on this data with kneser-ney discounting using srilm .
we trained a trigram language model on the chinese side , with the srilm toolkit , using the modified kneser-ney smoothing option .
morphological disambiguation is a useful first step for higher level analysis of any language but it is especially critical for agglutinative languages like turkish , czech , hungarian , and finnish .
sentiment analysis is the study of the subjectivity and polarity ( positive vs. negative ) of a text ( cite-p-7-1-10 ) .
we use the liblinear tool as our svm implementation .
we used a logistic regression classifier provided by the liblinear software .
the model weights are automatically tuned using minimum error rate training .
we train a trigram language model with modified kneser-ney smoothing from the training dataset using the srilm toolkit , and use the same language model for all three systems .
as a software we use srilm with the default algorithm .
for this reason , we used glove vectors to extract the vector representation of words .
we present an active sentiment domain adaptation approach to train accurate sentiment classifier for target domain with less labeled samples .
the formally syntax-based models use synchronous context-free grammar but induce a grammar from a parallel text without relying on any linguistic annotations or assumptions .
table 2 shows the blind test results using bleu-4 , meteor and ter .
we segment english and chinese tokens into subwords via byte-pair encoding .
we used the implementation of the scikit-learn 2 module .
katiyar and cardie proposed a recurrent neural network to extract features to learn an hypergraph structure of nested mentions , using a bilou encoding scheme .
however , we use a large 4-gram lm with modified kneser-ney smoothing , trained with the srilm toolkit , stolcke , 2002 and ldc english gigaword corpora .
pereira et al suggested deterministic annealing to cluster verb-argument pairs into classes of verbs and nouns .
we use the moses toolkit with a phrase-based baseline to extract the qe features for the x l , x u , and testing .
we initialize the embedding weights by the pre-trained word embeddings with 200 dimensional vectors .
we measure translation quality via the bleu score .
we used the srilm toolkit to build unpruned 5-gram models using interpolated modified kneser-ney smoothing .
sentiment classification is a special task of text categorization that aims to classify documents according to their opinion of , or sentiment toward a given subject ( e.g. , if an opinion is supported or not ) ( cite-p-11-1-2 ) .
guo et al , 2014 ) explored bilingual resources to learn sense-specific word representation .
word sense disambiguation ( wsd ) is the task of determining the meaning of an ambiguous word in its context .
the nodes are concepts ( or synsets as they are called in the wordnet ) .
we trained the machine translation toolkit moses to translate groups of letters rather than groups of words .
we used the sri language modeling toolkit for this purpose .
we estimated 5-gram language models using the sri toolkit with modified kneser-ney smoothing .
for entity tagging we used a maximum entropy model .
all models used interpolated modified kneser-ney smoothing .
we will show translation quality measured with the bleu score as a function of the phrase table size .
we have presented here a new methodology for acquiring comprehensive multiword lexicons from large corpora , using competition .
we train a 4-gram language model on the xinhua portion of the gigaword corpus using the sri language toolkit with modified kneser-ney smoothing .
a 4-gram language model was trained on the target side of the parallel data using the srilm toolkit from stolcke .
the model weights were trained using the minimum error rate training algorithm .
the srilm toolkit was used to build the trigram mkn smoothed language model .
sagae and tsujii used an ensemble to select high-quality dependency parses .
such as wordnet ( cite-p-11-1-13 ) with subjectivity labels could support better subjectivity analysis .
the system was trained using the moses toolkit .
we used a phrase-based smt model as implemented in the moses toolkit .
transition-based and graph-based models have attracted the most attention of dependency parsing in recent years .
we trained a 5-gram sri language model using the corpus supplied for this purpose by the shared task organizers .
we use the word2vec tool to pre-train the word embeddings .
the log-lineal combination weights were optimized using mert .
reviews demonstrate that proposed method outperforms the template extraction based algorithm .
we substitute our language model and use mert to optimize the bleu score .
relation extraction is a traditional information extraction task which aims at detecting and classifying semantic relations between entities in text ( cite-p-10-1-18 ) .
word embeddings are considered one of the key building blocks in natural language processing and are widely used for various applications .
semantic textual similarity is the task of judging the similarity of a pair of sentences on a scale from 0 to 5 , and was recently introduced as a semeval task .
in this paper , we describe an approach which overcomes this problem .
relation extraction is the task of automatically detecting occurrences of expressed relations between entities in a text and structuring the detected information in a tabularized form .
we use the glove vector representations to compute cosine similarity between two words .
duh et al used a neural network based language model trained on a small in-domain corpus to select from a larger data pool .
sentiment classification is the task of identifying the sentiment polarity ( e.g. , positive or negative ) of * 1 corresponding author a natural language text towards a given topic ( cite-p-18-1-19 , cite-p-18-3-1 ) and has become the core component of many important applications in opinion analysis ( cite-p-18-1-2 , cite-p-18-1-10 , cite-p-18-1-15 , cite-p-18-3-4 ) .
for nb and svm , we used their implementation available in scikit-learn .
coreference resolution is the process of determining whether two expressions in natural language refer to the same entity in the world .
the srilm toolkit was used to build the trigram mkn smoothed language model .
a 4-gram language model was trained on the monolingual data by the srilm toolkit .
the parameter weights are optimized with minimum error rate training .
sch眉tze created sense representations by clustering context representations derived from co-occurrence .
text categorization is a classical text information processing task which has been studied adequately ( cite-p-18-1-9 ) .
for this purpose , we turn to the expectation maximization algorithm .
traditional topic models like latent dirichlet allocation have been explored extensively to discover topics from text .
luong et al break words into morphemes , and use recursive neural networks to compose word meanings from morpheme meanings .
the language model used was a 5-gram with modified kneserney smoothing , built with srilm toolkit .
sentiment analysis is a research area where does a computational analysis of people ’ s feelings or beliefs expressed in texts such as emotions , opinions , attitudes , appraisals , etc . ( cite-p-12-1-3 ) .
we implemented the different aes models using scikit-learn .
relation extraction is the task of automatically detecting occurrences of expressed relations between entities in a text and structuring the detected information in a tabularized form .
additionally , coreference resolution is a pervasive problem in nlp and many nlp applications could benefit from an effective coreference resolver that can be easily configured and customized .
as our supervised classification algorithm , we use a linear svm classifier from liblinear , with its default parameter settings .
a 4-gram language model was trained on the target side of the parallel data using the srilm toolkit from stolcke .
in this paper , we examine the problem of detecting ideological bias .
callison-burch et al used paraphrases of the trainig corpus for translating unseen phrases .
parameters are initialized using the method described by glorot and bengio .
the language model is a 5-gram lm with modified kneser-ney smoothing .
kulkarni et al use neural word embeddings to model the shift in meaning of words such as gay over the last century .
we use several classifiers including logistic regression , random forest and adaboost implemented in scikit-learn .
we use the treebanks from the conll shared tasks on dependency parsing for evaluation .
for pos tagging and syntactic parsing , we use the stanford nlp toolkit .
we apply srilm to train the 3-gram language model of target side .
lexical simplification is a specific case of lexical substitution where the complex words in a sentence are replaced with simpler words .
negation is a linguistic phenomenon where a negation cue ( e.g . not ) can alter the meaning of a particular text segment or of a fact .
word alignment is a crucial early step in the training of most statistical machine translation ( smt ) systems , in which the estimated alignments are used for constraining the set of candidates in phrase/grammar extraction ( cite-p-9-3-5 , cite-p-9-1-4 , cite-p-9-3-0 ) .
in addition , a 5-gram lm with kneser-ney smoothing and interpolation was built using the srilm toolkit .
an english 5-gram language model is trained using kenlm on the gigaword corpus .
collobert et al , 2011 ) used word embeddings for pos tagging , named entity recognition and semantic role labeling .
we have also shown that phrase structure trees , even when deprived of the labels , retain in a certain sense .
the xml markup was removed , and the collection was tokenised and split into sentences using bio-specific nlp tools .
in this paper , we propose a neural system combination framework , which is adapted from the multi-source nmt model .
hatzivassiloglou and mckeown used a log-linear regression model to predict the similarity of conjoined adjectives .
for our purpose we use word2vec embeddings trained on a google news dataset and find the pairwise cosine distances for all words .
crowdsourcing is the use of the mass collaboration of internet passersby for large enterprises on the world wide web such as wikipedia and survey companies .
the phrase-based translation model uses the con- the baseline lm was a regular n-gram lm with kneser-ney smoothing and interpolation by means of the srilm toolkit .
we use srilm to build 5-gram language models with modified kneser-ney smoothing .
using word2vec , we compute word embeddings for our text corpus .
lexical simplification is a subtask of text simplification ( cite-p-16-3-3 ) concerned with replacing words or short phrases by simpler variants in a context aware fashion ( generally synonyms ) , which can be understood by a wider range of readers .
chapman et al developed negex , a regular expression based algorithm for determining whether a finding or disease mentioned within narrative medical reports is present or absent .
we use the logistic regression implementation of liblinear wrapped by the scikit-learn library .
luong et al propose bilingual skip-gram which extends the monolingual skip-gram model and learns bilingual embeddings using a parallel copora and word alignments .
in this way , our cache-based approach can provide useful data at the beginning of the translation process .
we used the srilm software 4 to build langauge models as well as to calculate cross-entropy based features .
the dts are based on collapsed dependencies from the stanford parser in the holing operation .
ever since the pioneering article of gildea and jurafsky , there has been an increasing interest in automatic semantic role labeling .
our trigram word language model was trained on the target side of the training corpus using the srilm toolkit with modified kneser-ney smoothing .
we used trigram language models with interpolated kneser-kney discounting trained using the sri language modeling toolkit .
the translation quality is evaluated by caseinsensitive bleu-4 metric .
the language models are 4-grams with modified kneser-ney smoothing which have been trained with the srilm toolkit .
to optimize model parameters , we use the adagrad algorithm of duchi et al with l2 regularization .
performance is measured in terms of bleu and ter computed using the multeval script .
the experiments are carried out on a subset of the basic travel expression corpus , as it is used for the supplied data track condition of the iwslt evaluation campaign .
pun is a way of using the characteristics of the language to cause a word , a sentence or a discourse to involve two or more different meanings .
our baseline system is an standard phrase-based smt system built with moses .
relation extraction is the task of automatically detecting occurrences of expressed relations between entities in a text and structuring the detected information in a tabularized form .
related to their information goals , the interaction context can provide useful cues for the system to automatically identify problematic situations .
translation quality is evaluated by case-insensitive bleu-4 metric .
we use srilm for training the 5-gram language model with interpolated modified kneser-ney discounting .
the source and target sentences are tagged respectively using the treetagger and amira toolkits .
we use the opensource moses toolkit to build a phrase-based smt system .
we initialize our model with 300-dimensional word2vec toolkit vectors generated by a continuous skip-gram model trained on around 100 billion words from the google news corpus .
for all models , we use fixed pre-trained glove vectors and character embeddings .
we use case-sensitive bleu-4 to measure the quality of translation result .
we use conditional random fields for sequence labelling .
itspoke , is a speech-enabled tutor built on top of the text-based why2-atlas conceptual physics tutor .
word sense disambiguation ( wsd ) is the task of assigning sense tags to ambiguous lexical items ( lis ) in a text .
the smt system deployed in our approach is an implementation of the alignment template approach of och and ney .
language modeling is trained using kenlm using 5-grams , with modified kneser-ney smoothing .
we use the moses toolkit to train our phrase-based smt models .
collobert and weston , in their seminal paper on deep architectures for nlp , propose a multilayer neural network for learning word embeddings .
the language models are 4-grams with modified kneser-ney smoothing which have been trained with the srilm toolkit .
the penn discourse treebank is the largest corpus richly annotated with explicit and implicit discourse relations and their senses .
table 4 shows the bleu scores of the output descriptions .
mcclosky et al presented a self-training approach for phrase structure parsing and the approach was shown to be effective in practice .
we use glove vectors with 100 dimensions trained on wikipedia and gigaword as word embeddings .
sentence compression is the task of producing a summary at the sentence level .
bleu is essentially a precision-based metric and is currently the standard metric for automatic evaluation of mt performance .
word sense disambiguation ( wsd ) is formally defined as the task of computationally identifying senses of a word in a context .
a 4-gram language model was trained on the target side of the parallel data using the srilm toolkit from stolcke .
tomanek et al utilised eye-tracking data to evaluate difficulties of named entities for selecting training instances for active learning techniques .
then , we encode each tweet as a sequence of word vectors initialized using glove embeddings .
sentiment analysis ( sa ) is a hot-topic in the academic world , and also in the industry .
we trained a 5-gram language model on the xinhua portion of gigaword corpus using the srilm toolkit .
socher et al present a compositional model based on a recursive neural network .
srilm toolkit is used to build these language models .
we use a sequential lstm to encode this description .
unsupervised word embeddings trained from large amounts of unlabeled data have been shown to improve many nlp tasks .
through extensive experiments on multiple real-world datasets , we find that sictf is not only more accurate than state-of-the-art baselines , but also significantly faster ( about 14x faster .
we use srilm for training the 5-gram language model with interpolated modified kneser-ney discounting .
we use the sri language model toolkit to train a 5-gram model with modified kneser-ney smoothing on the target-side training corpus .
luong et al segment words using morfessor , and use recursive neural networks to build word embeddings from morph embeddings .
to compensate the limit of in-domain data size , we use word2vec to learn the word embedding from a large amount of general-domain data .
for feature extraction , we used the stanford pos tagger .
we used svm classifier that implements linearsvc from the scikit-learn library .
we used the moses mt toolkit with default settings and features for both phrase-based and hierarchical systems .
we measure the translation quality using a single reference bleu .
coreference resolution is the task of grouping mentions to entities .
long short term memory units are proposed in hochreiter and schmidhuber to overcome this problem .
these language models were built up to an order of 5 with kneser-ney smoothing using the srilm toolkit .
we evaluate the translation quality using the case-sensitive bleu-4 metric .
we experiment with both the maltparser and the mstparser as the dg parser .
grammar induction is a task within the field of natural language processing that attempts to construct a grammar of a given language solely on the basis of positive examples of this language .
the stanford parser is used to parse chinese sentences on the training , dev and test sets .
the trigram language model is implemented in the srilm toolkit .
we use pretrained 100-d glove embeddings trained on 6 billion tokens from wikipedia and gigaword corpus .
hence we use the expectation maximization algorithm for parameter learning .
the target-side language models were estimated using the srilm toolkit .
abstract meaning representation is a framework suitable for integrated semantic annotation .
according to lakoff and johnson and others , these linguistic metaphors are an observable manifestation of our mental , conceptual metaphors .
bunescu and mooney give a shortest path dependency kernel for relation extraction .
we trained a 4-gram language model with kneser-ney smoothing and unigram caching using the sri-lm toolkit .
while pitler and nenkova have shown that the discourse relation feature is strongest at predicting the linguistic quality of a document , dr shows poor performance .
to measure the translation quality , we use the bleu score and the nist score .
we use srilm for training the 5-gram language model with interpolated modified kneser-ney discounting .
we used trigram language models with interpolated kneser-kney discounting trained using the sri language modeling toolkit .
evaluation was done with multi-reference bleu on test sets with four references for each language pair , and mira was used for tuning .
we use a popular word2vec neural language model to learn the word embeddings on an unsupervised tweet corpus .
all the weights of those features are tuned by using minimal error rate training .
for this purpose , we use the moses toolkit for training translation models and decoding , as well as srilm 2 to build the language models .
more importantly , event coreference resolution is a necessary component in any reasonable , broadly applicable computational model of natural language understanding ( cite-p-18-3-4 ) .
we created 5-gram language models for every domain using srilm with improved kneserney smoothing on the target side of the training parallel corpora .
in this paper we propose multi-space variational encoder-decoders , a new model for labeled sequence transduction .
a 3-gram language model was trained from the target side of the training data for chinese and arabic , using the srilm toolkit .
we used the pre-trained word embeddings that were learned using the word2vec toolkit on google news dataset .
relation extraction is the task of finding semantic relations between entities from text .
aspect-based sentiment analysis is one of the main frameworks for sentiment analysis .
active learning processin this work , we are interested in selective sampling for pool-based active learning , and focus on uncertainty sampling .
sentiment analysis is the process of identifying and extracting subjective information using natural language processing ( nlp ) .
we use pre-trained vectors from glove for word-level embeddings .
twitter is the medium where people post real time messages to discuss on the different topics , and express their sentiments .
the lms are build using the srilm language modelling toolkit with modified kneserney discounting and interpolation .
a 4-gram language model was trained on the target side of the parallel data using the srilm toolkit .
all source-target sentences were parsed with the stanford parser in order to label the text with syntactic information .
as discussed in the introduction , we use conditional random fields , since they are particularly suitable for sequence labelling .
we optimized the learned parameters with the adam stochastic gradient descent .
the input layers are initialized using the glove vectors , and are updated during training .
we use the pre-trained 300-dimensional word2vec embeddings trained on google news 1 as input features .
multi-task learning has resulted in successful systems for various nlp tasks , especially in cross-lingual settings .
word sense disambiguation ( wsd ) is the task of automatically determining the correct sense for a target word given the context in which it occurs .
we employ the glove and node2vec to generate the pre-trained word embedding , obtaining two distinct embedding for each word .
we use stanford corenlp for preprocessing and a supervised learning approach for classification .
finally , we represent subtree-based features on training data .
dependency parsing consists of finding the structure of a sentence as expressed by a set of directed links ( dependencies ) between words .
we set the feature weights by optimizing the bleu score directly using minimum error rate training on the development set .
we use the popular word2vec 1 tool proposed by mikolov et al to extract the vector representations of words .
chen et al and koo et al used large-scale unlabeled data to improve syntactic dependency parsing performance .
word alignment is the problem of annotating parallel text with translational correspondence .
marcu and wong , 2002 , defined the joint model , which modeled consecutive word m-to-n alignments .
to facilitate comparison with previous results , we used the upenn treebank corpus .
to keep consistent , we initialize the embedding weight with pre-trained word embeddings .
long sentences are removed , and the remaining sentences are pos-tagged and dependency parsed using the pre-trained stanford parser .
relation extraction is the problem of populating a target relation ( representing an entity-level relationship or attribute ) with facts extracted from natural-language text .
therefore , we use the long short-term memory network to overcome this problem .
coreference resolution is a key problem in natural language understanding that still escapes reliable solutions .
for learning coreference decisions , we used a maximum entropy model .
our primary contribution in this paper is a recasting and merging of the tasks of mention detection and entity disambiguation into a single endto-end entity linking task .
to represent the semantics of the nouns , we use the word2vec method which has proven to produce accurate approximations of word meaning in different nlp tasks .
also , we initialized all of the word embeddings using the 300 dimensional pre-trained vectors from glove .
the translation outputs were evaluated with bleu and meteor .
language models were built with srilm , modified kneser-ney smoothing , default pruning , and order 5 .
in particular , the vector-space word representations learned by a neural network have been shown to successfully improve various nlp tasks .
coreference resolution is the task of determining which mentions in a text refer to the same entity .
we used the implementation of random forest in scikitlearn as the classifier .
we measure machine translation performance using the bleu metric .
semantic parsing is the task of mapping natural language utterances to machine interpretable meaning representations .
a joint probability model for phrase translation was proposed by marcu and wong .
relation extraction ( re ) is the task of assigning a semantic relationship between a pair of arguments .
to train the link embeddings , we use the speedy , skip-gram neural language model of mikolov et al via their toolkit word2vec .
we use the mallet implementation of conditional random fields .
we use a phrase-based translation system similar to moses .
we use pre-trained 50 dimensional glove vectors 4 for word embeddings initialization .
we report mt performance in table 1 by case-insensitive bleu .
a trigram language model with modified kneser-ney discounting and interpolation was used as produced by the srilm toolkit .
we pretrain word vectors with the word2vec tool on the news dataset released by ding et al , which are fine-tuned during training .
information extraction ( ie ) is the task of generating structured information , often in the form of subject-predicate-object relation triples , from unstructured information such as natural language text .
clark and curran describes log-linear parsing models for ccg .
abstract meaning representation is a semantic formalism in which the meaning of a sentence is encoded as a rooted , directed , acyclic graph .
greedy transition-based dependency parsers incrementally process an input sentence from left to right .
we used a 5-gram language model with modified kneser-ney smoothing implemented using the srilm toolkit .
we use 5-grams for all language models implemented using the srilm toolkit .
results were evaluated with both bleu and nist metrics .
named entity recognition ( ner ) is the process by which named entities are identified and classified in an open-domain text .
relation extraction ( re ) is the task of extracting instances of semantic relations between entities in unstructured data such as natural language text .
we use a linear chain crf to predict the morphological features .
we adopt two standard metrics rouge and bleu for evaluation .
sentiment analysis ( sa ) is the determination of the polarity of a piece of text ( positive , negative , neutral ) .
inspired by previous work , we adapt the word2vec nnlm of mikolov et al to this qa task .
we implemented the different aes models using scikit-learn .
in this paper , we proposed a complex neural network model for geolocation prediction .
we derive 100-dimensional word vectors using word2vec skip-gram model trained over the domain corpus .
we used glove vectors trained on common crawl 840b 4 with 300 dimensions as fixed word embeddings .
we describe the first edition of the complex word identification task , organized at semeval 2016 .
finally , we used kenlm to create a trigram language model with kneser-ney smoothing on that data .
dependency parsing is a very important nlp task and has wide usage in different tasks such as question answering , semantic parsing , information extraction and machine translation .
we report decoding speed and bleu score , as measured by sacrebleu .
srilm toolkit is used to build these language models .
we use a sequential lstm to encode this description .
for the classification task , we use pre-trained glove embedding vectors as lexical features .
the model weights were trained using the minimum error rate training algorithm .
relation classification is the task of identifying the semantic relation holding between two nominal entities in text .
the evaluation metric for the overall translation quality is caseinsensitive bleu4 .
translation results are reported on the standard mt metrics bleu , meteor , and per , position independent word error rate .
to get a dictionary of word embeddings , we use the word2vec tool 2 and train it on the chinese gigaword corpus .
vignet is inspired by and based on framenet , a resource for lexical semantics .
in this paper , we propose detecting disfluencies using a right-to-left transition-based dependency .
sentiment analysis is a nlp task that deals with extraction of opinion from a piece of text on a topic .
for evaluation we use multeval to calculate bleu , meteor , ter , and length of the test set for each system .
we pretrain word vectors with the word2vec tool on the news dataset released by ding et al , which are fine-tuned during training .
word embeddings have been trained using word2vec 4 tool .
our results show that we improve over a state-of-the-art baseline by over 2 . 7 % ( relative bleu score ) .
information extraction ( ie ) is the task of extracting factual assertions from text .
bilingual lexicons are an important resource in multilingual natural language processing tasks such as statistical machine translation and cross-language information retrieval .
current state-of-the-art statistical parsers are all trained on large annotated corpora such as the penn treebank .
we then lowercase all data and use all sentences from the modern dutch part of the corpus to train an n-gram language model with the srilm toolkit .
dependency parsing is a valuable form of syntactic processing for nlp applications due to its transparent lexicalized representation and robustness with respect to flexible word order languages .
it was first used for unlabeled dependency parsing by kudo and matsumoto and yamada and matsumoto .
the log-lineal combination weights were optimized using mert .
for training the model , we use the linear kernel svm implemented in the scikit-learn toolkit .
and our simplest model has a concave objective that guarantees convergence to a global optimum .
we use glove vectors with 200 dimensions as pre-trained word embeddings , which are tuned during training .
coreference resolution is the process of finding discourse entities ( markables ) referring to the same real-world entity or concept .
zhou et al employed both unsupervised and supervised neural networks to learn bilingual sentiment word embedding .
the language model is trained and applied with the srilm toolkit .
one of the most important resources for discourse connectives in english is the penn discourse treebank .
relevance for satisfaction ¡¯ , ¡® contrastive weight ¡¯ and certain adverbials , that work to affect polarity in a more subtle but crucial manner , as evidenced also by the statistical analysis .
we use srilm toolkit to build a 5-gram language model with modified kneser-ney smoothing .
we evaluated translation output using case-insensitive ibm bleu .
word alignment , which can be defined as an object for indicating the corresponding words in a parallel text , was first introduced as an intermediate result of statistical translation models ( cite-p-13-1-2 ) .
for our experiments , we use 300-dimensional glove english word embeddings trained on the cased common crawl .
in this work , we propose to tackle the problem of list-only entity linking .
in our work , we build on lda , which is often used as a building block for topic models .
we used the default parameter in svm light for all trials .
berland and charniak used a similar method for extracting instances of meronymy relation .
luong and manning proposed a hybrid scheme that consults character-level information whenever the model encounters an oov word .
recently , the field has been influenced by the success of neural language models .
to solve the traditional recurrent neural networks , hochreiter and schmidhuber proposed the lstm architecture .
however , aspect extraction is a complex task that also requires fine-grained domain embeddings .
soricut and echihabi explore pseudo-references and document-aware features for document-level ranking , using bleu as quality label .
we train a 4-gram language model on the xinhua portion of the english gigaword corpus using the srilm toolkits with modified kneser-ney smoothing .
we use srilm toolkit to build a 5-gram language model with modified kneser-ney smoothing .
english texts were tokenized by the stanford parser 5 with the pcfg grammar .
word sense disambiguation is the process of selecting the most appropriate meaning for a word , based on the context in which it occurs .
an in-house language modeling toolkit was used to train the 4-gram language models with modified kneser-ney smoothing over the web-crawled data .
our parser is based on the shift-reduce parsing process from sagae and lavie and wang et al , and therefore it can be classified as a transition-based parser .
we use srilm for training the 5-gram language model with interpolated modified kneser-ney discounting .
the target language model is trained by the sri language modeling toolkit on the news monolingual corpus .
bannard and callison-burch proposed identifying paraphrases by pivoting through phrases in a bilingual parallel corpora .
we implemented the algorithms in python using the stochastic gradient descent method for nmf from the scikit-learn package .
the dimension of glove word vectors is set as 300 .
we measure the translation quality with automatic metrics including bleu and ter .
language models were estimated using the sri language modeling toolkit with modified kneser-ney smoothing .
we initialize the embedding layer using embeddings from dedicated word embedding techniques word2vec and glove .
chen et al proposed an approach that extracted partial tree structures from a large amount of data and used them as the additional features to improve dependency parsing .
we use word2vec from as the pretrained word embeddings .
the language models are 4-grams with modified kneser-ney smoothing which have been trained with the srilm toolkit .
we chose the skip-gram model provided by word2vec tool developed by for training word embeddings .
sentiment analysis is a natural language processing ( nlp ) task ( cite-p-10-1-14 ) which aims at classifying documents according to the opinion expressed about a given subject ( federici and dragoni , 2016a , b ) .
the parameter weights are optimized with minimum error rate training .
al-onaizan and knight find that a model mapping directly from english to arabic letters outperforms the phoneme-toletter model .
relation extraction ( re ) is the task of recognizing relationships between entities mentioned in text .
the seminal work in the field of hypernym learning was done by hearst .
training is done through stochastic gradient descent over shuffled mini-batches with adadelta update rule .
we trained a tri-gram hindi word language model with the srilm tool .
for smt decoding , we use the moses toolkit with kenlm for language model queries .
a more effective alternative , which however only delivers quasinormalized scores , is to train the network using the noise contrastive estimation or nce .
with the refined outputs , we build phrasebased transliteration systems using moses , a popular statistical machine translation framework .
the model is a log-linear model over synchronous cfg derivations .
we could approach the movie overview generation task using an attention-based encoder-decoder model .
we used a 5-gram language model with modified kneser-ney smoothing , built with the srilm toolkit .
in both pre-training and fine-tuning , we adopt adagrad and l2 regularizer for optimization .
the model parameters of word embedding are initialized using word2vec .
pun is a way of using the characteristics of the language to cause a word , a sentence or a discourse to involve two or more different meanings .
we propose an implicit content-introducing method which incorporates additional information into the seq2seq model .
the minimum error rate training was used to tune the feature weights .
we use skip-gram with negative sampling for obtaining the word embeddings .
for language model scoring , we use the srilm toolkit training a 5-gram language model for english .
we use 5-grams for all language models implemented using the srilm toolkit .
to encode the original sentences we used word2vec embeddings pre-trained on google news .
each grammar consists of a set of rules evaluated in a leftto-right fashion over the input annotations , with multiple grammars cascaded together and evaluated bottom-up .
word sense disambiguation is the task of assigning a sense to a word based on the context in which it occurs .
we used the bleu score to evaluate the translation accuracy with and without the normalization .
feature weights were set with minimum error rate training on a tuning set using bleu as the objective function .
coreference resolution is a set partitioning problem in which each resulting partition refers to an entity .
we use a cws-oriented model modified from the skip-gram model to derive word embeddings .
we created 5-gram language models for every domain using srilm with improved kneserney smoothing on the target side of the training parallel corpora .
twitter is a famous social media platform capable of spreading breaking news , thus most of rumour related research uses twitter feed as a basis for research .
for feature building , we use word2vec pre-trained word embeddings .
we use long shortterm memory networks to build another semanticsbased sentence representation .
for input representation , we used glove word embeddings .
in natural language , a word often assumes different meanings , and the task of determining the correct meaning , or sense , of a word in different contexts is known as word sense disambiguation ( wsd ) .
we used the part of speech tagged for tweets with the twitter nlp tool .
semantic role labeling ( srl ) is the task of labeling predicate-argument structure in sentences with shallow semantic information .
as classifier we use a traditional model , a support vector machine with linear kernel implemented in scikit-learn .
we use word2vec 1 toolkit to pre-train the character embeddings on the chinese wikipedia corpus .
we use the popular word2vec 1 tool proposed by mikolov et al to extract the vector representations of words .
stance detection is a difficult task since it often requires reasoning in order to determine whether an utterance is in favor of or against a specific issue .
we used a phrase-based smt model as implemented in the moses toolkit .
socher et al proposed the recursive neural network that has been proven to be efficient in terms of constructing sentences representations .
finkel and manning also proposed a parsing model for the extraction of nested named entity mentions , which , like this work , parses just the corresponding semantic annotations .
we translated each german sentence using the moses statistical machine translation toolkit .
socher et al introduced a family of recursive neural networks to represent sentence-level semantic composition .
within this subpart of our ensemble model , we used a svm model from the scikit-learn library .
we used the 300-dimensional glove word embeddings learned from 840 billion tokens in the web crawl data , as general word embeddings .
lastly , we populate the adjacency with a distributional similarity measure based on word2vec .
dependency parsing is a simpler task than constituent parsing , since dependency trees do not have extra non-terminal nodes and there is no need for a grammar to generate them .
therefore , we adopt the greedy feature selection algorithm as described in jiang et al to pick up positive features incrementally according to their contributions .
we used moses , a state-of-the-art phrase-based smt model , in decoding .
we train a 4-gram language model on the xinhua portion of english gigaword corpus by srilm toolkit .
since segmentation is the first stage of discourse parsing , quality discourse segments are critical to building quality discourse representations ( cite-p-12-1-10 ) .
cussens and pulman describe a symbolic approach which employs inductive logic programming and barg and walther and fouvry follow a unification-based approach .
readability can be used to provide satisfiable services in text recommendation or text visualization .
we used scikit-learn library for all the machine learning models .
we present our method for initializing a plsa model using lsa model .
morphological tagging is the process of labeling each word token with its morphological attributes .
we use case-insensitive bleu as evaluation metric .
gu et al , cheng and lapata , and nallapati et al also utilized seq2seq based framework with attention modeling for short text or single document summarization .
word sense disambiguation ( wsd ) is a problem of finding the relevant clues in a surrounding context .
for this task , we used the svm implementation provided with the python scikit-learn module .
since segmentation is the first stage of discourse parsing , quality discourse segments are critical to building quality discourse representations ( cite-p-12-1-10 ) .
coreference resolution is a set partitioning problem in which each resulting partition refers to an entity .
again , shen et al explore a dependency language model to improve translation quality .
word embedding has shown promising results in variety of the nlp applications , such as named entity recognition , sentiment analysis and parsing .
to get a dictionary of word embeddings , we use the word2vec tool 2 and train it on the chinese gigaword corpus .
relation extraction is the task of detecting and classifying relationships between two entities from text .
twitter 1 is a microblogging service , which according to latest statistics , has 284 million active users , 77 % outside the us that generate 500 million tweets a day in 35 different languages .
irony is a form of figurative language , considered as “ saying the opposite of what you mean ” , where the opposition of literal and intended meanings is very clear ( cite-p-23-1-1 , cite-p-23-3-8 ) .
xu et al used the knowledge graph to advance the learning of word embeddings .
we estimated 5-gram language models using the sri toolkit with modified kneser-ney smoothing .
the probabilistic language model is constructed on google web 1t 5-gram corpus by using the srilm toolkit .
we use the sri language modeling toolkit for language modeling .
we use the scikit-learn toolkit as our underlying implementation .
we use sri language model toolkit to train a 5-gram model with modified kneser-ney smoothing on the target-side training corpus .
we have used latent dirichlet allocation model as our main topic modeling tool .
we use glove pre-trained word embeddings , a 100 dimension embedding layer that is followed by a bilstm layer of size 32 .
word alignment is the problem of annotating parallel text with translational correspondence .
a context-free grammar ( cfg ) is a 4-tuple math-w-3-1-1-9 where math-w-3-1-1-21 and math-w-3-1-1-23 are finite disjoint sets of nonterminal and terminal symbols , respectively , math-w-3-1-1-36 is the start symbol and math-w-3-1-1-44 is a finite set of rules .
named entity recognition is a well established information extraction task with many state of the art systems existing for a variety of languages .
for the tf representation , we use the countvectorizer class from scikit-learn to process the text and create the appropriate representation .
the distributed word representation by word2vec factors word distance and captures semantic similarities through vector arithmetic .
all source-target sentences were parsed with the stanford parser in order to label the text with syntactic information .
the uima project provides an infrastructure to store unstructured documents .
we use the maximum entropy model for our classification task .
relation extraction ( re ) is the task of extracting instances of semantic relations between entities in unstructured data such as natural language text .
for feature building , we use word2vec pre-trained word embeddings .
we implement classification models using keras and scikit-learn .
part-of-speech ( pos ) tagging is a fundamental natural-language-processing problem , and pos tags are used as input to many important applications .
twitter is a very popular micro blogging site .
we measured translation performance with bleu .
lda is a probabilistic model that can be used to model and discover underlying topic structures of documents .
the system was general-domain oriented and it was tuned by using mert with a combination of six in-domain development datasets .
we use the stanford corenlp for obtaining pos tags and parse trees from our data .
translation quality is evaluated by case-insensitive bleu-4 metric .
a 4-gram language model was trained on the target side of the parallel data using the srilm toolkit .
the language models were interpolated kneser-ney discounted trigram models , all constructed using the srilm toolkit .
we use srilm toolkit to train a trigram language model with modified kneser-ney smoothing on the target side of training corpus .
for support vector learning , we use svm-light and svm-multiclass .
we use a popular word2vec neural language model to learn the word embeddings on an unsupervised tweet corpus .
coreference resolution is the process of linking multiple mentions that refer to the same entity .
takamura et al used the spin model to extract word semantic orientation .
we use sri language model toolkit to train a 5-gram model with modified kneser-ney smoothing on the target-side training corpus .
for phrase-based smt translation , we used the moses decoder and its support training scripts .
we train a kn-smoothed 5-gram language model on the target side of the parallel training data with srilm .
word sense disambiguation ( wsd ) is a difficult natural language processing task which requires that for every content word ( noun , adjective , verb or adverb ) the appropriate meaning is automatically selected from the available sense inventory 1 .
results were evaluated with both bleu and nist metrics .
for word embeddings , we used popular pre-trained word vectors from glove .
sentence compression is a text-to-text generation task in which an input sentence must be transformed into a shorter output sentence which accurately reflects the meaning in the input and also remains grammatically well-formed .
a 5-gram language model was built using srilm on the target side of the corresponding training corpus .
zeng et al proposed a deep convolutional neural network with softmax classification , extracting lexical and sentence level features .
we preprocess the data using standard nlp packages to tokenize , stem , and pos tag the words .
we primarily used the charniak-johnson generative parser to parse the english europarl data and the test data .
sentiment analysis ( sa ) is a field of knowledge which deals with the analysis of people ’ s opinions , sentiments , evaluations , appraisals , attitudes and emotions towards particular entities ( liu , 2012 ) .
choi and cardie present a more lightweight approach using compositional semantics towards classifying the polarity of expressions .
word sense disambiguation ( wsd ) is a key enabling technology that automatically chooses the intended sense of a word in context .
phrase-based models have until recently been a stateof-the-art method for statistical machine translation , and moses is one of the most used phrase-based translation systems .
this type of features are based on a trigram model with kneser-ney smoothing .
semantic role labeling ( srl ) is a kind of shallow semantic parsing task and its goal is to recognize some related phrases and assign a joint structure ( who did what to whom , when , where , why , how ) to each predicate of a sentence ( cite-p-24-3-4 ) .
the parameter weights are optimized with minimum error rate training .
the learning rate is automatically tuned by adam .
we found that using adagrad to update the parameters is very effective .
previous work consistently reported that word-based translation models yielded better performance than traditional methods for question retrieval .
semantic parsing is the task of mapping a natural language ( nl ) sentence into a complete , formal meaning representation ( mr ) which a computer program can execute to perform some task , like answering database queries or controlling a robot .
we evaluated translation quality with the case-insensitive bleu-4 and nist .
recently , deep learning has also been introduced to propose an end-to-end convolutional neural network for relation classification .
conditional random fields are a class of undirected graphical models with exponent distribution .
our trigram word language model was trained on the target side of the training corpus using the srilm toolkit with modified kneser-ney smoothing .
we then learn reranking weights using minimum error rate training on the development set for this combined list , using only these two features .
for training the model , we use the linear kernel svm implemented in the scikit-learn toolkit .
a ∗ parsing algorithm is 5 times faster than cky parsing , without loss of accuracy .
for all data sets , we trained a 5-gram language model using the sri language modeling toolkit .
twitter is a well-known social network service that allows users to post short 140 character status update which is called “ tweet ” .
for nb and svm , we used their implementation available in scikit-learn .
we used the implementation of the scikit-learn 2 module .
zeng et al introduce a convolutional neural network to extract relational facts with automatically learning features from text .
we solve this sequence tagging problem using the mallet implementation of conditional random fields .
we use srilm for training the 5-gram language model with interpolated modified kneser-ney discounting .
text summarization is the task of automatically condensing a piece of text to a shorter version while maintaining the important points .
we use the selectfrommodel 4 feature selection method as implemented in scikit-learn .
for word-level embedding e w , we utilize pre-trained , 300-dimensional embedding vectors from glove 6b .
semantic role labeling ( srl ) is the task of labeling the predicate-argument structures of sentences with semantic frames and their roles ( cite-p-18-1-2 , cite-p-18-1-19 ) .
relation extraction ( re ) is the task of assigning a semantic relationship between a pair of arguments .
the translation quality is evaluated by case-insensitive bleu and ter metrics using multeval .
moses is used as a baseline phrase-based smt system .
word sense disambiguation ( wsd ) is a difficult natural language processing task which requires that for every content word ( noun , adjective , verb or adverb ) the appropriate meaning is automatically selected from the available sense inventory 1 .
the target language model is built on the target side of the parallel data with kneser-ney smoothing using the irstlm tool .
we estimated 5-gram language models using the sri toolkit with modified kneser-ney smoothing .
twitter is a microblogging site where people express themselves and react to content in real-time .
meanwhile , we adopt glove pre-trained word embeddings 5 to initialize the representation of input tokens .
sentiment lexicon is a set of words ( or phrases ) each of which is assigned with a sentiment polarity score .
word embeddings have also been used in several nlp tasks including srl .
distributional semantic models represent the meanings of words by relying on their statistical distribution in text .
sentiment analysis is a collection of methods and algorithms used to infer and measure affection expressed by a writer .
word segmentation is the foremost obligatory task in almost all the nlp applications where the initial phase requires tokenization of input into words .
we used moses as the implementation of the baseline smt systems .
the annotation scheme is derived from the universal stanford dependencies , the google universal part-of-speech tags and the interset interlingua for morphological tagsets .
hochreiter and schmidhuber proposed long short-term memories as the specific version of rnn designed to overcome vanishing and exploding gradient problem .
for testing purposes , we used the wall street journal part of the penn treebank corpus .
we implement our lstm encoder-decoder model using the opennmt neural machine translation toolkit .
we use glove vectors with 100 dimensions trained on wikipedia and gigaword as word embeddings , which we do not optimize during training .
parsers trained on the penn treebank are reporting impressive numbers these days , but they don ’ t do very well on this problem .
in this work , we apply several unsupervised and supervised techniques of sentiment composition .
we built a 5-gram language model from it with the sri language modeling toolkit .
we used the sri language modeling toolkit to train a fivegram model with modified kneser-ney smoothing .
for example , blitzer et al proposed a domain adaptation method based on structural correspondence learning .
mimno et al similarly introduced a methodology for computing coherence , replacing pmi with log conditional probability .
ccg is a lexicalized , mildly context-sensitive parsing formalism that models a wide range of linguistic phenomena .
we used the moses tree-to-string mt system for all of our mt experiments .
in our experiments , we used the srilm toolkit to build 5-gram language model using the ldc arabic gigaword corpus .
we adopt the domain-adaptation method used by luong and manning to fine-tune the trained model using in-domain data .
sentiment analysis is a technique to classify documents based on the polarity of opinion expressed by the author of the document ( cite-p-16-1-13 ) .
the word vectors are learned using a skip-gram model with negative sampling , implemented in the word2vec toolkit .
coreference resolution is the task of automatically grouping references to the same real-world entity in a document into a set .
coreference resolution is a set partitioning problem in which each resulting partition refers to an entity .
we built a hierarchical phrase-based mt system based on weighted scfg .
stance detection is the task of assigning stance labels to a piece of text with respect to a topic , i.e . whether a piece of text is in favour of “ abortion ” , neutral , or against .
word sense disambiguation ( wsd ) is the problem of assigning a sense to an ambiguous word , using its context .
we use the 200-dimensional global vectors , pre-trained on 2 billion tweets , covering over 27-billion tokens .
we initialize the embedding layer using embeddings from dedicated word embedding techniques word2vec and glove .
sentiment classification is the task of identifying the sentiment polarity of a given text , which is traditionally categorized as either positive or negative .
therefore , we use em-based estimation for the hidden parameters .
latent dirichlet allocation is a generative model that overcomes some of the limitations of plsi by using a dirichlet prior on the topic distribution .
the use of unsupervised word embeddings in various natural language processing tasks has received much attention .
we map the pos labels in the conll datasets to the universal pos tagset .
in natural language , a word often assumes different meanings , and the task of determining the correct meaning , or sense , of a word in different contexts is known as word sense disambiguation ( wsd ) .
word segmentation is a fundamental task for processing most east asian languages , typically chinese .
sentiment classification is the task of identifying the sentiment polarity of a given text , which is traditionally categorized as either positive or negative .
semantic role labeling ( srl ) consists of finding the arguments of a predicate and labeling them with semantic roles ( cite-p-9-1-5 , cite-p-9-3-0 ) .
sentence compression is a text-to-text generation task in which an input sentence must be transformed into a shorter output sentence which accurately reflects the meaning in the input and also remains grammatically well-formed .
evaluation demonstrates that text generated by our model is preferred over that of baselines .
barzilay and mckeown extracted both single-and multiple-word paraphrases from a sentence-aligned corpus for use in multi-document summarization .
as a baseline smt system , we use the hierarchical phrase-based translation with an efficient left-to-right generation originally proposed by chiang .
from this , we extract an old domain sense dictionary , using the moses mt framework .
relation extraction is a challenging task in natural language processing .
dependency parsing is a valuable form of syntactic processing for nlp applications due to its transparent lexicalized representation and robustness with respect to flexible word order languages .
all smt models were developed using the moses phrase-based mt toolkit and the experiment management system .
we use pre-trained vectors from glove for word-level embeddings .
word alignment , which can be defined as an object for indicating the corresponding words in a parallel text , was first introduced as an intermediate result of statistical translation models ( cite-p-13-1-2 ) .
we use glove 300-dimension embedding vectors pre-trained on 840 billion tokens of web data .
sentiment analysis is the study of the subjectivity and polarity ( positive vs. negative ) of a text ( cite-p-7-1-10 ) .
we used the sri language modeling toolkit to train a fivegram model with modified kneser-ney smoothing .
we use pre-trained vectors from glove for word-level embeddings .
sentiment analysis is a nlp task that deals with extraction of opinion from a piece of text on a topic .
zeng et al use convolutional neural network for learning sentence-level features of contexts and obtain good performance even without using syntactic features .
we use srilm toolkit to build a 5-gram language model with modified kneser-ney smoothing .
we use the logistic regression implementation of liblinear wrapped by the scikit-learn library .
we trained word embeddings using word2vec on 4 corpora of different sizes and types .
a pseudoword is a composite comprised of two or more words chosen at random ; the individual occurrences of the original words within a text are replaced by their conflation .
the method of bannard and callison-burch requires bilingual parallel corpora , and uses the translations of expressions as its feature .
we compare our approach with a standard phrase-based mt system , moses trained using the same 1m sequence pairs constructed from the wikianswers dataset .
we have described a perceptron-style algorithm for training the neural networks , which is much easier to be implemented , and has speed advantage over the maximum-likelihood scheme .
and thus this study aims to examine the use of content features in speech scoring systems .
we first train a word2vec model on fr-wikipedia 11 to obtain non contextual word vectors .
the process of identifying the correct meaning , or sense of a word in context , is known as word sense disambiguation ( wsd ) .
we adapted the moses phrase-based decoder to translate word lattices .
issue framing is related to both analyzing biased language and subjectivity .
we assume that a morphological analysis consists of three processes : tokenization , dictionary lookup , and disambiguation .
the decoder uses a ckystyle parsing algorithm and cube pruning to integrate the language model scores .
we use a standard long short-term memory model to learn the document representation .
we use the moses software to train a pbmt model .
bleu is a system for automatic evaluation of machine translation .
table 2 presents the results from the automatic evaluation , in terms of bleu and nist scores , of 4 system setups .
we trained a 4-gram language model with kneser-ney smoothing and unigram caching using the sri-lm toolkit .
to obtain their corresponding weights , we adapted the minimum-error-rate training algorithm to train the outside-layer model .
semantic role labeling ( srl ) is the task of labeling the predicate-argument structures of sentences with semantic frames and their roles ( cite-p-18-1-2 , cite-p-18-1-19 ) .
preparing an aligned abbreviation corpus , we obtain the optimal combination of the features by using the maximum entropy framework .
to their syntactic expressions , these mixed feature sets are potentially useful for building verb classifications .
coreference resolution is the next step on the way towards discourse understanding .
we use the pre-trained word2vec embeddings provided by mikolov et al as model input .
the long short-term memory was first proposed by hochreiter and schmidhuber that can learn long-term dependencies .
as a sequence labeler we use conditional random fields .
in this paper , we describe our contribution at task 2 of semeval 2013 .
we ran mt experiments using the moses phrase-based translation system .
finally , goldwasser et al presented an unsupervised approach of learning a semantic parser by using an em-like retraining loop .
the parsing was performed with the berkeley parser and features were extracted from both source and target .
we train a 4-gram language model on the xinhua portion of the gigaword corpus using the sri language toolkit with modified kneser-ney smoothing .
the language models used were 7-gram srilm with kneser-ney smoothing and linear interpolation .
the latent descriptor for math-w-2-4-3-110 consists of the pair ( math-w-2-4-3-116 ) , math-w-2-4-3-122 ) ) ¡ª .
phonetic translation across these pairs is called transliteration .
bagga and baldwin used the vector space model together with summarization techniques to tackle the cross-document coreference problem .
to train the link embeddings , we use the speedy , skip-gram neural language model of mikolov et al via their toolkit word2vec .
we used the moses pbsmt system for all of our mt experiments .
we measure translation quality via the bleu score .
our 5-gram language model was trained by srilm toolkit .
in our work , we use latent dirichlet allocation to identify the sub-topics in the given body of texts .
costa-juss脿 and fonollosa , 2006 ) consider part-of-speech based source reordering as a translation task .
as textual features , we use the pretrained google news word embeddings , obtained by training the skip-gram model with negative sampling .
labeled data for msa we use the penn arabic treebank .
in 2013 , mikolov et al generated phrase representation using the same method used for word representation in word2vec .
we used the svd implementation provided in the scikit-learn toolkit .
syntactic parsing is the task of identifying the phrases and clauses in natural language sentences .
we used svm classifier that implements linearsvc from the scikit-learn library .
for the token-level sequence labeling tasks we use hidden markov models and conditional random fields appear sentences .
in order to tune all systems , we use the k-best batch mira .
we used the google news pretrained word2vec word embeddings for our model .
following och and ney , we adopt a general loglinear model .
we use the english penn treebank to evaluate our model implementations and yamada and matsumoto head rules are used to extract dependency trees .
statistical significance is computed using the bootstrap re-sampling approach proposed by koehn .
sentiment analysis is the process of identifying and extracting subjective information using natural language processing ( nlp ) .
we use the stanford nlp pos tagger to generate the tagged text .
we tune the systems using minimum error rate training .
the language model pis implemented as an n-gram model using the srilm-toolkit with kneser-ney smoothing .
ganchev et al propose a posterior regularization framework for weakly supervised learning to derive a multi-view learning algorithm .
mihalcea et al compared knowledgebased and corpus-based methods , using word similarity and word specificity to define one general measure of text semantic similarity .
to solve the traditional recurrent neural networks , hochreiter and schmidhuber proposed the lstm architecture .
coreference resolution is a key task in natural language processing ( cite-p-13-1-8 ) aiming to detect the referential expressions ( mentions ) in a text that point to the same entity .
the ape system for each target language was tuned on comparable development sets , optimizing ter with minimum error rate training .
the evaluation metric is case-sensitive bleu-4 .
word sense disambiguation ( wsd ) is the problem of assigning a sense to an ambiguous word , using its context .
bilingual lexicons are an important resource in multilingual natural language processing tasks such as statistical machine translation and cross-language information retrieval .
word vectors are vector representations of the words learned from their raw form , using models such as word2vec .
in this task , we use the 300-dimensional 840b glove word embeddings .
the evaluation metric for the overall translation quality was case-insensitive bleu4 .
the phrase-based translation model uses the con- the baseline lm was a regular n-gram lm with kneser-ney smoothing and interpolation by means of the srilm toolkit .
markov models were trained with modified kneser-ney smoothing as implemented in srilm .
coreference resolution is a central problem in natural language processing with a broad range of applications such as summarization ( cite-p-16-3-24 ) , textual entailment ( cite-p-16-3-12 ) , information extraction ( cite-p-16-3-11 ) , and dialogue systems ( cite-p-16-3-25 ) .
rhetorical structure theory is one of the most influential approaches for document-level discourse analysis .
sentiment analysis ( sa ) is the task of prediction of opinion in text .
we used standard classifiers available in scikit-learn package .
we used the disambig tool provided by the srilm toolkit .
we selected conditional random fields as the baseline model .
with the refined outputs , we build phrasebased transliteration systems using moses , a popular statistical machine translation framework .
information extraction ( ie ) is a main nlp aspects for analyzing scientific papers , which includes named entity recognition ( ner ) and relation extraction ( re ) .
sentiment analysis is a research area where does a computational analysis of people ’ s feelings or beliefs expressed in texts such as emotions , opinions , attitudes , appraisals , etc . ( cite-p-12-1-3 ) .
we used the srilm toolkit to simulate the behavior of flexgram models by using count files as input .
mikolov et al uses a continuous skip-gram model to learn a distributed vector representation that captures both syntactic and semantic word relationships .
discourse parsing is the process of assigning a discourse structure to the input provided in the form of natural language .
we use sri language modeling toolkit to train a 5-gram language model on the english sentences of fbis corpus .
we use the sri language model toolkit to train a 5-gram model with modified kneser-ney smoothing on the target-side training corpus .
we use support vector machines , a maximum-margin classifier that realizes a linear discriminative model .
the model weights are automatically tuned using minimum error rate training .
we used the sri language modeling toolkit to train lms on our training data for each ilr level .
we run parfda smt experiments using moses in all language pairs in wmt15 and obtain smt performance close to the top constrained moses systems .
the english side of the parallel corpus is trained into a language model using srilm .
we use the english penn treebank to evaluate our model implementations and yamada and matsumoto head rules are used to extract dependency trees .
the word embeddings are initialized with 100-dimensions vectors pre-trained by the cbow model .
we train a 4-gram language model on the xinhua portion of english gigaword corpus by srilm toolkit .
word alignment is a central problem in statistical machine translation ( smt ) .
the pre-processed monolingual sentences will be used by srilm or berkeleylm to train a n-gram language model .
to set the weights , 位 m , we performed minimum error rate training on the development set using bleu as the objective function .
we use srilm toolkit to build a 5-gram language model with modified kneser-ney smoothing .
as a classifier , we choose a first-order conditional random field model .
we used scikit-learn library for all the machine learning models .
our nmt baseline is an encoder-decoder model with attention and dropout implemented with nematus and amunmt .
we use a phrase-based translation system similar to moses .
twitter is a social platform which contains rich textual content .
we now review the path ranking algorithm introduced by lao and cohen .
we define a conditional random field for this task .
we used glove word embeddings with 300 dimensions pre-trained using commoncrawl to get a vector representation of the evidence sentence .
for this paper , we directly utilize the pre-trained fasttext word embeddings model which is trained on wikipedia data .
for all methods , we applied dropout to the input of the lstm layers .
these approaches range from distortion models to lexical reordering models .
we use 2-best parse trees of berkeley parser and 1-best parse tree of bikel parser and stanford parser as inputs to the full parsing based system .
marcu and echihabi propose an approach considering word-based pairs as useful features .
zeng et al proposed a deep convolutional neural network with softmax classification , extracting lexical and sentence level features .
we use srilm for training the 5-gram language model with interpolated modified kneser-ney discounting .
the objective measures used were the bleu score , the nist score and multi-reference word error rate .
we used the implementation of the scikit-learn 2 module .
reisinger and mooney and huang et al use context clustering to induce multiple word senses for a target word type , where each sense is represented by a different context feature vector .
we used the target side of the parallel corpus and the srilm toolkit to train a 5-gram language model .
in particular , neural language models have demonstrated impressive performance at the task of language modeling .
we train a 4-gram language model on the xinhua portion of the gigaword corpus using the sri language toolkit with modified kneser-ney smoothing .
that suggests that original texts are significantly different from translated ones in various aspects .
galley and manning use the shift-reduce algorithm to conduct hierarchical phrase reordering so as to capture long-distance reordering .
word sense disambiguation ( wsd ) is the task of identifying the correct meaning of a word in context .
the anaphor is a pronoun and the referent is in the cache ( in focus ) .
we used trigram language models with interpolated kneser-kney discounting trained using the sri language modeling toolkit .
visual question answering ( vqa ) is the task of predicting a suitable answer given an image and a question about it .
for input representation , we used glove word embeddings .
discourse parsing is a difficult , multifaceted problem involving the understanding and modeling of various semantic and pragmatic phenomena as well as understanding the structural properties that a discourse graph can have .
in addition , we use an english corpus of roughly 227 million words to build a target-side 5-gram language model with srilm in combination with kenlm .
information extraction ( ie ) is the task of extracting factual assertions from text .
we use srilm to train a 5-gram language model on the target side of our training corpus with modified kneser-ney discounting .
dhingra et al proposed a multi-turn dialogue agent which helps users search knowledge base by soft kb lookup .
the probabilistic language model is constructed on google web 1t 5-gram corpus by using the srilm toolkit .
table 4 shows the bleu scores of the output descriptions .
our translation system is an in-house phrasebased system analogous to moses .
for french , we used 300,000 parallel sentences from the europarl training data parsed on the english side with the stanford parser and on the french side with the xerox xip parser .
using word or phrase representations as extra features has been proven to be an effective and simple way to improve the predictive performance of an nlp system .
named entity recognition ( ner ) is the task of identifying named entities in free text—typically personal names , organizations , gene-protein entities , and so on .
we use the term-sentence matrix to train a simple generative topic model based on lda .
1 a bunsetsu is the linguistic unit in japanese that roughly corresponds to a basic phrase in english .
lda is a generative model that learns a set of latent topics for a document collection .
trigram language models are implemented using the srilm toolkit .
sentiment analysis is a multi-faceted problem .
the minimum error rate training was used to tune the feature weights .
first , arabic is a morphologically rich language ( cite-p-19-3-7 ) .
relation extraction is a fundamental step in many natural language processing applications such as learning ontologies from texts ( cite-p-12-1-0 ) and question answering ( cite-p-12-3-6 ) .
for word embedding , we used pre-trained glove word vectors with 300 dimensions , and froze them during training .
collobert et al used word embeddings as inputs of a multilayer neural network for part-of-speech tagging , chunking , named entity recognition and semantic role labelling .
the method proposed by huang et al incorporates the sinica word segmentation system to detect typos .
we use scikitlearn as machine learning library .
for regularization , dropout is applied to the input and hidden layers .
we regularize our network using dropout with the drop-out rate tuned using development set .
for input representation , we used glove word embeddings .
luong et al break words into morphemes , and use recursive neural networks to compose word meanings from morpheme meanings .
we use the 100-dimensional glove 4 embeddings trained on 2 billions tweets to initialize the lookup table and do fine-tuning during training .
relation extraction ( re ) is a task of identifying typed relations between known entity mentions in a sentence .
instead , we use bleu scores since it is one of the primary metrics for machine translation evaluation .
we use pre-trained 50 dimensional glove vectors 4 for word embeddings initialization .
the language model was constructed using the srilm toolkit with interpolated kneser-ney discounting .
in this paper we attempt to deliver a framework useful for analyzing text in blogs .
parameter tuning was carried out using both k-best mira and minimum error rate training on a held-out development set .
the standard classifiers are implemented with scikit-learn .
we implemented the algorithms in python using the stochastic gradient descent method for nmf from the scikit-learn package .
for the neural models , we use 100-dimensional glove embeddings , pre-trained on wikipedia and gigaword .
further , we apply a 4-gram language model trained with the srilm toolkit on the target side of the training corpus .
we use the datasets , experimental setup , and scoring program from the conll 2011 shared task , based on the ontonotes corpus .
the language model used was a 5-gram with modified kneserney smoothing , built with srilm toolkit .
for the cluster- based method , we use word2vec 2 which provides the word vectors trained on the google news corpus .
this baseline uses pre-trained word embeddings using word2vec cbow and fasttext .
coreference resolution is the process of linking together multiple expressions of a given entity .
we train a 4-gram language model on the xinhua portion of the gigaword corpus using the sri language toolkit with modified kneser-ney smoothing .
sentiment analysis is the computational analysis of people ’ s feelings or beliefs expressed in texts such as emotions , opinions , attitudes , appraisals , etc . ( cite-p-11-3-3 ) .
we created 5-gram language models for every domain using srilm with improved kneserney smoothing on the target side of the training parallel corpora .
among them , twitter is the most popular service by far due to its ease for real-time sharing of information .
lda is a probabilistic model of text data which provides a generative analog of plsa , and is primarily meant to reveal hidden topics in text documents .
in our wok , we have used the stanford log-linear part-of-speech to do pos tagging .
sentiment classification is a hot research topic in natural language processing field , and has many applications in both academic and industrial areas ( cite-p-17-1-16 , cite-p-17-1-12 , cite-p-17-3-4 , cite-p-17-3-3 ) .
we use glove vectors with 100 dimensions trained on wikipedia and gigaword as word embeddings , which we do not optimize during training .
part-of-speech ( pos ) tagging is a fundamental nlp task , used by a wide variety of applications .
among them , twitter is the most popular service by far due to its ease for real-time sharing of information .
table 4 shows the bleu scores of the output descriptions .
in our experiments , we used the srilm toolkit to build 5-gram language model using the ldc arabic gigaword corpus .
multi-task learning has been used with success in applications of machine learning , from natural language processing and speech recognition .
unlike lemma prediction , we use a liblinear classifier to build linear svm classification models for gnp and case prediction .
we used 4-gram language models , trained using kenlm .
coreference resolution is the task of clustering a sequence of textual entity mentions into a set of maximal non-overlapping clusters , such that mentions in a cluster refer to the same discourse entity .
sentiment analysis is the task of identifying the polarity ( positive , negative or neutral ) of review .
to evaluate the quality of our generated summaries , we choose to use the rouge 3 evaluation toolkit , that has been found to be highly correlated with human judgments .
our system is based on the phrase-based part of the statistical machine translation system moses .
coreference resolution is a key task in natural language processing ( cite-p-13-1-8 ) aiming to detect the referential expressions ( mentions ) in a text that point to the same entity .
translation performances are measured with case-insensitive bleu4 score .
we use the skipgram model with negative sampling to learn word embeddings on the twitter reference corpus .
the target language model was a trigram language model with modified kneser-ney smoothing trained on the english side of the bitext using the srilm tookit .
we used yamcha 1 , which is a general purpose svm-based chunker .
following mirza and tonelli , we use the three million 300-dimensional word2vec vectors 5 pre-trained on part of the google news dataset .
we obtained these scores by training a word2vec model on the wiki corpus .
part-of-speech tagging is a crucial preliminary process in many natural language processing applications .
relation extraction is a core task in information extraction and natural language understanding .
we further used a 5-gram language model trained using the srilm toolkit with modified kneser-ney smoothing .
semantic parsing is the task of transducing natural language ( nl ) utterances into formal meaning representations ( mrs ) , commonly represented as tree structures .
in this article , we present a framework to recommend relevant information in internet forums and blogs .
sentence compression is the task of compressing long , verbose sentences into short , concise ones .
we used the moseschart decoder and the moses toolkit for tuning and decoding .
to identify content words , we used the nltk-lite tagger to assign a part of speech to each word .
as a classifier we use an svm as implemented in svm light .
we used a 5-gram language model with modified kneser-ney smoothing implemented using the srilm toolkit .
we initialize the word embedding matrix with pre-trained glove embeddings .
zhu et al suggest a probabilistic , syntaxbased approach to text simplification .
named entity recognition ( ner ) is the first step for many tasks in the fields of natural language processing and information retrieval .
distributed representations for words and sentences have been shown to significantly boost the performance of a nlp system .
for the sick and msrvid experiments , we used 300-dimension glove word embeddings .
results are reported using case-insensitive bleu with a single reference .
a 4-gram language model is trained on the monolingual data by srilm toolkit .
we learn word and sentence embeddings jointly by training a multilingual skip-gram model together with a cross-lingual sentence similarity model .
the language models are 4-grams with modified kneser-ney smoothing which have been trained with the srilm toolkit .
pichotta and mooney showed that the lstm-based event sequence model outperformed previous co-occurrence-based methods for event prediction .
we created 5-gram language models for every domain using srilm with improved kneserney smoothing on the target side of the training parallel corpora .
we follow honnibal et al in using the dynamic oracle-based search-and-learn training strategy introduced by goldberg and nivre .
we trained linear-chain conditional random fields as the baseline .
improvements with additional measures always increases the overall reliability of the evaluation process .
major discourse annotated resources in english include the rst treebank and the penn discourse treebank .
relation extraction ( re ) is the task of recognizing the assertion of a particular relationship between two or more entities in text .
the log-linear parameter weights are tuned with mert on the development set .
we apply srilm to train the 3-gram language model of target side .
since sarcasm is a refined and indirect form of speech , its interpretation may be challenging for certain populations .
according to lakoff and johnson , humans use one concept in metaphors to describe another concept for reasoning and communication .
twitter is the medium where people post real time messages to discuss on the different topics , and express their sentiments .
transliteration is a process of translating a foreign word into a native language by preserving its pronunciation in the original language , otherwise known as translationby-sound .
linguistically , metaphor is defined as a language expression that uses one or several words to represent another concept , rather than taking their literal meanings of the given words in the context ( cite-p-14-1-6 ) .
we used kenlm with srilm to train a 5-gram language model based on all available target language training data .
morphological analysis is the first step for most natural language processing applications .
we use pre-trained glove vector for initialization of word embeddings .
for representing words , we used 100 dimensional pre-trained glove embeddings .
exact bleu and ter scores of the optimum on dev and the baseline are given in table 2 .
in which a one-to-one topic correspondence is enforced between the lsa models .
noun phrase can refer to the entity denoted by a noun phrase that has already appeared .
in this paper , we propose a novel uncertainty classification scheme and construct the first uncertainty corpus based on social media data ¨c tweets in specific .
for this language model , we built a trigram language model with kneser-ney smoothing using srilm from the same automatically segmented corpus .
through our experimental results , we demonstrated that our approach was able to accurately predict missing topic preferences of users ( 80 ¨c 94 % ) .
we use the wn similarity jcn score on nouns since this gave reasonable results for mccarthy et al and it is efficient at run time given precompilation of frequency information .
text categorization is the classification of documents with respect to a set of predefined categories .
coreference resolution is the task of partitioning a set of mentions ( i.e . person , organization and location ) into entities .
wiebe et al use statistical methods to automatically correct the biases in annotations of speaker subjectivity .
text classification is a well-studied problem in machine learning , natural language processing , and information retrieval .
conditional random fields are a class of undirected graphical models with exponent distribution .
we use datasets of semeval-2017 ‘ fine-grained sentiment analysis on financial microblogs and news ’ shared task .
semantic parsing is the task of mapping natural language sentences to a formal representation of meaning .
our second method is based on the recurrent neural network language model approach to learning word embeddings of mikolov et al and mikolov et al , using the word2vec package .
we derive our gold standard from the semeval 2007 lexical substitution task dataset .
to minimize the objective , we use stochastic gradient descent with the diagonal variant of adagrad .
we ran mt experiments using the moses phrase-based translation system .
we trained linear-chain conditional random fields as the baseline .
for simplicity , we use the well-known conditional random fields for sequential labeling .
rambow et al addressed the challenge of summarizing entire threads by treating it as a binary sentence classification task .
this tree kernel was slightly generalized by culotta and sorensen to compute similarity between two dependency trees .
twitter is a rich resource for information about everyday events – people post their tweets to twitter publicly in real-time as they conduct their activities throughout the day , resulting in a significant amount of mundane information about common events .
domain adaptation is a common concern when optimizing empirical nlp applications .
for data preparation and processing we use scikit-learn .
part-of-speech tagging is the process of assigning to a word the category that is most probable given the sentential context ( cite-p-4-1-2 ) .
we extract the corresponding feature from the output of the stanford parser .
ccgs are a linguistically-motivated formalism for modeling a wide range of language phenomena , steedman , 1996 , steedman , 2000 .
we employ the glove and node2vec to generate the pre-trained word embedding , obtaining two distinct embedding for each word .
named entity recognition ( ner ) is a well-known problem in nlp which feeds into many other related tasks such as information retrieval ( ir ) and machine translation ( mt ) and more recently social network discovery and opinion mining .
word sense disambiguation ( wsd ) is the task to identify the intended sense of a word in a computational manner based on the context in which it appears ( cite-p-13-3-4 ) .
in particular , we consider conditional random fields and a variation of autoslog .
therefore , word segmentation is a crucial first step for many chinese language processing tasks such as syntactic parsing , information retrieval and machine translation .
we used a 5-gram language model with modified kneser-ney smoothing , built with the srilm toolkit .
we use pre-trained vectors from glove for word-level embeddings .
we trained a 4-gram language model with kneser-ney smoothing and unigram caching using the sri-lm toolkit .
named entity recognition ( ner ) is the task of identifying and typing phrases that contain the names of persons , organizations , locations , and so on .
we use the word2vec tool with the skip-gram learning scheme .
we also extract subject-verbobject event representations , using the stanford partof-speech tagger and maltparser .
these parameters are tuned using mert algorithm on development data using a criterion of accuracy maximization .
relation extraction is a challenging task in natural language processing .
the treebank consists of poems from the tang dynasty ( 618 – 907 ce ) , considered one of the crowning achievements in traditional chinese literature .
for these implementations , we use mallet and svm-light package 3 .
in the conditional distribution of a word , it is not only influenced by its context words , but also by a topic , which is an embedding vector .
we use the popular moses toolkit to build the smt system .
we use stochastic gradient descent with adagrad , l 2 regularization and minibatch training .
we use word2vec tool for learning distributed word embeddings .
rel-lda is an application of the lda topic model to the relation discovery task .
we instead use adagrad , a variant of stochastic gradient descent in which the learning rate is adapted to the data .
we parsed all corpora using the berkeley parser .
conditional random fields are undirected graphical models trained to maximize a conditional probability .
we initialize our word vectors with 300-dimensional word2vec word embeddings .
for the neural models , we use 100-dimensional glove embeddings , pre-trained on wikipedia and gigaword .
the special difficulty of this task is the length disparity between the compared pair .
bilmes and kirchhoff generalize lattice-based language models further by allowing arbitrary factors in addition to words and classes .
coreference resolution is the task of grouping all the mentions of entities 1 in a document into equivalence classes so that all the mentions in a given class refer to the same discourse entity .
to alleviate this shortcoming , we performed smoothing of the phrase table using the good-turing smoothing technique .
previous works illustrated the need for qualitative analysis to identify error sources .
for simplicity , we use the well-known conditional random fields for sequential labeling .
we train each model on the training set for 10 epochs using word-level log-likelihood , minibatches of size 50 , and the adam optimization method with the default parameters suggested by kingma and ba .
coreference resolution is the task of determining which mentions in a text are used to refer to the same real-world entity .
for all tasks , we use the adam optimizer to train models , and the relu activation function for fast calculation .
sentiment classification is a very domain-specific problem ; training a classifier using the data from one domain may fail when testing against data from another .
we used srilm to build a 4-gram language model with kneser-ney discounting .
this rough stemming is a preliminary technique , but it avoids the need for hand-crafted morphological information .
linear combinations of word embedding vectors have been shown to correspond well to the semantic composition of the individual words .
socher et al used an rnn-based architecture to generate compositional vector representations of sentences .
semantic parsing is the task of mapping a natural language query to a logical form ( lf ) such as prolog or lambda calculus , which can be executed directly through database query ( zettlemoyer and collins , 2005 , 2007 ; haas and riezler , 2016 ; kwiatkowksi et al. , 2010 ) .
metonymy is defined as the use of a word or a phrase to stand for a related concept which is not explicitly mentioned .
link grammar is a grammar theory that is strongly dependencybased .
latent dirichlet allocation is a widely adopted generative model for topic modeling .
the parameters of our mt system were tuned on a development corpus using minimum error rate training .
we used the pre-trained google embedding to initialize the word embedding matrix .
the representative ml approaches used in ner are hidden markov model , me , crfs and svm .
for the embeddings trained on stack overflow corpus , we use the word2vec implementation of gensim 8 toolkit .
we trained a 4-gram language model with kneser-ney smoothing and unigram caching using the sri-lm toolkit .
we implement our lstm encoder-decoder model using the opennmt neural machine translation toolkit .
we used srilm for training the 5-gram language model with interpolated modified kneser-ney discounting , .
we further used a 5-gram language model trained using the srilm toolkit with modified kneser-ney smoothing .
semeval 2014 is a semantic evaluation of natural language processing ( nlp ) that comprises several tasks .
relation extraction is the task of automatically detecting occurrences of expressed relations between entities in a text and structuring the detected information in a tabularized form .
we represent input words using pre-trained glove wikipedia 6b word embeddings .
we train a 4-gram language model on the xinhua portion of the gigaword corpus using the sri language toolkit with modified kneser-ney smoothing .
relation extraction is the task of automatically detecting occurrences of expressed relations between entities in a text and structuring the detected information in a tabularized form .
we therefore use kenlm to train a 6-gram language model with the monolingual data outlined in table 1 .
we trained the initial parser on the ccgbank training set , consisting of 39603 sentences of wall street journal text .
we seek to produce an automatic readability metric that is tailored to the literacy skills of adults with id .
we then lowercase all data and use all sentences from the modern dutch part of the corpus to train an n-gram language model with the srilm toolkit .
relation extraction ( re ) is the task of recognizing relationships between entities mentioned in text .
information extraction ( ie ) is a main nlp aspects for analyzing scientific papers , which includes named entity recognition ( ner ) and relation extraction ( re ) .
for the tree-based system , we applied a 4-gram language model with kneserney smoothing using srilm toolkit trained on the whole monolingual corpus .
tuning is performed to maximize bleu score using minimum error rate training .
we followed tiedemann by using linear svms implemented in liblinear .
the word vectors of vocabulary words are trained from a large corpus using the glove toolkit .
we further used a 5-gram language model trained using the srilm toolkit with modified kneser-ney smoothing .
the stanford parser was used to generate the dependency parse information for each sentence .
coreference resolution is the task of determining whether two or more noun phrases refer to the same entity in a text .
we used moses as the implementation of the baseline smt systems .
the word embeddings are word2vec of dimension 300 pre-trained on google news .
fader et al present a question answering system that learns to paraphrase a question so that it can be answered using a corpus of open ie triples .
a 4-gram language model was trained on the target side of the parallel data using the srilm toolkit from stolcke .
for all machine learning results , we train a logistic regression classifier implemented in scikitlearn with l2 regularization and the liblinear solver .
the 5-gram kneser-ney smoothed language models were trained by srilm , with kenlm used at runtime .
the log linear weights for the baseline systems are optimized using mert provided in the moses toolkit .
in our implementation , we use a kn-smoothed trigram model .
we use srilm for training the 5-gram language model with interpolated modified kneser-ney discounting .
our machine translation system is a phrase-based system using the moses toolkit .
ding and palmer propose a syntax-based translation model based on a probabilistic synchronous dependency insertion grammar .
distributed representations for words and sentences have been shown to significantly boost the performance of a nlp system .
sentence compression is the task of compressing long , verbose sentences into short , concise ones .
we train a 4-gram language model on the xinhua portion of the gigaword corpus using the sri language toolkit with modified kneser-ney smoothing .
word sense disambiguation ( wsd ) is the task of determining the correct meaning ( “ sense ” ) of a word in context , and several efforts have been made to develop automatic wsd systems .
polysemy is a major characteristic of natural languages .
relation extraction is the key component for building relation knowledge graphs , and it is of crucial significance to natural language processing applications such as structured search , sentiment analysis , question answering , and summarization .
for our baseline we use the moses software to train a phrase based machine translation model .
relation extraction ( re ) is the task of recognizing relationships between entities mentioned in text .
for the embeddings trained on stack overflow corpus , we use the word2vec implementation of gensim 8 toolkit .
we use the moses smt toolkit to test the augmented datasets .
coreference resolution is the task of determining which mentions in a text refer to the same entity .
thus , we pre-train the embeddings on a huge unlabeled data , the chinese wikipedia corpus , with word2vec toolkit .
the target-side language models were estimated using the srilm toolkit .
stance detection is the task of automatically determining from text whether the author of the text is in favor of , against , or neutral towards a proposition or target .
coreference resolution is the task of grouping mentions to entities .
table 2 summarizes machine translation performance , as measured by bleu , calculated on the full corpus with the systems resulting from each iteration .
neural models , with various neural architectures , have recently achieved great success .
relation extraction is a traditional information extraction task which aims at detecting and classifying semantic relations between entities in text ( cite-p-10-1-18 ) .
we use srilm for training the 5-gram language model with interpolated modified kneser-ney discounting .
relation extraction is the task of finding relational facts in unstructured text and putting them into a structured ( tabularized ) knowledge base .
sentiment analysis is a natural language processing ( nlp ) task ( cite-p-10-1-14 ) which aims at classifying documents according to the opinion expressed about a given subject ( federici and dragoni , 2016a , b ) .
experiments have shown that word embedding models are superior to conventional distributional models .
traditional semantic space models represent meaning on the basis of word co-occurrence statistics in large text corpora .
to get the the sub-fields of the community , we use latent dirichlet allocation to find topics and label them by hand .
shallow semantic representations could prevent the sparseness of deep structural approaches and the weakness of bow models .
text segmentation can be defined as the automatic identification of boundaries between distinct textual units ( segments ) in a textual document .
luong et al break words into morphemes , and use recursive neural networks to compose word meanings from morpheme meanings .
to do so , we utilized the popular latent dirichlet allocation , topic modeling method .
we used the sri language modeling toolkit to train a fivegram model with modified kneser-ney smoothing .
word alignment is the problem of annotating parallel text with translational correspondence .
we present a novel approach to fsd , which operates in constant time / space .
twitter is a well-known social network service that allows users to post short 140 character status update which is called “ tweet ” .
we use sri language model toolkit to train a 5-gram model with modified kneser-ney smoothing on the target-side training corpus .
relation extraction is a subtask of information extraction that finds various predefined semantic relations , such as location , affiliation , rival , etc. , between pairs of entities in text .
additionally , a back-off 2-gram model with goodturing discounting and no lexical classes was built from the same training data , using the srilm toolkit .
for all the systems we train , we build n-gram language model with modified kneserney smoothing using kenlm .
twitter is a popular microblogging service , which , among other things , is used for knowledge sharing among friends and peers .
semantic role labeling ( srl ) is the task of identifying the semantic arguments of a predicate and labeling them with their semantic roles .
tuning is performed to maximize bleu score using minimum error rate training .
semantic parsing is a domain-dependent process by nature , as its output is defined over a set of domain symbols .
word alignment is a fundamental problem in statistical machine translation .
in our experiments we use word2vec as a representative scalable model for unsupervised embeddings .
we use the popular word2vec 1 tool proposed by mikolov et al to extract the vector representations of words .
we use the skipgram model with negative sampling to learn word embeddings on the twitter reference corpus .
we initialize our word vectors with 300-dimensional word2vec word embeddings .
we use srilm train a 5-gram language model on the xinhua portion of the english gigaword corpus 5th edition with modified kneser-ney discounting .
relation extraction is the task of automatically detecting occurrences of expressed relations between entities in a text and structuring the detected information in a tabularized form .
for tagging , we use the stanford pos tagger package .
gru has been shown to achieve comparable performance with less parameters than lstm .
as a further test , we ran the stanford parser on the queries to generate syntactic parse trees .
we compare the final system to moses 3 , an open-source translation toolkit .
we use the word2vec tool to train monolingual vectors , 6 and the cca-based tool for projecting word vectors .
in this task , we use the 300-dimensional 840b glove word embeddings .
coreference resolution is the process of linking multiple mentions that refer to the same entity .
relation extraction is a crucial task in the field of natural language processing ( nlp ) .
we used minimum error rate training mert for tuning the feature weights .
word sense disambiguation ( wsd ) is the problem of assigning a sense to an ambiguous word , using its context .
word embeddings have proved useful in downstream nlp tasks such as part of speech tagging , named entity recognition , and machine translation .
to calculate language model features , we train traditional n-gram language models with ngram lengths of four and five using the srilm toolkit .
pitler et al experiment with polarity tags , verb classes , length of verb phrases , modality , context and lexical features and found that word pairs with non-zero information gain yield best results .
relation extraction is a fundamental step in many natural language processing applications such as learning ontologies from texts ( cite-p-12-1-0 ) and question answering ( cite-p-12-3-6 ) .
for all the systems we train , we build n-gram language model with modified kneserney smoothing using kenlm .
we used moses , a phrase-based smt toolkit , for training the translation model .
we used a 5-gram language model with modified kneser-ney smoothing implemented using the srilm toolkit .
our system is built using the open-source moses toolkit with default settings .
the phrase-based approach developed for statistical machine translation is designed to overcome the restrictions of many-to-many mappings in word-based translation models .
to train the models we use the default stochastic gradient descent classifier provided by scikit-learn .
for collapsed syntactic dependencies we use the stanford dependency parser .
all feature models are estimated in the in-domain corpus with standard techniques .
we used 300 dimensional skip-gram word embeddings pre-trained on pubmed .
a 4-gram language model is trained on the xinhua portion of the gigaword corpus with the srilm toolkit .
we created 5-gram language models for every domain using srilm with improved kneserney smoothing on the target side of the training parallel corpora .
sentiment analysis is a natural language processing ( nlp ) task ( cite-p-10-1-14 ) which aims at classifying documents according to the opinion expressed about a given subject ( federici and dragoni , 2016a , b ) .
we trained a trigram language model on the chinese side , with the srilm toolkit , using the modified kneser-ney smoothing option .
morphological disambiguation is the task of selecting the correct morphological parse for a given word in a given context .
the bleu score , introduced in , is a highly-adopted method for automatic evaluation of machine translation systems .
sequence labeling is a widely used method for named entity recognition and information extraction from unstructured natural language data .
word alignment is a well-studied problem in natural language computing .
english annotations were all produced using the stanford core-nlp toolkit .
the pdtb is the largest corpus annotated for discourse relations , formed by newspaper articles from the wall street journal .
twitter is a rich resource for information about everyday events – people post their tweets to twitter publicly in real-time as they conduct their activities throughout the day , resulting in a significant amount of mundane information about common events .
we used the implementation of the scikit-learn 2 module .
named entity disambiguation ( ned ) is the task of resolving ambiguous mentions of entities to their referent entities in a knowledge base ( kb ) ( e.g. , wikipedia ) .
for language model , we used sri language modeling toolkit to train a 4-gram model with modified kneser-ney smoothing .
for the language model , we used srilm with modified kneser-ney smoothing .
table 4 presents case-insensitive evaluation results on the test set according to the automatic metrics bleu , ter , and meteor .
in part because such distributions can be estimated from positive data .
we build an open-vocabulary language model with kneser-ney smoothing using the srilm toolkit .
li et al propose a hybrid method based on wordnet and the brown corpus to incorporate semantic similarity between words , semantic similarity between sentences , and word order similarity to measure the overall sentence similarity .
to support this point , we further train a topic model based on lda by treating each poem as a document .
we used the bleu score to evaluate the translation accuracy with and without the normalization .
our evaluation metric is case-insensitive bleu-4 .
the n-gram models are created using the srilm toolkit with good-turning smoothing for both the chinese and english data .
semantic role labeling ( srl ) is a form of shallow semantic parsing whose goal is to discover the predicate-argument structure of each predicate in a given input sentence .
we implement this proposal with a hierarchical dirichlet process , which allows for sharing categories across data groups .
to get a dictionary of word embeddings , we use the word2vec tool 2 and train it on the chinese gigaword corpus .
pang and lee attempted to improve the performance of an svm classifier by identifying and removing objective sentences from the texts .
we pre-trained word embeddings using word2vec over tweet text of the full training data .
we use the penn wsj treebank for our experiments .
minimum error rate training under bleu criterion is used to estimate 20 feature function weights over the larger development set .
we then used word2vec to train word embeddings with 512 dimensions on each of the prepared corpora .
shen et al describe the result of filtering rules by insisting that target-side rules are well-formed dependency trees .
semeval is a yearly event in which international teams of researchers work on tasks in a competition format where they tackle open research questions in the field of semantic analysis .
we implement logistic regression with scikit-learn and use the lbfgs solver .
user affect parameters increase the usefulness of these models .
syntactic information is a useful feature to phrase reordering .
in this baseline , we applied the word embedding trained by skipgram on wiki2014 .
wikipedia is a large , multilingual , highly structured , multi-domain encyclopedia , providing an increasingly large wealth of knowledge .
in natural language , a word often assumes different meanings , and the task of determining the correct meaning , or sense , of a word in different contexts is known as word sense disambiguation ( wsd ) .
a bunsetsu consists of one independent word and more than zero ancillary words .
context-free grammar augmented with ¦ë-operators is learned given a set of training sentences and their correct logical forms .
we trained the statistical phrase-based systems using the moses toolkit with mert tuning .
relation extraction ( re ) is a task of identifying typed relations between known entity mentions in a sentence .
we evaluated our models using bleu and ter .
we use the attention-based nmt model introduced by bahdanau et al as our text-only nmt baseline .
the parameter for each feature function in log-linear model is optimized by mert training .
for language models , we use the srilm linear interpolation feature .
jiang et al , 2007 ) put forward a ptc framework based on the svm model .
sentiment analysis is the computational analysis of people ’ s feelings or beliefs expressed in texts such as emotions , opinions , attitudes , appraisals , etc . ( cite-p-11-3-3 ) .
we also measure overall performance with uncased bleu .
sentiment analysis is a fundamental problem aiming to give a machine the ability to understand the emotions and opinions expressed in a written text .
we use the pre-trained glove 50-dimensional word embeddings to represent words found in the glove dataset .
we use the cmu twitter part-of-speech tagger to select only instances in the verb sense .
language models were built using the srilm toolkit 16 .
in natural language , a word often assumes different meanings , and the task of determining the correct meaning , or sense , of a word in different contexts is known as word sense disambiguation ( wsd ) .
we use the 300-dimensional skip-gram word embeddings built on the google-news corpus .
relation extraction ( re ) is the task of assigning a semantic relationship between a pair of arguments .
relation extraction is a fundamental step in many natural language processing applications such as learning ontologies from texts ( cite-p-12-1-0 ) and question answering ( cite-p-12-3-6 ) .
discourse parsing is a fundamental task in natural language processing that entails the discovery of the latent relational structure in a multi-sentence piece of text .
the language model is a large interpolated 5-gram lm with modified kneser-ney smoothing .
turney and littman compute the point wise mutual information of the target term with each seed positive and negative term as a measure of their semantic association .
we use the sri language model toolkit to train a 5-gram model with modified kneser-ney smoothing on the target-side training corpus .
marcu and wong present a joint probability model for phrase-based translation .
we use pre-trained glove vector for initialization of word embeddings .
for decoding , we used the state-of-the-art phrasebased smt toolkit moses with default options , except for the distortion limit .
coreference resolution is the task of partitioning a set of mentions ( i.e . person , organization and location ) into entities .
for all models , we use the 300-dimensional glove word embeddings .
the character embeddings are computed using a method similar to word2vec .
translation quality can be measured in terms of the bleu metric .
we use srilm for training a trigram language model on the english side of the training corpus .
part-of-speech ( pos ) tagging is a fundamental nlp task , used by a wide variety of applications .
we use sri language modeling toolkit to train a 5-gram language model on the english sentences of fbis corpus .
we trained a 5-gram language model on the xinhua portion of gigaword corpus using the srilm toolkit .
the standard classifiers are implemented with scikit-learn .
we used latent dirichlet allocation to construct our topics .
we also report the results using bleu and ter metrics .
the trigram language model is implemented in the srilm toolkit .
sentiment analysis ( sa ) is the determination of the polarity of a piece of text ( positive , negative , neutral ) .
semantic role labeling was pioneered by gildea and jurafsky .
we estimated lexical surprisal using trigram models trained on 1 million hindi sentences from emille corpus using the srilm toolkit .
semantic role labeling ( srl ) is a task of analyzing predicate-argument structures in texts .
ccg is a linguistically-motivated categorial formalism for modeling a wide range of language phenomena .
for the implementation of discriminative sequential model , we chose the wapiti 4 toolkit .
in arabic , there is a reasonable number of sentiment lexicons but with major deficiencies .
we use srilm toolkit to train a trigram language model with modified kneser-ney smoothing on the target side of training corpus .
we use glove vectors with 200 dimensions as pre-trained word embeddings , which are tuned during training .
we used the sri language modeling toolkit to train a fivegram model with modified kneser-ney smoothing .
all systems are evaluated using case-insensitive bleu .
a 4-gram language model generated by sri language modeling toolkit is used in the cube-pruning process .
for a fair comparison to our model , we used word2vec , that pretrain word embeddings at a token level .
bracketing transduction grammar is a special case of synchronous context free grammar .
the baseline system is a phrase-based smt system , built almost entirely using freely available components .
the parameter for each feature function in log-linear model is optimized by mert training .
semantic parsing is the task of converting natural language utterances into their complete formal meaning representations which are executable for some application .
coreference resolution is the process of linking multiple mentions that refer to the same entity .
we used kenlm with srilm to train a 5-gram language model based on all available target language training data .
we use stochastic gradient descent with adagrad , l 2 regularization and minibatch training .
we use the sri language model toolkit to train a 5-gram model with modified kneser-ney smoothing on the target-side training corpus .
we used the implementation of the scikit-learn 2 module .
the language models are 4-grams with modified kneser-ney smoothing which have been trained with the srilm toolkit .
we use 5-grams for all language models implemented using the srilm toolkit .
we use a fourgram language model with modified kneser-ney smoothing as implemented in the srilm toolkit .
our mt decoder is a proprietary engine similar to moses .
the language models were trained with kneser-ney backoff smoothing using the sri language modeling toolkit , .
zelenko et al used the kernel methods for extracting relations from text .
in view of this background , this paper presents a novel error correction framework called error case frames .
to this end , we propose a new annotation scheme to study how preferences are linguistically expressed in two different corpus .
following , we develop a continuous bag-of-words model that can effectively model the surrounding contextual information .
semantic parsing is the task of mapping natural language sentences to a formal representation of meaning .
we use sri language modeling toolkit to train a 5-gram language model on the english sentences of fbis corpus .
kalchbrenner et al show that a cnn for modeling sentences can achieve competitive results in polarity classification .
we use stanford ner for named entity recognition .
we perform pre-training using the skipgram nn architecture available in the word2vec tool .
we further used a 5-gram language model trained using the srilm toolkit with modified kneser-ney smoothing .
we use the liblinear package with the linear kernel 5 .
we used the srilm toolkit to simulate the behavior of flexgram models by using count files as input .
coreference resolution is the problem of identifying which mentions ( i.e. , noun phrases ) refer to which real-world entities .
our baseline is a standard phrase-based smt system .
twitter is a microblogging site where people express themselves and react to content in real-time .
for all models , we use the 300-dimensional glove word embeddings .
we use a set of 318 english function words from the scikit-learn package .
relation extraction is a fundamental step in many natural language processing applications such as learning ontologies from texts ( cite-p-12-1-0 ) and question answering ( cite-p-12-3-6 ) .
the model parameters of word embedding are initialized using word2vec .
ambiguity is the task of building up multiple alternative linguistic structures for a single input .
turian et al used unsupervised word representations as extra word features to improve the accuracy of both ner and chunking .
improved decision list can raise the f-measure of error detection .
for the support vector machine , we used svm-light .
to keep consistent , we initialize the embedding weight with pre-trained word embeddings .
word sense disambiguation ( wsd ) is a key task in computational lexical semantics , inasmuch as it addresses the lexical ambiguity of text by making explicit the meaning of words occurring in a given context ( cite-p-18-3-10 ) .
we use pre-trained embeddings from glove .
we will show translation quality measured with the bleu score as a function of the phrase table size .
we use the 300-dimensional skip-gram word embeddings built on the google-news corpus .
an in-house language modeling toolkit was used to train the 4-gram language models with modified kneser-ney smoothing over the web-crawled data .
we use the srilm toolkit to obtain the per-word-perplexity of each suggested phrase , and normalize it by the maximal perplexity of the language model .
rnns have proven to be a very powerful model in many natural language tasks .
by extracting structures from translated texts , we can generate a phylogenetic tree that reflects the ¡° true ¡± distances among the source languages .
the evaluation metric for the overall translation quality is caseinsensitive bleu4 .
on all datasets and models , we use 300-dimensional word vectors pre-trained on google news .
we used glove vectors trained on common crawl 840b 4 with 300 dimensions as fixed word embeddings .
word sense disambiguation ( wsd ) is the task of determining the meaning of a word in a given context .
word sense disambiguation is the process of determining which sense of a homograph is correct in a given context .
text categorization is the classification of documents with respect to a set of predefined categories .
we use the word2vec skip-gram model to learn initial word representations on wikipedia .
transition-based methods have given competitive accuracies and efficiencies for dependency parsing .
discourse parsing is the process of discovering the latent relational structure of a long form piece of text and remains a significant open challenge .
to calculate language model features , we train traditional n-gram language models with ngram lengths of four and five using the srilm toolkit .
the present paper is a report of these investigations , their results and conclusions drawn therefrom .
the target-side language models were estimated using the srilm toolkit .
these models can be tuned using minimum error rate training .
more importantly , event coreference resolution is a necessary component in any reasonable , broadly applicable computational model of natural language understanding ( cite-p-18-3-4 ) .
model parameters that maximize the loglikelihood of the training data are computed using a numerical optimization method .
relation extraction is the task of finding relational facts in unstructured text and putting them into a structured ( tabularized ) knowledge base .
we apply sri language modeling toolkit to train a 4-gram language model with kneser-ney smoothing .
dependency parsing is a topic that has engendered increasing interest in recent years .
coreference resolution is the task of partitioning the set of mentions of discourse referents in a text into classes ( or ‘ chains ’ ) corresponding to those referents ( cite-p-12-3-14 ) .
we adopt pretrained embeddings for word forms with the provided training data by word2vec .
latent variable models such as latent dirichlet allocation and latent semantic analysis have been widely used to extract topic models from corpora .
additionally , coreference resolution is a pervasive problem in nlp and many nlp applications could benefit from an effective coreference resolver that can be easily configured and customized .
for this task , we use glove pre-trained word embedding trained on common crawl corpus .
